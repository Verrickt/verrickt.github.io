<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Melchior on CLR</title>
  
  <subtitle>Verrickt as a Programmer</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://verrickt.github.io/"/>
  <updated>2020-08-08T10:22:13.075Z</updated>
  <id>https://verrickt.github.io/</id>
  
  <author>
    <name>Verrickt</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>半监督学习</title>
    <link href="https://verrickt.github.io/2020/08/04/semi-supervised-learning/"/>
    <id>https://verrickt.github.io/2020/08/04/semi-supervised-learning/</id>
    <published>2020-08-04T07:27:52.000Z</published>
    <updated>2020-08-08T10:22:13.075Z</updated>
    
    <content type="html"><![CDATA[<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML" async></script><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>本文的是台湾大学2020机器学习<a href="https://youtu.be/fX_guE7JNnY" target="_blank" rel="external">Semi-supervised leraning</a>的笔记。奉行<a href="why-semi-supervised.png">Lazy evaluation</a>策略，对这些知识更深层次的探究只在<strong>绝对必要</strong>时完成。<br><a id="more"></a></p><p>现在是所谓的大数据时代，每人每天产生的数据成千上万。对商业公司来说，数据的收集已经不是问题，但给数据添加标记的工作因为其所需要消耗的人力物力成为老大难问题；而在机器学习中，带标记数据的不足会给模型引入bias。一个很自然的想法是，能不能用这些不带标记的数据来提高模型的泛化性能呢？答案是肯定的。这就是半监督学习。</p><p><img src="/2020/08/04/semi-supervised-learning/introduction.png" alt="1"><br>在半监督学习中，数据分为两块。</p><script type="math/tex; mode=display">{(x^r,\hat y^r)}_{r=1}^R,{x^u}_{u=R}^{R+U}</script><p>其中R是带标记数据(labeled data)的数量，U是不带标记数据(unlabelled data)的数量。U通常远远大于U；<br>根据testing data是否包括unlabelled data，可将半监督学习分为两类。</p><p><img src="/2020/08/04/semi-supervised-learning/why-semi-supervised.png" alt="2"><br>半监督学习为什么有效，一个比较令人接受的说法是，无标签数据的分布对问题有些启发。<br>而半自动学习有没有用，有多大用，取决于假设做的好不好；</p><h2 id="Semi-supervised-Learning-for-Generative-Model"><a href="#Semi-supervised-Learning-for-Generative-Model" class="headerlink" title="Semi-supervised Learning for Generative Model"></a>Semi-supervised Learning for Generative Model</h2><p>在<a href="../../../07/04/from-logistic-regression-to-neural-network/">二元分类</a>时，曾经用过生成模型解决这个问题。<br><img src="/2020/08/04/semi-supervised-learning/recall.png" alt="4"><br>在半监督学习中，分类依旧可以用生成模型来解决。<br><img src="/2020/08/04/semi-supervised-learning/semi-supervised-generative.png" alt="5"></p><p>大体的的思路是，使用unlabelled data来修正对prior的估计，以期更小的泛化错误</p><p>具体算法如下:<br><img src="/2020/08/04/semi-supervised-learning/how-to-train-a-semi-supervised-generative-model.png" alt="6"></p><ol><li>初始化参数θ</li><li>对unlabelled data计算其属于某一个类别的后验概率</li><li>更新参数θ:<script type="math/tex; mode=display">P(C_1)=\frac{N_1+\sum_{x^u} P(C_1|x_u)}{N} \\\mu^1=\frac{1}{N_1}\sum_{x^r \in C_1}x^r+\frac{1}{\sum_{x^u}P(C_1|x^u)}\sum P(C_1|x^u)x^u</script>算法最终会收敛，但初始化的参数值θ会影响结果。</li></ol><p><img src="/2020/08/04/semi-supervised-learning/why-does-it-converge.png" alt="7"></p><p>在监督学习的时候，最大似然函数的结果是有闭式解的；<br>而在半监督学习时，unlabelled data对应的似然函数依赖于当前参数θ，所以只能迭代的来求解。</p><hr><p>半监督学习通常基于两个假设</p><ul><li>Low-density Separation Assumption</li><li>Smoothness Assumption</li></ul><p>这两个假设成立不成立，对半监督学习的效果息息相关。</p><h2 id="Low-density-Separation-Assumption"><a href="#Low-density-Separation-Assumption" class="headerlink" title="Low-density Separation Assumption"></a>Low-density Separation Assumption</h2><p><img src="/2020/08/04/semi-supervised-learning/what-is-low-density.png" alt="8"></p><p> Low-density Separation Assumption认为，在分界线上，数据的密度是低的。换言之，这个世界是非黑即白的。</p><p> 在投影中，左边分界线位置的密度比右边分界线上的密度更低，因此认为左侧分界线更好。</p><p> 基于这个想法，我们可以提出Self training算法：<br><img src="/2020/08/04/semi-supervised-learning/self-training.png" alt="9"></p><ol><li>根据labelled data训练一个model</li><li>用1中得到的model给一部分的unlabelled data打上标记,作为labelled data。这个标记叫做pseudo label(伪标记)</li><li>回到1</li></ol><p>需要注意的是伪标记是否对新训练的model有用。</p><p>如果做regression，pseudo label并不能让model学到新东西，所以就完全没有用处。</p><p>如果用神经网络做分类，用soft label也没有用处<br><img src="/2020/08/04/semi-supervised-learning/self-training-pitfall-2.png" alt="10"></p><p>换一种思路，我们也可以把Low-density Separation Assumption当作一个regularization term:<br><img src="/2020/08/04/semi-supervised-learning/entropy-regularizer.png" alt="13"></p><p>对于一个概率分布，我们希望密度越集中越好。概率分布的集中程度可以用香农熵来衡量。这样把香农熵当作regularization的一个term，就能鼓励网络找到我们所期待的密度分布。</p><h2 id="Smoothness-Assumption"><a href="#Smoothness-Assumption" class="headerlink" title="Smoothness Assumption"></a>Smoothness Assumption</h2><p><img src="/2020/08/04/semi-supervised-learning/smothness.png" alt="11"></p><p>Smoothness Assumption认为对于接近的x，他们的标记y应该是相近的。换言之，近朱者赤近墨者黑；</p><p>下面是Smoothness Assumption的一个例子<br><img src="/2020/08/04/semi-supervised-learning/entropy-regularizer.png" alt="13"></p><p>通过“相似”的传递性，model可以给很多数据打上正确的标签。</p><p><img src="/2020/08/04/semi-supervised-learning/what-is-smoothness.png" alt="12"><br>比较正式的说法是，若x1和x2在一个高密度区域之内非常接近，那么y1和y2就是接近的；</p><p><img src="/2020/08/04/semi-supervised-learning/clustering.png" alt="17"></p><p>这和clustering比较类似。但clustering的数目并不好确定。因此引入了基于图的方法。</p><p><img src="/2020/08/04/semi-supervised-learning/graph-distance.png" alt="18"><br>将数据看作高维空间中的图，只需把图建立起来即可。</p><p>关于图的构建方法，有如下几种<br><img src="/2020/08/04/semi-supervised-learning/how-to-construct-a-graph.png" alt="19"></p><p>可以看到，通过构建图，一个点的影响可以扩散的很大一片区域：<br><img src="/2020/08/04/semi-supervised-learning/graph-advantage.png" alt="20"></p><p>但图也有劣势。如果不能给每一个连通分量收集到一个数据点，这些unlabelled data就无法被计算相似度。</p><p><img src="/2020/08/04/semi-supervised-learning/graph-disadvantage.png" alt="21"><br>有了图之后就可以计算图的smoothness了。这时可以把smoothness当作网络的一个regularizer<br><img src="/2020/08/04/semi-supervised-learning/graph-smoothness.png" alt="22"></p><h2 id="Better-Represerentation"><a href="#Better-Represerentation" class="headerlink" title="Better Represerentation"></a>Better Represerentation</h2><p><img src="/2020/08/04/semi-supervised-learning/better-represerentation.png" alt="15"></p><p>对使用者来说，数据的呈现方式对使用的便利程度影响很大。例如罗马数字的除法就比阿拉伯数字的除法困难很多。</p><p>在机器学习中也是如此。使用更好的呈现方式会让学习事半功倍。关于如何找到更好的呈现方式，可以参考Autoencoders。</p><p><img src="/2020/08/04/semi-supervised-learning/why-better-represerentation.png" alt="23"></p>]]></content>
    
    <summary type="html">
    
      &lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML&quot; async&gt;&lt;/script&gt;

&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;本文的是台湾大学2020机器学习&lt;a href=&quot;https://youtu.be/fX_guE7JNnY&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Semi-supervised leraning&lt;/a&gt;的笔记。奉行&lt;a href=&quot;why-semi-supervised.png&quot;&gt;Lazy evaluation&lt;/a&gt;策略，对这些知识更深层次的探究只在&lt;strong&gt;绝对必要&lt;/strong&gt;时完成。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Machine learning" scheme="https://verrickt.github.io/categories/Machine-learning/"/>
    
    
      <category term="Machine learning" scheme="https://verrickt.github.io/tags/Machine-learning/"/>
    
      <category term="Semi-supervised learning" scheme="https://verrickt.github.io/tags/Semi-supervised-learning/"/>
    
  </entry>
  
  <entry>
    <title>支持向量机</title>
    <link href="https://verrickt.github.io/2020/07/31/support-vector-machine/"/>
    <id>https://verrickt.github.io/2020/07/31/support-vector-machine/</id>
    <published>2020-07-31T11:49:52.000Z</published>
    <updated>2020-08-02T08:22:48.707Z</updated>
    
    <content type="html"><![CDATA[<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML" async></script><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>支持向量机(Support Vector Machine,SVM)是一个能求解通用二元分类问题的模型。SVM使用Kernel trick，使得自己在能够表示非线性函数的同时Loss是有闭式解的Convex function。也有人把SVM当作线性模型向神经网络过渡的中间阶段。</p><a id="more"></a><h2 id="SVM"><a href="#SVM" class="headerlink" title="SVM"></a>SVM</h2><p><img src="/2020/07/31/support-vector-machine/What-is-SVM.png" alt="What is SVM"></p><p>对于</p><script type="math/tex; mode=display">D = \{(x_1,y_1),(x_2,y_2)...(x_n,y_n)\} , y_i \in \{-1,+1\}</script><p>的分类问题，SVM试图在n维空间上找一个超平面，<code>wx+b</code>，使得</p><ul><li>当wx_i+b≥1时，yi=+1</li><li>当wx_i+b≤1时，yi=-1</li></ul><p>并且超平面应该离两侧最近的点都尽可能远。解出这一组w和b就解出了SVM。</p><p>其实如果在将w和b并在一起作为新的w,将x后接1,则SVM可以进一步简化为<code>wx</code>:</p><script type="math/tex; mode=display">f(x)= \sum_i w_ix_i+b = \left[ \begin{array}{c} w \\ b\end{array} \right] \cdot\left[ \begin{array}{c} x \\ 1\end{array} \right]</script><h3 id="Hinge-loss"><a href="#Hinge-loss" class="headerlink" title="Hinge loss"></a>Hinge loss</h3><p><img src="/2020/07/31/support-vector-machine/delta-loss.png" alt="delta-loss"></p><p>回到分类问题本身，通常Loss会选择在整个训练集上分类错误的次数：</p><script type="math/tex; mode=display">L(f)=\sum_i \delta(g(x^n) \not ={\hat{y}^n})</script><p>其中<code>δ</code>函数的为真时值为1，反之值为0。<br>正如<a href="../../../07/04/from-logistic-regression-to-neural-network/">对率回归</a>中所说，<br>在分类问题上使用这类函数是不利于训练的，因此希望找到一个函数</p><script type="math/tex; mode=display">l(f(x^n),\hat y^n)</script><p>来近似代替<code>δ</code>函数。</p><p>hinge loss就是这样的一个函数。他是这么定义的：</p><script type="math/tex; mode=display">l(f(x^n),\hat y^n)=\max(0,1-\hat y^nf(x))</script><p><img src="/2020/07/31/support-vector-machine/comparsion-of-loss.png" alt="comparsion of loss"></p><p>图中紫色的线就是hinge loss。横坐标1处，hinge loss把两条直线连接起来，就像是连接骨头的关节(hinge)，因此得名。</p><p>从图上可以看出</p><ul><li>MSE随着横坐标的增加loss反而增加，根本不适用</li><li>sigmoid+square loss对横坐标的变化不敏感(saturate)，训练起来也不好做</li><li>sigmoid+cross entropy会积极的将坐标点往右推，但其实只要横坐标超过了1就已经足够了</li></ul><p><img src="/2020/07/31/support-vector-machine/entropy-vs-hinge.png" alt="entropy-vs-hinge"></p><p>cross entropy好像完美主义者，而hinge loss会想着及格就好。既然在现实世界里两者的性能是一致的，我们也就乐得省下这些无畏的开销。</p><h3 id="Gradient-descent"><a href="#Gradient-descent" class="headerlink" title="Gradient descent"></a>Gradient descent</h3><p><img src="/2020/07/31/support-vector-machine/gradient-descent.png" alt="gradient-descent"></p><p>hinge loss对w的偏微分后，得到梯度下降的公式</p><script type="math/tex; mode=display">w_{i+1}=w_i-\eta\sum_n c^n(w)x_i^n</script><p>其中</p><script type="math/tex; mode=display">c^n(w) = -\delta(\hat y^nf(x^n)<1)\hat y^nx_i</script><h3 id="soft-margin"><a href="#soft-margin" class="headerlink" title="soft margin"></a>soft margin</h3><p>将整体的损失函数L</p><script type="math/tex; mode=display">L(f)=l(0,1-\hat y^nf(x)) + \lambda|w|_2=\sum_n \max(0,1-\hat y^nf(x)) + \lambda|w|_2</script><p>中的l用ε代替，得到下列形式<br><img src="/2020/07/31/support-vector-machine/slack-variable.png" alt="slack-variable"></p><script type="math/tex; mode=display">L = \sum_n\epsilon^n+\lambda|w|_2</script><script type="math/tex; mode=display">\hat y^nf(x) \ge1-\epsilon^n</script><p>而不是硬性规定</p><script type="math/tex; mode=display">\hat y^nf(x) \ge1</script><p><code>ε</code>作为边界条件的放松，被称作松弛变量(slack variable)</p><h3 id="Support-vector"><a href="#Support-vector" class="headerlink" title="Support vector"></a>Support vector</h3><p>SVM的w是所有训练样本点的线性组合，并且多数点的系数都是0(sparsity)。</p><p>这个性质可以从KKT系数反映出来<br><img src="/2020/07/31/support-vector-machine/traditonal-support-vector.png" alt="traditonal-support-vector"></p><p>当然，它也可以从另一个角度说明。<br><img src="/2020/07/31/support-vector-machine/support-vector.png" alt="support-vector"><br>考虑梯度下降的式子</p><script type="math/tex; mode=display">w_{i+1}=w_i-\eta\sum_n c^n(w)x_i^n</script><p>其中</p><script type="math/tex; mode=display">c^n(w)  =\frac{\partial l(f(x^n),\hat y^n)}{\partial f(x^n)}=-\delta(\hat y^nf(x^n)<1)\hat y^nx_i</script><p>当w被初始化为0向量时，每一次参数更新加上了一个样本点的线性组合；而hinge loss有大约一半的定义域里微分是0，因此大部分系数会为0</p><h2 id="Kernel-method"><a href="#Kernel-method" class="headerlink" title="Kernel method"></a>Kernel method</h2><p><img src="/2020/07/31/support-vector-machine/kernel-function.png" alt="kernel function"></p><p>既然w是X的一个线性组合，将其写作<code>w=Xα</code>。<br>带入f(x)</p><script type="math/tex; mode=display">f(x)=w^\top x = \sum_n \alpha_n(x^n \cdot x)</script><p>将x^n与x的内积用函数K(kernel function)表示，则</p><script type="math/tex; mode=display">f(x)=\sum_n \alpha_nK(x^n,x)</script><p><img src="/2020/07/31/support-vector-machine/kernel-trick.png" alt="kernel trick"><br>将得到的f(x)带入loss，发现L(f)与x已经无关了，可以直接进行梯度下降。这就是Kernel trick<br><img src="/2020/07/31/support-vector-machine/useful-kernel-trick.png" alt="useful kenel trick"></p><p>若我们先对x做feature transform <code>φ(x)</code>，<code>φ(x)·φ(z)</code>有时可以表示为K(x,z)。这即扩展了SVM的表述能力，也提高了效率。</p><p>在使用RBF Kernel时，可以在无穷维空间做内积</p><p><img src="/2020/07/31/support-vector-machine/RBF-Kernel.png" alt="RBF-Kernel"></p><h2 id="SVM-and-Neural-network"><a href="#SVM-and-Neural-network" class="headerlink" title="SVM and Neural network"></a>SVM and Neural network</h2><p>SVM也可以被看作是有一层hidden unit的神经网络:</p><p><img src="/2020/07/31/support-vector-machine/SVM-as-NN.png" alt="SVM-as-NN"></p><p>每一个neural的weight是x^n与x的内积，所有neural的输出经过tanh后再以α为权值求和就得到输出了。</p><p><img src="/2020/07/31/support-vector-machine/SVM-and-NN.png" alt="SVM-and-NN"><br>实际上，SVM也可以看作是一个神经网络。</p><ul><li>Kernel function相当于是神经网络中做feature transform的hidden layer</li><li>SVM学到的分类器与神经网络的输出层学到的分类器是一致的。</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML&quot; async&gt;&lt;/script&gt;

&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;支持向量机(Support Vector Machine,SVM)是一个能求解通用二元分类问题的模型。SVM使用Kernel trick，使得自己在能够表示非线性函数的同时Loss是有闭式解的Convex function。也有人把SVM当作线性模型向神经网络过渡的中间阶段。&lt;/p&gt;
    
    </summary>
    
      <category term="Machine learning" scheme="https://verrickt.github.io/categories/Machine-learning/"/>
    
    
      <category term="Machine learning" scheme="https://verrickt.github.io/tags/Machine-learning/"/>
    
      <category term="SVM" scheme="https://verrickt.github.io/tags/SVM/"/>
    
  </entry>
  
  <entry>
    <title>主成分分析</title>
    <link href="https://verrickt.github.io/2020/07/31/principal-component-analysis/"/>
    <id>https://verrickt.github.io/2020/07/31/principal-component-analysis/</id>
    <published>2020-07-31T11:49:41.000Z</published>
    <updated>2020-08-02T04:39:44.773Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML" async></script><p>主成分分析(Principal component analysis,PCA)是一种常用的无监督学习算法。经过PCA后的数据会在它所处理的维度上会不相关(decorelation)。 而PCA所选的维度又通常比本来的维度要小，因此也可以用来做降维处理。</p><a id="more"></a><h2 id="PCA"><a href="#PCA" class="headerlink" title="PCA"></a>PCA</h2><p>PCA的想法很朴素。它试图找到一个线性变化W，使得经过变化后的向量<code>z=Wx</code>在各个维度<code>z_i</code>上都拥有最大的方差。</p><p><img src="/2020/07/31/principal-component-analysis/maximum-variance.png" alt="maximum variance"></p><h2 id="Constraints"><a href="#Constraints" class="headerlink" title="Constraints"></a>Constraints</h2><p>仅仅要求方差最大而不做任何限制是不行的。我们总是可以让<code>w_i</code>含有无穷从而使方差变为无穷大，但这没有意义。</p><p>因此限制<code>w_i</code>的模是1。<br>同样的，当找出使得第i维的方差最大的参数<code>w_i</code>后，机器可以把这组参数<code>w_i</code>用在第j维上来“偷懒”，因此限制任意的参数是垂直的。</p><p><img src="/2020/07/31/principal-component-analysis/constraint.png" alt="constraint"></p><h2 id="Dimension-reduction"><a href="#Dimension-reduction" class="headerlink" title="Dimension reduction"></a>Dimension reduction</h2><h3 id="to-1-dimension"><a href="#to-1-dimension" class="headerlink" title="to 1 dimension"></a>to 1 dimension</h3><p>先考虑一维的情况。</p><script type="math/tex; mode=display">z_1=w_1'x,其中 (w_1')^\top w_1'=1</script><p><img src="/2020/07/31/principal-component-analysis/z1-var.png" alt="z1_var"></p><script type="math/tex; mode=display">\mathrm{Var}(z_1)=\sum_{z_{i}}(z_1-\bar z)^2=\sum_x(w_1'x-w_1'\bar x)^2=\sum_x(w_1'(x- \bar x))^2 \\=\sum_x(w_1')^\top(x- \bar x)(x- \bar x)^\top w_1' \\=(w_1')^\top\left(\sum_x(x-\bar x)(x-\bar x)^\top\right)w_1' \\=(w_1')^\top\mathrm{Cov}(x)w_1'</script><p><img src="/2020/07/31/principal-component-analysis/w1-eigenvector.png" alt="w1-eigenvector"></p><p>构造拉格朗日函数，其中S=Cov(x),</p><script type="math/tex; mode=display">L(w_1')=(w_1')^\top S w_1'-\alpha((w_1')^\top w_1'-1) \\\frac{\partial L}{\partial w}=0 \rArr Sw_1'-\alpha w_1'\rArr Sw_1' = \alpha w_1'\rArr \alpha是S的特征值,w'是对应的特征向量</script><p>带回到上式</p><script type="math/tex; mode=display">(w_1')^\top\mathrm{Cov}(x)w_1'= (w_1')^\top Sw_1' = (w_1')^\top \alpha w_1'= \alpha (w_1')^\top w_1'=\alpha</script><p><code>α</code>的最大值，即是Cov(x)最大的特征值。此时的<code>w1&#39;</code>为<code>α</code>对应的特征向量</p><p>对于二元的情况也类似</p><h3 id="to-2-dimensions"><a href="#to-2-dimensions" class="headerlink" title="to 2 dimensions"></a>to 2 dimensions</h3><p><img src="/2020/07/31/principal-component-analysis/z2_lagrange_operator.png" alt="z2_lagrange_operator"></p><p>拉格朗日函数</p><script type="math/tex; mode=display">g(w_2)=(w_2')^\top S w_2 - \alpha((w_2')^\top w_2' = 1) - \beta((w_2')^\top w_1')</script><p>而</p><script type="math/tex; mode=display">(w_2')^\top w_2' = 1 \\(w_1')^\top w_2' = 0 \\(w_1')^\top Sw_2'=((w_1')^\top Sw_2')^\top=(w_2')^\top S^\top w_1  \\=(w_2')^\top S w_1 = (w_2')^\top \alpha w_1 \\=\alpha  (w_2')^\top w_1 = 0</script><p>所以系数拉格朗日函数的只剩下<code>β</code>.</p><p><img src="/2020/07/31/principal-component-analysis/z2_simplified.png" alt="z2_simplified"></p><p><code>β</code>的最大值，即是Cov(x)第二大的特征值。此时的<code>w2&#39;</code>为<code>β</code>对应的特征向量</p><h3 id="to-k-dimensions"><a href="#to-k-dimensions" class="headerlink" title="to k dimensions"></a>to k dimensions</h3><p>把PCA推广到k维，则<code>W</code>是<code>Cov(X)</code>的最大的k个特征值所对应的特征向量的行矩阵。</p><h2 id="Decorelation"><a href="#Decorelation" class="headerlink" title="Decorelation"></a>Decorelation</h2><p>PCA处理后的z矩阵的各个维度之间是没有相互关系的。</p><p><img src="/2020/07/31/principal-component-analysis/decorelation.png" alt="decorelation"></p><script type="math/tex; mode=display">Cov(z)=\sum(z-\bar{z})(z-\bar{z})^\top=\sum_xW(x- \bar x)(x- \bar x)^\top W^\top=WCov(x)W^\top</script><p>将W展开，</p><script type="math/tex; mode=display">Cov(z)=WS\left[w^1,w^2...w^k\right]</script><script type="math/tex; mode=display">=W\left[Sw^1,Sw^2...Sw^k\right]</script><script type="math/tex; mode=display">=W\left[\lambda_1w^1,\lambda_2w^2...\lambda_kw^k\right]</script><script type="math/tex; mode=display">=\left[w^1,w^2...w^k\right]^\top \left[\lambda_1w^1,\lambda_2w^2...\lambda_kw^k\right]</script><script type="math/tex; mode=display">\forall i, (w^i)^\top w^i = 1</script><script type="math/tex; mode=display">\forall i\not ={j}, (w^i)^\top w^j = 0</script><p>因此</p><script type="math/tex; mode=display">Cov(z)=diag(\lambda_1,\lambda_2,...,\lambda_k)</script><p>协方差矩阵为对角阵，所以不同分量之间没有线性的相关性。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML&quot; async&gt;&lt;/script&gt;

&lt;p&gt;主成分分析(Principal component analysis,PCA)是一种常用的无监督学习算法。经过PCA后的数据会在它所处理的维度上会不相关(decorelation)。 而PCA所选的维度又通常比本来的维度要小，因此也可以用来做降维处理。&lt;/p&gt;
    
    </summary>
    
      <category term="Machine learning" scheme="https://verrickt.github.io/categories/Machine-learning/"/>
    
    
      <category term="Machine learning" scheme="https://verrickt.github.io/tags/Machine-learning/"/>
    
      <category term="PCA" scheme="https://verrickt.github.io/tags/PCA/"/>
    
  </entry>
  
  <entry>
    <title>使用CUDA为Tensorflow加速</title>
    <link href="https://verrickt.github.io/2020/07/28/gpu-accelerated-tranining-with-cuda/"/>
    <id>https://verrickt.github.io/2020/07/28/gpu-accelerated-tranining-with-cuda/</id>
    <published>2020-07-28T13:39:05.000Z</published>
    <updated>2020-07-28T14:29:22.421Z</updated>
    
    <content type="html"><![CDATA[<p>梯度下降法大部分时间都在进行向量和矩阵运算。这些运算是天然可以并行化的。因此使用GPU进行运算会比CPU运算快得多。而常用的框架Tensorflow就通过CUDA提供了GPU运算的支持。<br><a id="more"></a><br>根据<a href="https://www.tensorflow.org/install/gpu#windows_setup" target="_blank" rel="external">官方页面</a>，对软硬件有如下要求:</p><blockquote><p>The following GPU-enabled devices are supported:</p><ul><li>NVIDIA® GPU card with CUDA® architectures 3.5 or higher. See the list of CUDA®-enabled GPU cards.</li><li>For GPUs with unsupported CUDA® architectures, or to avoid JIT compilation from PTX, or to use different versions of the NVIDIA® libraries, see the Linux build from source guide.<br>-On systems with NVIDIA® Ampere GPUs (CUDA architecture 8.0) or newer, kernels are JIT-compiled from PTX and TensorFlow can take over 30 minutes to start up. This overhead can be limited to the first start up by increasing the default JIT cache size with: ‘export CUDA_CACHE_MAXSIZE=2147483648’ (see JIT Caching for details).<br>-Packages do not contain PTX code except for the latest supported CUDA® architecture; therefore, TensorFlow fails to load on older GPUs when CUDA_FORCE_PTX_JIT=1 is set. (See Application Compatibility for details.)</li></ul><p>The following NVIDIA® software must be installed on your system:</p><ul><li>NVIDIA® GPU drivers —CUDA® 10.1 requires 418.x or higher.</li><li>CUDA® Toolkit —TensorFlow supports CUDA® 10.1 (TensorFlow &gt;= 2.1.0)</li><li>CUPTI ships with the CUDA® Toolkit.</li><li>cuDNN SDK 7.6 </li></ul></blockquote><p>换句话说，只要不是上古时代的NVIDIA GPU，都可以进行运算。</p><h2 id="踩坑"><a href="#踩坑" class="headerlink" title="踩坑"></a>踩坑</h2><h3 id="Tensorflow"><a href="#Tensorflow" class="headerlink" title="Tensorflow"></a>Tensorflow</h3><p>安装tensorflow后要还要安装tensorflow-gpu。tensorflow-gpu<strong>不</strong>是tensorflow的替代者，而是支持运算GPU的模块。不要被网上的信息误导，两者都需要安装<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">pip install tensorflow</div><div class="line">pip install tensorflow-gpu</div></pre></td></tr></table></figure></p><h3 id="CUDA"><a href="#CUDA" class="headerlink" title="CUDA"></a>CUDA</h3><p>CUDA请一定安装10.1版本，更新的和更旧的版本都不支持。要去Archive里找。</p><h3 id="cuDNN"><a href="#cuDNN" class="headerlink" title="cuDNN"></a>cuDNN</h3><p>cuDNN请一定安装7.6版本，更新的和更旧的版本都不支持。<br>cuDNN要先去注册NVIDIA developer再去Archive里找7.6的</p><h3 id="Coding"><a href="#Coding" class="headerlink" title="Coding"></a>Coding</h3><p>全部安装好后去跑hello world。tensorflow可能会卡在<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0</div></pre></td></tr></table></figure></p><p>现在去泡杯咖啡，坐和放宽，大概过几个小时就好了。这么大的延迟只会第一次出现。原因似乎是因为GPU那边在做JIT🙃</p><p>等出现这一行<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1376 MB memory) -&gt; physical GPU (device: 0, name: GeForce 840M, pci bus id: 0000:01:00.0, compute capability: 5.0)</div></pre></td></tr></table></figure></p><p>就说明成功了。</p><p>我的渣渣840M跑训练比CPU快了一个数量级;-)</p><h2 id="吐槽"><a href="#吐槽" class="headerlink" title="吐槽"></a>吐槽</h2><p>已经2020年了，CUDA已经出到<code>11.0</code>了的RC了，tensorflow居然还只支持2019年2月发布的<code>10.1</code>。<br>cuDNN同理，也用的是很老的版本。</p><p>CUDA作为NVIDIA家私有的一套API，形成了事实标准，这很不好。而AMD家的搞得叫做<code>ROCm</code>的一套东西，很遗憾的还没成什么气候。<code>ROCm</code>的tensorflow是官方版的一份<a href="https://github.com/ROCmSoftwarePlatform/tensorflow-upstream" target="_blank" rel="external">fork</a>，binary还是社区自己编译的，可以想象坑是无比的多。</p><p>希望开源的标准尽快取代掉私有的CUDA，让A家的GPU也能无痛的跑科学计算。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;梯度下降法大部分时间都在进行向量和矩阵运算。这些运算是天然可以并行化的。因此使用GPU进行运算会比CPU运算快得多。而常用的框架Tensorflow就通过CUDA提供了GPU运算的支持。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Machine learning" scheme="https://verrickt.github.io/tags/Machine-learning/"/>
    
      <category term="CUDA" scheme="https://verrickt.github.io/tags/CUDA/"/>
    
      <category term="Tensorflow" scheme="https://verrickt.github.io/tags/Tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>循环神经网络</title>
    <link href="https://verrickt.github.io/2020/07/23/recurrent-neural-network/"/>
    <id>https://verrickt.github.io/2020/07/23/recurrent-neural-network/</id>
    <published>2020-07-23T10:21:45.000Z</published>
    <updated>2020-07-23T14:31:59.010Z</updated>
    
    <content type="html"><![CDATA[<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML" async></script><p>Recurrent Neural network(RNN)是一种专门处理序列的神经网络。正如CNN可以轻易地处理大规模的“网格状”数据，RNN能够处理其他网络架构都无法处理的长的序列。</p><a id="more"></a><h2 id="Why-RNN"><a href="#Why-RNN" class="headerlink" title="Why RNN"></a>Why RNN</h2><p>考虑这样的问题：航空公司要从文字中提取顾客的目的地和出发地。</p><p>对于</p><blockquote><p>I’ll arrive from Shanghai to ShenZhen</p></blockquote><p>目的地是ShenZhen，出发地是Shanghai</p><p>而对于</p><blockquote><p>I’ll arrive to Shanghai from ShenZhen</p></blockquote><p>则目的地是Shanghai，出发地是ShenZhen。</p><p>使用基于地点在句中位置的方法无法解决问题。到底是出发地还是目的地，与前后的词语都有关。这些信息这称为Context(上下文)</p><p>若使用MLP，则需要在每个位置都重复的学会人类语言的规则，大量重复的参数不但使得计算代价激增，而且也显著增加over-fitting的几率。</p><p>因此，需要一种专门的结构来处理序列数据。RNN应运而生。</p><h2 id="Vanilla-RNN"><a href="#Vanilla-RNN" class="headerlink" title="Vanilla-RNN"></a>Vanilla-RNN</h2><p>RNN企图使用Memory(记忆)来解决问题：<br>当前数据处理过程中隐藏单元的值被保留，下个数据处理时作为额外的输入，这样RNN就拥有了“记住”已经看过数据的能力。</p><p><img src="/2020/07/23/recurrent-neural-network/lemma-RNN.png" alt="Network with memory"></p><p>根据“记忆”的来自隐藏单元还是输出的不同，可以将RNN分为Elman network和Jordan network两种<br><img src="/2020/07/23/recurrent-neural-network/vanilla-RNN.png" alt="Vanilla RNN"></p><p>现在的RNN只能往一个方向看，有时只“往前看”的网络并不能解决问题，我们还需要“往后看”。既往前看又往后看的RNN叫做bidirectional RNN(双向RNN)</p><p>这时只要训练两个RNN：一个从前往后，一个从后往前，再把他们的输出丢到另一个MLP去做处理就可以了<br><img src="/2020/07/23/recurrent-neural-network/bidirectional-RNN.png" alt="bidirectional RNN"></p><h2 id="Exploding-Vanishing-gradients"><a href="#Exploding-Vanishing-gradients" class="headerlink" title="Exploding/Vanishing gradients"></a>Exploding/Vanishing gradients</h2><p>RNN的性能十分出色，但训练的过程往往困难重重。其中一个重要的因素就是Exploding/Vanishing gradients(梯度爆炸/消失)。</p><p><img src="/2020/07/23/recurrent-neural-network/exploding-vanishing-gradient.png" alt="exploding-vanishing-gradient"><br>在RNN中，t-1时刻的隐藏单元的输出会影响到t时刻的隐藏单元。</p><script type="math/tex; mode=display">h^{t}=Wh^{t-1}</script><p>重复使用这个式子，则</p><script type="math/tex; mode=display">h^{t}=W^t\prod_{i=1}^{i=t}h_{i}</script><p>若W可进行特征值分解，则</p><script type="math/tex; mode=display">W=Q^{\top}A Q\\W^t=Q^{\top}A^{t}Q</script><ul><li>若W的特征值λ&gt;1，则经过t次相乘后，λ^t的值会变得非常大(梯度爆炸)。</li><li>若λ&lt;1，则经过t次相乘后，λ^t的值会变得非常接近零(梯度消失)</li></ul><p>梯度爆炸/消失对梯度下降法的干扰非常大，以至于训练过程中出现匪夷所思的结果<br><img src="/2020/07/23/recurrent-neural-network/low-convergence.png" alt="匪夷所思"></p><p>解决这个问题的关键在于给“记忆”可变的权重。</p><h2 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h2><p>Long short-term memory(长<strong>短期记忆</strong>)是一个解决梯度爆炸/消失问题效果较好的方案。<br><img src="/2020/07/23/recurrent-neural-network/LSTM.png" alt="LSTM"><br>在LSTM中，在存储“记忆”的地方(cell)加上了三个gate：</p><ul><li>input gate</li><li>output gate</li><li>forget gate</li></ul><p>这些gate的控制信号经过sigmoid函数后得到的0~1之间的数值表示这些门“打开”的程度</p><ul><li>input gate决定“记忆”是否接受新的输入</li><li>output gate决定“记忆”是否被读出</li><li>forget gate决定“记忆是否被遗忘”</li></ul><p><img src="/2020/07/23/recurrent-neural-network/LSTM-cell.png" alt="LSTM cell"></p><p>图中<code>z</code>是输入,<code>zi,zo,zf</code>分别是上述三个gate的控制信号。<br>设原来存储在“记忆”中的值为c,则新的值c’如此产生</p><ol><li>输入z经过激活函数的值g(z)与input gate的值相乘，得到g(z)*f(zi)</li><li>原来“记忆”中的值c与forget gate的值相乘得到c*f(zf)</li><li>将输入与原“记忆”相加的结果作为新的记忆c’=g(z)<em>f(zi)+c</em>f(zf)</li><li>新的值c’与output gate的值相乘得到c’*f(zo)，结果作为“记忆”的输出</li></ol><p>其中zi,zo,zf是网络的参数，由训练过程中自己学得。将多个LSTM的cell连接起来，就得到了能够处理复杂问题的RNN。当然在实际使用中，zi,zo,和zf也可以接受网络其他部分的参数<br>，这就要看具体的问题了。</p><p><img src="/2020/07/23/recurrent-neural-network/Deep-LSTM.png" alt="Deep-LSTM.png"></p>]]></content>
    
    <summary type="html">
    
      &lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML&quot; async&gt;&lt;/script&gt;

&lt;p&gt;Recurrent Neural network(RNN)是一种专门处理序列的神经网络。正如CNN可以轻易地处理大规模的“网格状”数据，RNN能够处理其他网络架构都无法处理的长的序列。&lt;/p&gt;
    
    </summary>
    
      <category term="Machine learning" scheme="https://verrickt.github.io/categories/Machine-learning/"/>
    
    
      <category term="Machine learning" scheme="https://verrickt.github.io/tags/Machine-learning/"/>
    
      <category term="RNN" scheme="https://verrickt.github.io/tags/RNN/"/>
    
  </entry>
  
  <entry>
    <title>卷积神经网络</title>
    <link href="https://verrickt.github.io/2020/07/18/convolutional-neural-network/"/>
    <id>https://verrickt.github.io/2020/07/18/convolutional-neural-network/</id>
    <published>2020-07-18T11:31:16.000Z</published>
    <updated>2020-07-18T14:42:13.972Z</updated>
    
    <content type="html"><![CDATA[<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML" async></script><p>任何一个机器学习的任务都可以被拆解为三步</p><ol><li>找到一组函数(model)</li><li>找到评价函数好坏的标准(loss)</li><li>找出最好的函数(optimization)</li></ol><p>这三步在神经网络中同样成立。但在神经网络中，要找的不是一组函数，而是一种网络架构(architecture)。本文中的卷积神经网络(CNN)就是一种网络架构<br><a id="more"></a><br>根据universal approximation theorem，MLP已经足够表达任意函数。而在实际中，因为相邻层级的神经元之间的任意互联导致MLP容易过拟合。而在处理一些具有结构化的输入时，MLP中又会有大量的参数重复。为了解决这些问题，引入了CNN。</p><h2 id="Why-CNN"><a href="#Why-CNN" class="headerlink" title="Why CNN?"></a>Why CNN?</h2><p>考虑这样一个问题：如何让机器认出图片里的一张鸟？</p><p>回顾人类的思考过程，若图片中的物体具有鸟的基本特征，如喙、翅膀和爪，则认为它是鸟。在这个过程中，我们并不关心翅膀在图片中的位置：只要有喙就可以了。对翅膀和爪也是同样成立。</p><p>继续思考，怎么认出喙？喙可以由它的边界(geometry)来定义：两条斜线围成的尖尖的多西。翅膀和爪也可以有对应的边界来定义。同样的，我们并不关心边界出现的位置，只要它能确定这个物体是个喙就可以了。</p><p>实际上这就是CNN出现的动机之一：我们只关心一些模式是否出现，而不关心它们出现的位置。换句话说，我们所期待的模式会在图片的不同区域出现。</p><p>回到图像的例子上，人们发现了三个属性：</p><ol><li>比起整张图片，模式通常是比较小的</li><li>相同的模式会在不同的区域出现</li><li>对图片做subsampling(如删掉奇数行和偶数列)并不会改变图中的物体</li></ol><p><img src="/2020/07/18/convolutional-neural-network/properties.png" alt="properties"></p><p>基于这三种属性，人们提出了convolution和pooling两种操作。convolution对应属性1、2，pooling对应属性3。convolution和pooling交替就是CNN的架构。</p><h2 id="Convolution"><a href="#Convolution" class="headerlink" title="Convolution"></a>Convolution</h2><p>Convolution的基础单位是filter(kernel)。filter是一个矩阵，用来检测模式<br>考虑如下的情景<br><img src="/2020/07/18/convolutional-neural-network/filter.png" alt="filter"></p><p>输入是<code>6*6</code>的图片，filter是<code>3*3</code>的矩阵。在图片的左上角找到一个与filter一样大的矩阵，做element-wise product，会得到一个数值。将filter往右移动一格，继续这个步骤，就又得到新的数值。移动的距离称为stride(步长)</p><p>filter通常比输入更小，而filter在移动的过程中可以在不同的位置上检测出对应的模式。这就体现了属性1和2。</p><p><img src="/2020/07/18/convolutional-neural-network/stride.png" alt="stride"></p><p>filter移动完毕后会得到一个新的矩阵。我们可以使用多个filter,将这些filter的输出放在一起会得到一个三维的张量，称为feature map.<br><img src="/2020/07/18/convolutional-neural-network/feature-map.png" alt="feature map"></p><p><img src="/2020/07/18/convolutional-neural-network/multi-channel.png" alt="multi-channel"><br>输入也不一定是黑白的。若输入彩色图片，则需要一个三维的张量来表示</p><script type="math/tex; mode=display">\mathrm{V}_{c,i,j}</script><p>这时的filter也是一个三维张量。<br>这里的重点是，不管输入是什么样，filter的“z轴”要与输入的“z轴”一样高，然后在其他轴上以stride移动，得到的标量集合起来成为新的张量作为下一层的输入。<br>Convolution得到的feature map会缩小，这可以补零(zero padding)来避免</p><p>实际中通过在MLP中共享参数来实现。使用了更少的参数来做同样的事情就不容易过拟合。<br><img src="/2020/07/18/convolutional-neural-network/shared-weight.png" alt="shared-weight"></p><h2 id="Pooling"><a href="#Pooling" class="headerlink" title="Pooling"></a>Pooling</h2><p>Pooling的思路更简单了。对一些区域进行采样，得到一张更小的image,这就是属性3。<br><img src="/2020/07/18/convolutional-neural-network/before-pooling.png" alt="before-pooling"></p><p>pooling有很多选择，可以取算术平均，可以取L2 norm。一般比较常见的是max pooling。<br><img src="/2020/07/18/convolutional-neural-network/after-pooling.png" alt="after-pooling"></p><p>通过pooling，网络就可以对输入中比较规律的变化不那么敏感。这称为invariance。<br><img src="/2020/07/18/convolutional-neural-network/invariance.png" alt="invariance"></p><h2 id="Flatten"><a href="#Flatten" class="headerlink" title="Flatten"></a>Flatten</h2><p>做完pooling后得到的是一个张量。把它“拉直”成一个向量，丢给MLP就可以实现我们所要做的事情了。<br><img src="/2020/07/18/convolutional-neural-network/whole-cnn.png" alt="whole"></p><p>CNN并不只能用于图像处理，只要要处理的问题具有上述的三个属性，就可以用CNN来解决。<br>下面是一些用CNN效果比较好的任务:<br><img src="/2020/07/18/convolutional-neural-network/more-cnn.png" alt="more cnn"></p>]]></content>
    
    <summary type="html">
    
      &lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML&quot; async&gt;&lt;/script&gt;

&lt;p&gt;任何一个机器学习的任务都可以被拆解为三步&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;找到一组函数(model)&lt;/li&gt;
&lt;li&gt;找到评价函数好坏的标准(loss)&lt;/li&gt;
&lt;li&gt;找出最好的函数(optimization)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;这三步在神经网络中同样成立。但在神经网络中，要找的不是一组函数，而是一种网络架构(architecture)。本文中的卷积神经网络(CNN)就是一种网络架构&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Machine learning" scheme="https://verrickt.github.io/categories/Machine-learning/"/>
    
    
      <category term="Machine learning" scheme="https://verrickt.github.io/tags/Machine-learning/"/>
    
      <category term="CNN" scheme="https://verrickt.github.io/tags/CNN/"/>
    
  </entry>
  
  <entry>
    <title>神经网络中的Regularization</title>
    <link href="https://verrickt.github.io/2020/07/15/regularization-of-neural-network/"/>
    <id>https://verrickt.github.io/2020/07/15/regularization-of-neural-network/</id>
    <published>2020-07-15T02:22:44.000Z</published>
    <updated>2020-07-15T06:33:00.639Z</updated>
    
    <content type="html"><![CDATA[<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML" async></script><p>Deep learning algorithms are typically applied to extremely complicated domains where the true generation process essentially involves simulating the entire universe… Controlling the complexity of the model is not a simple matter of finding the model of the right size, with the right number of parameters. Instead, we might find—and indeed in practical deep learning scenarios, we almost always do find—that the best fitting model (in the sense of minimizing generalization error) is a large model that has been regularized appropriately</p><a id="more"></a><p>regularization是我们应对overfitting的常用手段。在比较简单的模型，例如linear-regresseion中，人对的capacity的估计通常比较准确，一般不需要做regularization。而在神经网络动辄成千上万个节点，数以百万的参数让估测capacity变得极为困难。正如花书所说，泛化性能最好的往往是复杂的模型加上合理的regularization而得到的</p><h2 id="Parameter-Penalities"><a href="#Parameter-Penalities" class="headerlink" title="Parameter Penalities"></a>Parameter Penalities</h2><p>正如三次式最高次的系数为零时退化为二次式，有越多的参数接近0，模型的capacity就越低，即「绑住手脚」。一般来说regularization不会考虑<code>bias</code>，因为<code>bias</code>对capacity的影响并不如<code>weight</code>那么大，但有需要仍可以给<code>bias</code>加上惩罚。</p><p>参数惩罚的思路是，给<code>Loss</code>加上所有参数<code>θ</code>的函数<code>Ω(θ)</code>这一项，让模型更偏好于θ接近0的归纳假设。</p><script type="math/tex; mode=display">\hat{L}(\theta,\alpha)=L(\theta)+\alpha \Omega(\theta)</script><h3 id="L2-norm"><a href="#L2-norm" class="headerlink" title="L2 norm"></a>L2 norm</h3><p>L2 norm中所选的函数Ω是</p><script type="math/tex; mode=display">\Omega(\theta)=\frac{1}{2}\theta^{\top}\theta</script><p><code>Loss</code>的梯度变为</p><script type="math/tex; mode=display">\nabla_{\theta} \hat{L}=\nabla_{\theta}L+\alpha\theta</script><p>带入梯度下降公式</p><script type="math/tex; mode=display">\theta^{N+1} \leftarrow\theta^N-\epsilon\nabla_\theta \hat{L}=\theta^N-\epsilon\nabla_\theta L - \alpha\epsilon \theta^N = (1-\alpha \epsilon)\theta ^N-\epsilon\nabla_\theta L</script><p>每次梯度下降更新参数时会把原来的参数<strong>乘</strong>上一个固定的值<code>1-αε</code>，使得θ接近于0</p><h3 id="L1-norm"><a href="#L1-norm" class="headerlink" title="L1 norm"></a>L1 norm</h3><p>L1 norm中的函数Ω是</p><script type="math/tex; mode=display">\Omega(\theta)=\sum_{i=1}^N|\theta_i|</script><p><code>Loss</code>的梯度变为</p><script type="math/tex; mode=display">\nabla_{\theta} \hat{L}=\nabla_{\theta}L+\alpha \mathrm{sgn}(\theta)</script><p>带入梯度下降公式</p><script type="math/tex; mode=display">\theta^{N+1} \leftarrow\theta^N-\epsilon\nabla_\theta \hat{L}=\theta^N-\epsilon\nabla_{\theta}L-\alpha\epsilon \mathrm{sgn}(\theta^N) = \theta ^N-\epsilon\nabla_\theta L -\alpha\epsilon \mathrm{sgn}(\theta^N)</script><p>发现参数更新时会在每一个分量<code>i</code>中<strong>减</strong>掉一个固定值<code>αε*sgn(i)</code></p><p>L2 norm中，每次都乘上一个小于1的数，当参数大于1时，减小的速度就非常快；当参数小于1时，减小的速度就比较慢。因此L2中最终各个参数比较接近，但不会出现很多0。</p><p>而L1 norm中，每次减小一个固定值。对很大的参数，减法的步长很小，减小的效果就不是很明显。而参数很小时，减法的步长相对很大，就很容易出现0。因此L1中很容易出现很大的参数和较多的0。</p><p>含较多0的矩阵叫做稀疏矩阵(Sparse matrix)，因此用L1能够得到比较稀疏的参数。</p><h2 id="Early-stopping"><a href="#Early-stopping" class="headerlink" title="Early stopping"></a>Early stopping</h2><p><img src="/2020/07/15/regularization-of-neural-network/early-stopping.png" alt="early stopping"></p><p>early stopping的思路就更简单了。既然复杂的模型训练时会关注一些无关的特征，那干脆不要跑那么多次训练就好了。</p><p>early stopping的形式化描述：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line">Let n be the number of steps between evaluations.</div><div class="line">Let p be the “patience,” the number of times to observe worsening validation set error before giving up. </div><div class="line">Let θ^o be the initial parameters. </div><div class="line">θ ← θ^o </div><div class="line">i ← 0 </div><div class="line">j ← 0 </div><div class="line">v ←∞ </div><div class="line">θ∗ ← θ </div><div class="line">i∗ ← i </div><div class="line">while j &lt; p do θ</div><div class="line">    Update by running the training algorithm for n steps. </div><div class="line">    i ← i + n</div><div class="line">    v&apos; ← ValidationSetError(θ) </div><div class="line">    if v&apos; &lt; v then </div><div class="line">        j  ← 0</div><div class="line">        θ∗ ← θ </div><div class="line">        i∗ ← i</div><div class="line">        v  ← v&apos;</div><div class="line">    else </div><div class="line">        j ← j + 1 </div><div class="line">    end if</div><div class="line">end while</div></pre></td></tr></table></figure></p><hr><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">nodes:</div><div class="line">Best parameters are θ∗ , best number of training steps is i∗</div></pre></td></tr></table></figure><h2 id="Ensemble"><a href="#Ensemble" class="headerlink" title="Ensemble"></a>Ensemble</h2><p>Ensemble认为，模型中出现的错误是随机的，那么使用<strong>同样</strong>的数据训练出k个不同的模型，将它们的结果取平均就可以得到比较好的结果。</p><p>复杂模型error主要来自variance。从直觉上说，对多个模型取平均就能很好的抵消一部分variance。<br>ensemble的坏处在于，它需要训练k个不同的模型，导致计算量大增。有时这样的计算量是无法承受的，因此希望有一个代价不那么大的ensemble方法，这就是dropout</p><h3 id="dropout"><a href="#dropout" class="headerlink" title="dropout"></a>dropout</h3><p>dropout和以往的方法不同，它把当前训练的模型当作是很多模型的叠加：每次训练时，每个单元都有p的概率从网络中被移除，每次只针对这些被留下来的单元做参数的更新。有n个单元时，因为每一个单元都可以被留下/丢掉，这就可能产生2^N个不同的网络结构。dropout就认为当前的模型是这2^N个网络的ensemble。</p><p>下面是一个2层的神经网络。它有2个隐藏单元。dropout可以产生的网络由16个。使用dropout的网络经过一次训练后相当于训练了2^N个子网络，计算量的问题就这样解决了。<br><img src="/2020/07/15/regularization-of-neural-network/dropout.png" alt="droupout"></p><p>使用dropout的网络训练完成后，在测试时不再移除神经元，而是给网络整体的参数乘上1-p。<br>一个直觉的解释：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">设w是dropout得到的参数，w&apos;是训练时的参数。</div><div class="line">在训练时神经元有p的概率被丢弃，所以网络中实际神经元个数的期望是n*(1-p)。</div><div class="line">现在要让网络的输出尽可能的一致，则</div><div class="line">n*w*(1-p) = n*w&apos;</div><div class="line">即w&apos;=w*(1-p)</div></pre></td></tr></table></figure></p><p><img src="/2020/07/15/regularization-of-neural-network/linearity.png" alt="linearity"></p><p>实际的分析发现，在整个网络是线性时，上面的论述是精确成立的，而加上激活函数的网络往往不是线性的。但实际使用上dropout的性能也确实比较接近ensemble的结果。为什么会这样我们不清楚，总之拿来主义了。世界各地有很多科学家也在探究背后的奥秘，希望有一天能找出一个精确的解释吧。</p>]]></content>
    
    <summary type="html">
    
      &lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML&quot; async&gt;&lt;/script&gt;

&lt;p&gt;Deep learning algorithms are typically applied to extremely complicated domains where the true generation process essentially involves simulating the entire universe… Controlling the complexity of the model is not a simple matter of finding the model of the right size, with the right number of parameters. Instead, we might find—and indeed in practical deep learning scenarios, we almost always do find—that the best fitting model (in the sense of minimizing generalization error) is a large model that has been regularized appropriately&lt;/p&gt;
    
    </summary>
    
      <category term="Machine learning" scheme="https://verrickt.github.io/categories/Machine-learning/"/>
    
    
      <category term="Machine learning" scheme="https://verrickt.github.io/tags/Machine-learning/"/>
    
      <category term="Regularization" scheme="https://verrickt.github.io/tags/Regularization/"/>
    
  </entry>
  
  <entry>
    <title>后向传播-进阶版的梯度下降算法</title>
    <link href="https://verrickt.github.io/2020/07/11/back-propagation-gradient-descent-improved/"/>
    <id>https://verrickt.github.io/2020/07/11/back-propagation-gradient-descent-improved/</id>
    <published>2020-07-11T03:21:40.000Z</published>
    <updated>2020-07-11T14:11:29.102Z</updated>
    
    <content type="html"><![CDATA[<p><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML" async></script><br>本文是<a href="http://speech.ee.ntu.edu.tw/~tlkagk/courses_ML20.html" target="_blank" rel="external">NTU ML 2020</a>中<a href="https://youtu.be/Dr-WRlEFefw" target="_blank" rel="external">BackPropagation</a>部分的笔记。</p><p>梯度下降是非常常用的优化算法。而在深度学习中，一个神经网络的参数动辄几百万，像线性模型那样人手工算好再告诉机器的方式已经不合适了，需要一种更高效的算法来计算梯度。<br><a id="more"></a><br>后向传播是计算梯度的高效算法，它的理论基础是链式法则。</p><script type="math/tex; mode=display">p=\phi(x),q=\varphi(x),g=f(p,q) \\\frac{dg}{dx}=\frac{\partial g}{\partial p}\frac{dp}{dx}+\frac{\partial g}{\partial q}\frac{dq}{dx}</script><hr><p>梯度下降的目标函数<code>total loss</code>是各个example的和</p><script type="math/tex; mode=display">L(\theta)=\sum_{n=1}^N C^{n}(\theta)</script><p>所以只需能计算出其中一项的<code>loss</code>就可求<code>total loss</code>的梯度。</p><p>考虑如下的一个3x2的神经网络<br><img src="/2020/07/11/back-propagation-gradient-descent-improved/network_structure.png" alt="network structure"></p><p>第一层第一个神经元的参数<code>w1</code>，输入激活函数的值<code>z=w1*x1+w2*x2+b1</code><br>则由链式法则</p><script type="math/tex; mode=display">\frac{\partial L}{\partial w_1}=\frac{\partial L}{\partial z}\frac{\partial z}{\partial w_1}=\frac{\partial L}{\partial z}x_1</script><p>转化为求</p><script type="math/tex; mode=display">\frac{\partial L}{\partial z}</script><p>考虑该神经元的下一层，<code>z</code>经过激活后的输出<code>a</code>被当作下一层的输入：<br><img src="/2020/07/11/back-propagation-gradient-descent-improved/notions_backward_pass.png" alt="backward-pass"><br>则由链式法则，</p><script type="math/tex; mode=display">\frac{\partial L}{\partial z} = \frac{\partial L}{\partial a}\frac{\partial a}{\partial z}=\frac{\partial L}{\partial a}\sigma'(z)</script><p>观察到<code>a</code>已经变成了下一层网络的输入，问题又化为了最初求<code>loss</code>对网络输入的微分，但问题的规模-求梯度的“层数”却减少了一层。思维敏锐的同学可能已经发现了，这是分治法。</p><p>让我们更进一步</p><script type="math/tex; mode=display">\frac{\partial L}{\partial a}=\frac{\partial L}{\partial z'}\frac{\partial z'}{\partial a}+\frac{\partial L}{\partial z''}\frac{\partial z''}{\partial a}= \frac{\partial L}{\partial z'} w_3+ \frac{\partial L}{\partial z''} w_4</script><p>从而</p><script type="math/tex; mode=display">\frac{\partial L}{\partial z}=\sigma'(z) \left[\frac{\partial L}{\partial z'} w_3+ \frac{\partial L}{\partial z''} w_4 \right]</script><p><img src="/2020/07/11/back-propagation-gradient-descent-improved/amplify.png" alt="amplify"></p><p>看起来就像是一个以偏微分为输入,<code>y=σ&#39;(z)x</code>为激活函数的神经元的输出</p><p>考虑所有激活函数的输入<code>z</code>，当前层的偏微分<code>z</code>与后一层之间的偏微分<code>z&#39;</code>存在计算上的依赖关系。因此计算<code>loss</code>对<code>z</code>的偏微分时，要从后往前算。因此得名<code>backward pass</code>。<br><img src="/2020/07/11/back-propagation-gradient-descent-improved/dependencies.png" alt="dependencies"></p><p>而计算本层神经元的输入<code>a</code>时是从前往后算的，因此叫做<code>foreward pass</code></p><p><img src="/2020/07/11/back-propagation-gradient-descent-improved/sum.png" alt="sum"></p><p>总结一下，要计算L对参数<code>w</code>的偏微分需要进行两个步骤：</p><ol><li>foreward pass. 算出L对当前层神经元的输入<code>a</code>的偏微分。从前往后算。</li><li>backward pass. 计算出L对当前层激活函数的输入<code>z</code>的偏微分。从后往前算。</li></ol><p>两者相乘，即得</p><script type="math/tex; mode=display">\frac{\partial L}{\partial w}</script>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML&quot; async&gt;&lt;/script&gt;&lt;br&gt;本文是&lt;a href=&quot;http://speech.ee.ntu.edu.tw/~tlkagk/courses_ML20.html&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;NTU ML 2020&lt;/a&gt;中&lt;a href=&quot;https://youtu.be/Dr-WRlEFefw&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;BackPropagation&lt;/a&gt;部分的笔记。&lt;/p&gt;
&lt;p&gt;梯度下降是非常常用的优化算法。而在深度学习中，一个神经网络的参数动辄几百万，像线性模型那样人手工算好再告诉机器的方式已经不合适了，需要一种更高效的算法来计算梯度。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Machine learning" scheme="https://verrickt.github.io/categories/Machine-learning/"/>
    
    
      <category term="Machine learning" scheme="https://verrickt.github.io/tags/Machine-learning/"/>
    
      <category term="Backpropagation" scheme="https://verrickt.github.io/tags/Backpropagation/"/>
    
  </entry>
  
  <entry>
    <title>从分类到对率回归再到到神经网络</title>
    <link href="https://verrickt.github.io/2020/07/04/from-logistic-regression-to-neural-network/"/>
    <id>https://verrickt.github.io/2020/07/04/from-logistic-regression-to-neural-network/</id>
    <published>2020-07-04T14:01:07.000Z</published>
    <updated>2020-07-05T06:18:15.799Z</updated>
    
    <content type="html"><![CDATA[<p><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML" async></script></p><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>本文的是台湾大学2020机器学习的Classification<a href="https://youtu.be/fZAZUYEeIMg" target="_blank" rel="external">1</a>和<a href="https://youtu.be/hSXFuypLukA" target="_blank" rel="external">2</a>的笔记，奉行<a href="https://en.wikipedia.org/wiki/Lazy_evaluation" target="_blank" rel="external">Lazy evaluation</a>策略，对这些知识更深层次的探究只在<strong>绝对必要</strong>时完成。<br><a id="more"></a></p><h2 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h2><p>在分类问题，我们要找一个函数f，f的输入是代表样本的向量，输出是一个代表类别的标量。将样本分为两类的叫做二元分类，多于两类的叫做多元分类。我们先考虑二元分类问题。</p><h3 id="引例"><a href="#引例" class="headerlink" title="引例"></a>引例</h3><p>XCOM中面对的敌人有变种人(ADVENT)和外星人(Alien)两种，长老(The elder)送来了新的物种，现有训练资料变种人数据15组，外星人10组，你能据此帮助被关在外星人网络中充当首脑的人类指挥官分辨新物种的种类吗？</p><p>先考虑这个问题，蓝绿两个盒子，其中各有蓝绿球若干。从蓝盒子抽球的概率是1/3，从绿盒子抽球的概率是2/3。已知抽到了蓝球，问从蓝色盒子里抽到的概率是多少?<br><img src="/2020/07/04/from-logistic-regression-to-neural-network/two_boxes.png" alt="两个盒子"></p><script type="math/tex; mode=display">P(蓝盒子)=\frac{1}{3} \\P(绿盒子)=\frac{2}{3} \\P(抽到蓝球|蓝盒子)=\frac{4}{5} \\P(绿盒子|抽到蓝球)=\frac{2}{5} \\P(抽到蓝球)=P(蓝盒子)P(抽到蓝球|蓝盒子)+P(绿盒子)P(绿盒子|抽到蓝球)=\frac{1*4}{3*5}*\frac{2*2}{3*5}=\frac{8}{15} \\P(蓝盒子|抽到蓝球)=\frac{P(抽到蓝球|蓝盒子)P(蓝盒子)}{P(抽到蓝球)}=\frac{\frac{1*4}{3*5}}{\frac{8}{15}}=\frac{1}{2}</script><h3 id="贝叶斯公式"><a href="#贝叶斯公式" class="headerlink" title="贝叶斯公式"></a>贝叶斯公式</h3><p>我们预测的依据是贝叶斯公式：</p><script type="math/tex; mode=display">P(A|B)=\frac{P(B|A)P(A)}{P(B)}</script><p>用C1表示敌人属于变种人，C2表示敌人属于外星人，那么</p><script type="math/tex; mode=display">P(C_1|x)=\frac{P(x|C_1)P(C_1)}{P(x|C_1)P(C_1)+P({x|C_2})P(C_2))}</script><p>training set中一共有15组变种人，10组外星人。则</p><script type="math/tex; mode=display">P(C_1)=\frac{15}{15+10}=\frac{3}{5} \\P(C_2)=\frac{10}{15+10}=\frac{2}{5}</script><p>接下来是确定P(x|C1)。思考一下，testing-set上的数据我们的算法从来没有见过，那么P(x|C1)应该是0咯。可这样贝叶斯公式就变成0/0了，还怎么预测?</p><h3 id="P-C1-x"><a href="#P-C1-x" class="headerlink" title="P(C1|x)"></a>P(C1|x)</h3><p>在此，我们大胆假设，目前所见过的C1的example是由一个概率分布产生的，这样就可以对testing-set上没见过的数据了求概率了。</p><p><img src="/2020/07/04/from-logistic-regression-to-neural-network/prior.png" alt="先验"></p><p>这是课件的一张图，对于图中79个example，认为是一个Gaussian产生的，但好多个Gaussian都可以产生这样的点，具体是哪一个呢？这里使用极大似然的思想，就认为使得产生训练数据的概率最大的Gaussian好了</p><p>回到我们的例子，似然函数是</p><script type="math/tex; mode=display">L(\mu,\Sigma)=\prod_i^{15}f_{\mu,\Sigma}(x^i) \\f_{\mu,\Sigma}=\frac{1}{2\pi^{D/2}}\frac{1}{|\Sigma|^{1/2}}\exp{\{-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)\}}\\\mu^*,\Sigma^*=\argmax_{\mu,\Sigma} L(\mu,\Sigma)</script><p>经过一番运算后，得到μ和Σ的闭式解:</p><script type="math/tex; mode=display">\mu^*=\frac{1}{15}\sum_{n=1}^{15}x^n\Sigma^*=\frac{1}{15}\sum_{n=1}^{15}(x^n-\mu^*)(x^n-\mu^*)^T</script><p>这样就得到了C1的分布<code>G1(μ,Σ)</code>，同理可以得到C2的分布<code>G2(μ,Σ)</code>，有了这两项就可以计算条件概率从而完成预测了。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">if(x|C1)&gt;0.5 output ADVENT</div><div class="line">else output ALIEN</div></pre></td></tr></table></figure></p><p><img src="/2020/07/04/from-logistic-regression-to-neural-network/res_1.png" alt="结果"><br>回到上课的例子，这个模型的效果并不好:(。</p><p>想到<a href="../../../06/26/machine-learning-basics-probability-and-information-theory/">Probability</a>中Gaussion的PS</p><blockquote><p>如果用高斯分布来做分类问题，让分布共用协方差矩阵Σ通常效果会比使用各自的协方差矩阵效果好。</p></blockquote><p>那就试试共用协方差矩阵吧</p><h3 id="共用协方差矩阵"><a href="#共用协方差矩阵" class="headerlink" title="共用协方差矩阵"></a>共用协方差矩阵</h3><p>似然函数变为</p><script type="math/tex; mode=display">L(\mu_1,\mu_2,\Sigma)=\prod_i^{15}f_{\mu1,\Sigma}(x^i)\prod_{j=16}^{25}f_{\mu2,\Sigma}(x^i) \\</script><p><img src="/2020/07/04/from-logistic-regression-to-neural-network/res_2.png" alt="共用covariance-matrix"><br>求解过程省略，来看一下结果，准确度一下子就上来了。这时发现分界线变成了一条直线。这与线性模型有关系吗？推一下看看</p><h3 id="Why-linear-一些推导"><a href="#Why-linear-一些推导" class="headerlink" title="Why linear?一些推导"></a>Why linear?一些推导</h3><script type="math/tex; mode=display">P(C_1|x)=\frac{P(x|C_1)P(C_1)}{P(x|C_1)P(C_1)+P({x|C_2})P(C_2))} \\</script><p>上下同除以分子</p><script type="math/tex; mode=display">P(C_1|x)=\frac{1}{1+\frac{P({x|C_2})P(C_2)}{P(x|C_1)P(C_1)}}</script><script type="math/tex; mode=display">令z=\ln \frac{P({x|C_1})P(C_1)}{P(x|C_2)P(C_2)}，则P(C_1|x)=\frac{1}{1+\exp(-z)}=\sigma(z)</script><p>sigmoid函数就这样出现了!<br>继续分解z </p><script type="math/tex; mode=display">z=\ln \frac{P({x|C_1})P(C_1)}{P(x|C_2)P(C_2)}=\ln \frac{P({x|C_1})}{P(x|C_2)}+\ln\frac{P(C_1)}{P(C_2)}</script><script type="math/tex; mode=display">\lg \frac{P(C_1)}{P(C_2)}=\lg \frac{\frac{N_1}{N_1+N_2}}{\frac{N_2}{N_1+N_2}}=\lg \frac{N_1}{N2}</script><p>是个常数 </p><script type="math/tex; mode=display">P(x|C_1)=\frac{1}{2\pi^{D/2}}\frac{1}{|\Sigma|^{1/2}}\exp{\{-\frac{1}{2}(x-\mu^1)^T(\Sigma^1)^{-1}(x-\mu^1)\}}</script><script type="math/tex; mode=display">P(x|C_2)=\frac{1}{2\pi^{D/2}}\frac{1}{|\Sigma|^{1/2}}\exp{\{-\frac{1}{2}(x-\mu^2)^T(\Sigma^2)^{-1}(x-\mu^2)\}}</script><p>带入，则</p><script type="math/tex; mode=display">\ln \frac{P({x|C_1})}{P(x|C_2)} = \ln \frac{\frac{1}{2\pi^{D/2}}\frac{1}{|\Sigma|^{1/2}}\exp{\{-\frac{1}{2}(x-\mu^1)^T(\Sigma^1)^{-1}(x-\mu^1)\}}}{\frac{1}{2\pi^{D/2}}\frac{1}{|\Sigma|^{1/2}}\exp{\{-\frac{1}{2}(x-\mu^2)^T(\Sigma^2)^{-1}(x-\mu^2)\}}}</script><script type="math/tex; mode=display">=\ln \frac{|\Sigma^2|^{1/2}}{|\Sigma^1|^{1/2}} +\ln \exp\lgroup -\frac{1}{2}[(x-\mu^1)^T(\Sigma^1)^{-1}(x-\mu^1)-(x-\mu^2)^T(\Sigma^2)^{-1}(x-\mu^2)]\rgroup</script><script type="math/tex; mode=display">= \ln \frac{|\Sigma^2|^{1/2}}{|\Sigma^1|^{1/2}} +\lgroup -\frac{1}{2}[(x-\mu^1)^T(\Sigma^1)^{-1}(x-\mu^1)-(x-\mu^2)^T(\Sigma^2)^{-1}(x-\mu^2)]\rgroup</script><script type="math/tex; mode=display">= \ln \frac{|\Sigma^2|^{1/2}}{|\Sigma^1|^{1/2}} +\lgroup -\frac{1}{2}[x^T(\Sigma^1)^{-1}x-2(\mu^1)^T(\Sigma^1)^{-1}x+(\mu^1)^T(\Sigma^1)^{-1}\mu^1-x^T(\Sigma^2)^{-1}x+2(\mu^1)^T(\Sigma^2)^{-1}x-(\mu^2)^T(\Sigma^2)^{-1}\mu^2]\rgroup</script><script type="math/tex; mode=display">z= \ln \frac{|\Sigma^2|^{1/2}}{|\Sigma^1|^{1/2}} +\lgroup -\frac{1}{2}[x^T(\Sigma^1)^{-1}x-2(\mu^1)^T(\Sigma^1)^{-1}x+(\mu^1)^T(\Sigma^1)^{-1}\mu^1-x^T(\Sigma^2)^{-1}x+2(\mu^1)^T(\Sigma^2)^{-1}x-(\mu^2)^T(\Sigma^2)^{-1}\mu^2]\rgroup + \lg \frac{N_1}{N2}</script><p>共用协方差矩阵时，</p><script type="math/tex; mode=display">\Sigma^1=\Sigma^2=\Sigma</script><script type="math/tex; mode=display"> \ln \frac{|\Sigma^2|^{1/2}}{|\Sigma^1|^{1/2}} =0 x^T(\Sigma^1)^{-1}x - x^T(\Sigma^2)^{-1}x=0</script><p> 此时 </p><script type="math/tex; mode=display">z = (\mu^1-\mu^2)^T\Sigma^{-1}x-\frac{1}{2}(\mu^1)^T(\Sigma)^{-1}\mu^1+\frac{1}{2}(\mu^2)^T\Sigma^{-1}\mu^2+\ln \frac{N_1}{N2}</script><script type="math/tex; mode=display"> 令w^T=(\mu^1-\mu^2)^T\Sigma^{-1},b=-\frac{1}{2}(\mu^1)^T(\Sigma)^{-1}\mu^1+\frac{1}{2}(\mu^2)^T\Sigma^{-1}\mu^2+\ln \frac{N_1}{N2}</script><script type="math/tex; mode=display">则z=w^T+b</script><script type="math/tex; mode=display">P(C_1|x)= \sigma(z)</script><p> 是z的广义线性模型，给它一个名字，即logistic regression。</p><p> 由</p><script type="math/tex; mode=display">\mu^2,\mu^2,\Sigma^1,\Sigma^2</script><p> 得到w和b的模型称为生成模型(generative model)，因为各个x可以由一组分布生成出来。 直接找出w和b的模型称为判别模型(discriminative model)</p><h2 id="logistic-regression"><a href="#logistic-regression" class="headerlink" title="logistic regression"></a>logistic regression</h2><h3 id="Step1-find-a-function-set"><a href="#Step1-find-a-function-set" class="headerlink" title="Step1, find a function set"></a>Step1, find a function set</h3><p>logistic regression问题中，我们要找一组参数w和b，σ(wTx+b)给出的值是x属于正例的概率。</p><h3 id="Step2-determine-goodness-of-function"><a href="#Step2-determine-goodness-of-function" class="headerlink" title="Step2, determine goodness of function"></a>Step2, determine goodness of function</h3><p>这里使用似然函数<br><img src="/2020/07/04/from-logistic-regression-to-neural-network/goodness.png" alt="loss"></p><p>最小化对数似然函数(NLL)实际上就是最小化交叉熵</p><p><img src="/2020/07/04/from-logistic-regression-to-neural-network/cross_entropy.png" alt="cross_entropy"></p><h3 id="Step3-find-the-best-function"><a href="#Step3-find-the-best-function" class="headerlink" title="Step3, find the best function"></a>Step3, find the best function</h3><p>梯度下降走你</p><h3 id="Why-likelihood-instead-of-MSE"><a href="#Why-likelihood-instead-of-MSE" class="headerlink" title="Why likelihood instead of MSE?"></a>Why likelihood instead of MSE?</h3><p><img src="/2020/07/04/from-logistic-regression-to-neural-network/MSE.png" alt="MSE"></p><p>若使用MSE，则偏微分的值一直是0，无法进行梯度下降。<br><img src="/2020/07/04/from-logistic-regression-to-neural-network/cross_entropy_vs_MSE.png" alt="MSE&amp;cross entropy"><br>由图可见，在logistic regression这个问题上，交叉熵(极大似然)确实比MSE的梯度更大，更适合当作评价函数好坏的标准。</p><h3 id="多元分类"><a href="#多元分类" class="headerlink" title="多元分类"></a>多元分类</h3><p><img src="/2020/07/04/from-logistic-regression-to-neural-network/multi-class_classification.png" alt="Multiclass-classification"></p><h4 id="Step-1"><a href="#Step-1" class="headerlink" title="Step 1"></a>Step 1</h4><p>以三元分类为例，要做三元分类，要找三组参数</p><script type="math/tex; mode=display">C_1:w^1,b^1 \text{  } z_1=w^1x+b_1 \\C_2:w^2,b^2 \text{  } z_2=w^2x+b_2 \\C_3:w^3,b^3 \text{  } z_3=w^3x+b_3 \\\mathrm{softmax}: \vec{z} \in \mathbb{R}^n\to \vec{y} \in\mathbb{R}^n \\\text{}\\\\y_i = \frac{\exp{z_i}}{\sum_{i=1}^3}</script><h4 id="Step-2"><a href="#Step-2" class="headerlink" title="Step 2"></a>Step 2</h4><p><img src="/2020/07/04/from-logistic-regression-to-neural-network/multi_class_loss.png" alt="Multiclass-Loss"></p><p>把<code>z</code>经过<code>softmax</code>函数变换后得到的概率分布<code>y</code>与真实值计算交叉熵，即为需要最小化的函数</p><h4 id="Step-3"><a href="#Step-3" class="headerlink" title="Step 3"></a>Step 3</h4><p>梯度下降</p><h2 id="线性模型的限制"><a href="#线性模型的限制" class="headerlink" title="线性模型的限制"></a>线性模型的限制</h2><p>线性模型的函数是直线，而稍微复杂点的问题是无法用直线解决的。例如XOR问题</p><p><img src="/2020/07/04/from-logistic-regression-to-neural-network/XOR.png" alt="XOR"></p><h3 id="feature-transformation"><a href="#feature-transformation" class="headerlink" title="feature transformation"></a>feature transformation</h3><p><img src="/2020/07/04/from-logistic-regression-to-neural-network/feature_transformation.png" alt="feature transformation"></p><p>通过对feature做一些处理，是有可能突破线性模型的限制的。<br>但需要人工参与，而且没有一般规律。这就变成<em>人工</em>学习而不是<em>机器</em>学习了:(</p><h3 id="feature-transformation-with-linear-unit"><a href="#feature-transformation-with-linear-unit" class="headerlink" title="feature transformation with linear unit"></a>feature transformation with linear unit</h3><p><img src="/2020/07/04/from-logistic-regression-to-neural-network/cascading_logistic_regression.png" alt="cascading_logistic_regression"><br>但我们可以把feature transformation看作是一个线性的变化，这样就可以由logistic regression来解决。这样把多个logistic regression串起来，就可以对feature做复杂的变化。</p><p><img src="/2020/07/04/from-logistic-regression-to-neural-network/feature_transformation_with_linear_unit.png" alt="feature_transformation_with_linear_unit"><br>上图是使用两个logistic regression进行feature transformation的结果，只要再接上一个logistic regression接收他俩的输出就可以进行分类了。</p><h3 id="Neural-network"><a href="#Neural-network" class="headerlink" title="Neural network"></a>Neural network</h3><p>由此推广，把若干个线性模型串起来就可以实现很多复杂的功能。那么给logistic regression的单元起个新名字叫做神经元(neuron)，把它们互相连接形成的结构叫做神经网络(neural network)，瞬间就高大上起来了。</p><p>现在你可以骗麻瓜说「我们在模拟人类大脑的运作实现人工智慧」了;-)</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML&quot; async&gt;&lt;/script&gt;&lt;/p&gt;
&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;本文的是台湾大学2020机器学习的Classification&lt;a href=&quot;https://youtu.be/fZAZUYEeIMg&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;1&lt;/a&gt;和&lt;a href=&quot;https://youtu.be/hSXFuypLukA&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;2&lt;/a&gt;的笔记，奉行&lt;a href=&quot;https://en.wikipedia.org/wiki/Lazy_evaluation&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Lazy evaluation&lt;/a&gt;策略，对这些知识更深层次的探究只在&lt;strong&gt;绝对必要&lt;/strong&gt;时完成。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Machine learning" scheme="https://verrickt.github.io/categories/Machine-learning/"/>
    
    
      <category term="Machine learning" scheme="https://verrickt.github.io/tags/Machine-learning/"/>
    
      <category term="classification" scheme="https://verrickt.github.io/tags/classification/"/>
    
      <category term="logistic regression" scheme="https://verrickt.github.io/tags/logistic-regression/"/>
    
      <category term="neural network" scheme="https://verrickt.github.io/tags/neural-network/"/>
    
  </entry>
  
  <entry>
    <title>机器学习基础——纸上谈兵</title>
    <link href="https://verrickt.github.io/2020/07/03/machine-learning-in-pure-paperworks/"/>
    <id>https://verrickt.github.io/2020/07/03/machine-learning-in-pure-paperworks/</id>
    <published>2020-07-03T13:29:45.000Z</published>
    <updated>2020-07-04T07:52:17.877Z</updated>
    
    <content type="html"><![CDATA[<p><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML" async></script></p><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>本文的主体是Ian Goodfellow 「Deep Learning」第五章「MACHINE LEARNING BASICS」，未经动手实做，故称纸上谈兵。目前奉行<a href="https://en.wikipedia.org/wiki/Lazy_evaluation" target="_blank" rel="external">Lazy evaluation</a>，对相关知识的补充会在实践后进行。<br><a id="more"></a></p><h2 id="what-is-machine-learning"><a href="#what-is-machine-learning" class="headerlink" title="what is machine learning"></a>what is machine learning</h2><blockquote><p>A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E</p></blockquote><h3 id="The-Task-T"><a href="#The-Task-T" class="headerlink" title="The Task T"></a>The Task T</h3><ul><li>Classification</li><li>Regression</li><li>Structured output<h3 id="The-performance-measure-P"><a href="#The-performance-measure-P" class="headerlink" title="The performance measure P"></a>The performance measure P</h3></li></ul><p>Usually we are interested in how well the machine learning algorithm performs<br>on data that it has <strong>not</strong> seen before, since this determines how well it will work when deployed in the real world. We therefore evaluate these performance measures using a <strong>test set</strong> of data that is separate from the data used for training the machine learning system.</p><p>It’s often difficult to choose a performance measure that corresponds well to the desired behavior of the system for two reasons</p><ul><li>The performance measure can be difficult to decide. When performing a regression task, should we penalize the system more if it frequently makes medium-sized mistakes or if it rarely makes very large mistakes? These kinds of design choices depend on the application</li><li>we know what quantity we would ideally like to measure, but measuring it is impractical. Computing the actual probability value assigned to a specific point in space in many density estimation models is intractable</li></ul><h3 id="The-Experience-E"><a href="#The-Experience-E" class="headerlink" title="The Experience E"></a>The Experience E</h3><p>Machine learning algorithms can be broadly categorized as unsupervised or supervised by what kind of experience they are allowed to have during the learning process.</p><ul><li>Unsupervised learning algorithms experience a dataset containing many features, then learn useful properties of the structure of this dataset. In the context of deep learning, we usually want to learn the entire probability distribution that generated a dataset, whether explicitly as in density estimation or implicitly for tasks like synthesis or denoising. Some other unsupervised learning algorithms perform other roles, like clustering, which consists of dividing the dataset into clusters of similar examples.</li><li>Supervised learning algorithms experience a dataset containing features,but each example is also associated with a label or target. </li></ul><p>Roughly speaking, unsupervised learning involves observing several examples<br>of a random vector x, and attempting to implicitly or explicitly learn the probability distribution p(x), or some interesting properties of that distribution, while supervised learning involves observing several examples of a random vector x and an associated value or vector y, and learning to predict y from x, usually by estimating <code>p(y|x)</code> The term supervised learning originates from the view of the target y being provided by an instructor or teacher who shows the machine learning system what to do. In unsupervised learning, there is no instructor or teacher, and the algorithm must learn to make sense of the data without this guide</p><p>Some machine learning algorithms do not just experience a fixed dataset. For<br>example, <strong>reinforcement learning</strong> algorithms interact with an environment, so there is a feedback loop between the learning system and its experiences</p><h3 id="Example-Linear-regression"><a href="#Example-Linear-regression" class="headerlink" title="Example: Linear regression"></a>Example: Linear regression</h3><ul><li>The task T: linear regression solves a regression problem. The goal is to build a system that can take a vector x ∈ R^n as input and predict the value of a scalar y ∈ R as its output.</li></ul><script type="math/tex; mode=display">\hat{y}=w^{\top}x+b</script><p><strong>w</strong> is a vector of the weights over different features.</p><p><strong>b</strong> is the bias(not the bias in statistic)</p><p>Together ,<strong>w</strong> and <strong>b</strong> are called the <strong>parameters</strong></p><p><strong>y</strong> is the <strong>label</strong> of the data.</p><p><strong>y-hat</strong> is the prediction value</p><ul><li>The performance measurement P:</li></ul><p>mean squared error(MSE)</p><script type="math/tex; mode=display">\mathrm{MSE_{test}}=\frac{1}{m}\sum_i(\hat{y}^{(test)}-y^{(test)})</script><h2 id="Capacity-Overfitting-and-Underfitting"><a href="#Capacity-Overfitting-and-Underfitting" class="headerlink" title="Capacity, Overfitting and Underfitting"></a>Capacity, Overfitting and Underfitting</h2><p>The central challenge in machine learning is that we must perform well on <em>new, previously unseen inputs</em>—not just those on which our model was trained. The ability to perform well on previously unobserved inputs is called generalization.</p><p>The factors determining how well a machine learning algorithm will perform are its ability to:</p><ol><li>Make the training error small. </li><li>Make the gap between training and test error small.</li></ol><p>These two factors are underfitting and overfitting. Underfitting occurs when the model is not able to obtain a sufficiently low error value on the training set. Overfitting occurs when the gap between the training error and test error is too large.</p><p>Informally, a model’s capacity is its ability to fit a wide variety of functions. Models with low capacity may struggle to fit the training set. Models with high capacity can overfit by memorizing properties of the training set that do not serve them well on the test set. Usually,the error mainly comes from bias in the case of underfitting and variance in the case of overfitting.<br><img src="/2020/07/03/machine-learning-in-pure-paperworks/capacity.png" alt="Capacity"><br><img src="/2020/07/03/machine-learning-in-pure-paperworks/optimal_capacity.png" alt="Capacity,overfitting and underfitting"></p><h3 id="Bayes-error"><a href="#Bayes-error" class="headerlink" title="Bayes error"></a>Bayes error</h3><p>The error incurred by an oracle making predictions from the true distribution<br><code>p(x,y)</code> is called the Bayes error.</p><blockquote><p>Why are there errors if we know the true probability distribution ?</p></blockquote><p>because there may still be some noise in the distribution</p><h3 id="How-about-training-size"><a href="#How-about-training-size" class="headerlink" title="How about training size"></a>How about training size</h3><p>Training and generalization error vary as the size of the training set varies.<br>Expected generalization error can never increase as the number of training examples increases. For non-parametric models, more data yields better generalization until the best possible error is achieved. Any fixed parametric model with less than optimal capacity will asymptote to an error value that exceeds the Bayes error. See figure<br>5.4 for an illustration. Note that it is possible for the model to have optimal<br>capacity and yet still have a large gap between training and generalization error. In this situation, we may be able to reduce this gap by gathering more training examples.<br><img src="/2020/07/03/machine-learning-in-pure-paperworks/training_set_size_and_errors.png" alt="The effect of the training dataset size on the train and test error"></p><h3 id="Hyperparameters"><a href="#Hyperparameters" class="headerlink" title="Hyperparameters"></a>Hyperparameters</h3><p>hyperparameters are use to control the behavior of the learning algorithm.</p><p>Eg. In linear regression, we could use a model</p><script type="math/tex; mode=display">\hat{y}=b+\sum_i^kw_ix^i</script><p><strong>k</strong> controls degree of the polynomial,which acts as a <em>capacity hyperparameter</em></p><p>Why hyperparameters?</p><p>Sometimes a setting is chosen to be a hyperparameter that the learning algorithm does not learn because it is difficult to optimize. The setting must be a hyperparameter because it is not appropriate to learn that hyperparameter on the training set. This applies to all hyperparameters that control model capacity(e.g:<strong>k</strong>). If learned on the training set, such hyperparameters would always choose the maximum possible model capacity, resulting in overfitting</p><p>How to adjust hyperparameters?</p><p>Validation set!</p><h3 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a>Regularization</h3><p>We can give a learning algorithm a preference for one solution in its<br>hypothesis space to another. This means that both functions are eligible, but one is preferred.(即西瓜书提到的归纳假设)</p><p>For example, we can modify the training criterion for linear regression to include<br><strong>weight decay</strong></p><script type="math/tex; mode=display">J(w)=\mathrm{MSE_{train}}+\lambda w^\top w</script><p>where λ is a value chosen ahead of time that controls the strength of our preference for smaller weights. in this sense, λ is also a hyperparameter<br><img src="/2020/07/03/machine-learning-in-pure-paperworks/weight_decay.png" alt="weight_decay"></p><h2 id="Frequentist-vs-baysian"><a href="#Frequentist-vs-baysian" class="headerlink" title="Frequentist vs baysian"></a>Frequentist vs baysian</h2><ul><li>Frequentist  statistics:<br>we assume that the true parameter value θ is fixed but unknown, while the point estimate θ_hat is a function of the data. Since the data is drawn from a random process, any function of the data is random. Therefore θ_hat is a random variable</li><li>Bayesian Statistics:<br>the dataset is directly observed and so is not random. On the other hand, the true parameter θ is unknown or uncertain and thus is represented as a random variable. Before observing the data, we represent our knowledge of θ using <em>the prior probability distribution</em>, <strong>p(θ)</strong>. Generally, the machine learning practitioner selects a prior distribution that is quite broad (i.e. with high entropy) to reflect a high degree of uncertainty in the value of θ before observing any data</li></ul><h3 id="Maximum-likelihood"><a href="#Maximum-likelihood" class="headerlink" title="Maximum likelihood"></a>Maximum likelihood</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">最大似然估计：使得现有观测值出现概率最大的参数θ</div></pre></td></tr></table></figure><p>The maximum likelihood estimator for is defined as</p><script type="math/tex; mode=display">\theta_{ML}=\argmax _\theta\prod_{i=1}^mp_{model}(x^i;\theta)</script><p>log-likelihood estimator</p><script type="math/tex; mode=display">\theta_{ML}=\argmax _\theta\sum_{i=1}^m\log p_{model}(x^i;\theta)</script><p>观察KL-Divergence:</p><script type="math/tex; mode=display">D_{KL}(\hat{p}_{data}\|p_{model})=\mathbb{E}_{x\sim \hat{p}_{data}}[\log \hat{p}_{data}(x) - \log \hat{p}_{model}(x)]</script><p>为了最小化KL-Divergence，只需最小化</p><script type="math/tex; mode=display">- \mathbb{E}_{x\sim \hat{p}_{data}} \log \hat{p}_{model}(x)]</script><p>这与最大化log-likelihood是<strong>等价</strong>的</p><h2 id="Simple-machine-learning-algorithm"><a href="#Simple-machine-learning-algorithm" class="headerlink" title="Simple machine learning algorithm"></a>Simple machine learning algorithm</h2><h3 id="SVM"><a href="#SVM" class="headerlink" title="SVM"></a>SVM</h3><p>One of the most influential approaches to supervised learning is the support vector machine  This model is similar to logistic regression in that it is driven by a linear function </p><script type="math/tex; mode=display">y=w^\top x+b</script><p>. The SVM predicts that the positive class is present when y is positive. Likewise, it predicts that the negative class is present when y is negative.</p><p>One key innovation associated with support vector machines is the kernel trick. The kernel trick consists of observing that many machine learning algorithms can be written exclusively in terms of dot products between examples. For example, it can be shown that the linear function used by the support vector machine can be re-written as</p><script type="math/tex; mode=display">w^\top x+b=b+\sum_i^m \alpha_i x^\top x^{(i)}</script><p>where x^i is a training example and α is a vector of coefficients.Rewriting the learning algorithm this way allows us to replace x by the output of a given feature function φ(x) and the dot product with a function k(x,x^i) = φ(x)· φ(x^i) called a kernel.</p><p>The most commonly used kernel is the <strong>Gaussian kernel</strong></p><script type="math/tex; mode=display">k(u,v)=\mathbin{N}(u-v;0;\sigma^2I)</script><p>this kernel is also known as the radial basis function (RBF) kernel. The Gaussian kernel is performing a kind of template matching. A training example x associated with training label y becomes a template for class y. When a test point x’ is near x according to Euclidean distance, the Gaussian kernel has a large response, indicating that x’ is very similar to the x template. The model then puts a large weight on the associated training label y. Overall, the prediction will combine many such training labels weighted by the similarity of the corresponding training examples</p><h3 id="Gradient-descent-improved-stochastic-gradient-descent"><a href="#Gradient-descent-improved-stochastic-gradient-descent" class="headerlink" title="Gradient descent improved: stochastic gradient descent"></a>Gradient descent improved: stochastic gradient descent</h3><p>Idea:</p><ul><li>take a step over a ‘minibatch’ instead of observing the whole training-set</li><li>step cost does not depend on size of training set, thus achieving convergence much faster.</li></ul><p>A recurring problem in machine learning is that large training sets are necessary<br>for good generalization, but large training sets are also more computationally expensive.</p><p>the insight of stochastic gradient descent is that the gradient is an expectation.<br>The expectation may be approximately estimated using a small set of samples. Specifically, on each step of the algorithm, we can sample a <strong>minibatch</strong> of examples</p><p>For a fixed model size, the cost per SGD update does not depend on the training set size m. In practice, we often use a larger model as the training set size increases, but we are not forced to do so. The number of updates required to reach convergence usually increases with training set size. However, as m approaches infinity, the model will eventually converge to its best possible test error before SGD has sampled every example in the training set. Increasing m further will not extend the amount of training time needed to reach the model’s best possible test error. From this point of view, one can argue that the asymptotic cost of training a model with SGD is O(1) as a function of m</p><h2 id="Why-deep-learning"><a href="#Why-deep-learning" class="headerlink" title="Why deep learning?"></a>Why deep learning?</h2><h3 id="curse-of-dimensionality"><a href="#curse-of-dimensionality" class="headerlink" title="curse of dimensionality"></a>curse of dimensionality</h3><p>Many machine learning problems become exceedingly difficult when the number of dimensions in the data is high.</p><div class="table-container"><table><thead><tr><th style="text-align:center">dimension</th><th style="text-align:center">number of states</th></tr></thead><tbody><tr><td style="text-align:center">1</td><td style="text-align:center">n</td></tr><tr><td style="text-align:center">2</td><td style="text-align:center">n^2</td></tr><tr><td style="text-align:center">3</td><td style="text-align:center">n^3</td></tr></tbody></table></div><p>Traditional machine learning algorithm can’t distinguish a state that’s not seen in the training set.</p><h3 id="Local-Constancy-and-Smoothness-Regularization"><a href="#Local-Constancy-and-Smoothness-Regularization" class="headerlink" title="Local Constancy and Smoothness Regularization"></a>Local Constancy and Smoothness Regularization</h3><p>In order to generalize well, machine learning algorithms need to be guided by prior beliefs about what kind of function they should learn.</p><p>Among the most widely used of these implicit “priors” is the smoothness<br>prior or local constancy prior. This prior states that the function we learn should not change very much within a small region.<br>Many simpler algorithms rely exclusively on this prior to generalize well, and<br>as a result they <em>fail</em> to scale to the statistical challenges involved in solving AIlevel tasks.</p><h3 id="How-do-we-deal-with-curse-of-dimensionality"><a href="#How-do-we-deal-with-curse-of-dimensionality" class="headerlink" title="How do we deal with curse of dimensionality?"></a>How do we deal with curse of dimensionality?</h3><p>The key insight is that a very large number of regions, e.g., O(2^k), can be defined with O(k) examples, so long as we introduce some <strong>dependencies</strong> between the regions via additional assumptions about the underlying data generating distribution</p><p>The core idea in deep learning is that we assume that the data was generated by the composition of factors or features, potentially at multiple levels in a hierarchy.</p><h3 id="Manifold-Learning-amp-manifold-hypothesis"><a href="#Manifold-Learning-amp-manifold-hypothesis" class="headerlink" title="Manifold Learning &amp; manifold hypothesis"></a>Manifold Learning &amp; manifold hypothesis</h3><p>Manifold learning algorithms  assuming that most of R^n<br>consists of <strong>invalid inputs</strong>, and that interesting inputs occur only along<br>a collection of manifolds containing a small subset of points.</p><p>manifold hypothesis:</p><ul><li><p>probability distribution over images, text strings, and sounds that occur in real life is highly concentrated. </p></li><li><p>we can also imagine such neighborhoods and transformations, at least informally. In the case of images, we can certainly think of many possible transformations that allow us to trace out a manifold in image space: we can gradually dim or brighten the lights, gradually move or rotate objects in the image, gradually alter the colors on the surfaces of objects</p></li></ul><p>流形假说:</p><ol><li>大部分有结构的数据的分布函数并不是分散的，而是集中聚集在某些范围。<br>支持证据：随机取像素试图产生而期待其产生日常生活中的照片的概率是微乎其微的。随机取字母指望它生成一篇文章的概率也是微乎其微的。所以有结构的数据的分布函数必然是在某个范围内聚集，而在大部分区域分散<br><img src="/2020/07/03/machine-learning-in-pure-paperworks/noise.png" alt="noise"></li><li>在流形的表面移动，将可以得到流形所代表的全部数据。如在代表人脸的流形上移动，A点代表微笑的人，B点代表流泪的人。若从直线直接过去，则在A，B中间点所对应的数据可能不是人脸；若从流形的表面移动到B，则数据一直都是人脸。（微笑-&gt;皱眉-&gt;哭泣）</li></ol><p>请参考<a href="https://www.youtube.com/watch?v=BePQBWPnYuE" target="_blank" rel="external">Youtube视频：My understanding of the Manifold Hypothesis | Machine learning</a></p><iframe maxwidth="100%" width="560" height="315" src="https://www.youtube.com/embed/BePQBWPnYuE" style="max-width:100%" frameborder="0" allow="accelerometer; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML&quot; async&gt;&lt;/script&gt;&lt;/p&gt;
&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;本文的主体是Ian Goodfellow 「Deep Learning」第五章「MACHINE LEARNING BASICS」，未经动手实做，故称纸上谈兵。目前奉行&lt;a href=&quot;https://en.wikipedia.org/wiki/Lazy_evaluation&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Lazy evaluation&lt;/a&gt;，对相关知识的补充会在实践后进行。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Machine learning" scheme="https://verrickt.github.io/categories/Machine-learning/"/>
    
    
      <category term="Machine learning" scheme="https://verrickt.github.io/tags/Machine-learning/"/>
    
      <category term="The flower book" scheme="https://verrickt.github.io/tags/The-flower-book/"/>
    
  </entry>
  
  <entry>
    <title>机器学习基础——数值计算</title>
    <link href="https://verrickt.github.io/2020/06/28/machine-learning-basics-numerical-computation/"/>
    <id>https://verrickt.github.io/2020/06/28/machine-learning-basics-numerical-computation/</id>
    <published>2020-06-28T03:00:27.000Z</published>
    <updated>2020-07-04T07:51:28.575Z</updated>
    
    <content type="html"><![CDATA[<p><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML" async></script></p><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>本文的主体是机器学习中所用到的数值计算知识，因此奉行<a href="https://en.wikipedia.org/wiki/Lazy_evaluation" target="_blank" rel="external">Lazy evaluation</a>，对这些知识更深层次的探究只在<strong>绝对必要</strong>时完成。<br><a id="more"></a></p><h2 id="Source-of-errors"><a href="#Source-of-errors" class="headerlink" title="Source of errors"></a>Source of errors</h2><h3 id="Overflow-and-Underflow"><a href="#Overflow-and-Underflow" class="headerlink" title="Overflow and Underflow"></a>Overflow and Underflow</h3><p>计算机的内存是有限的，而有限的内存上无法实现无限精度的数值计算，因此数值计算是有误差的。</p><p>接近零的非零数值因为舍入变为0称为Underflow。</p><p>超出表示范围的wrap-around称为Overflow</p><ul><li>positive-overflow 指超过范围的正数被截断为负数</li><li>negative-overflow 指超过范围的负数被截断为正数</li></ul><h3 id="Poor-Conditioning"><a href="#Poor-Conditioning" class="headerlink" title="Poor Conditioning"></a>Poor Conditioning</h3><p>Condition指输入的微小变化引起函数值变化的剧烈程度。计算机中的数字是有表示范围的，因此Condition大的函数会遇到很多问题。</p><p>对于</p><script type="math/tex; mode=display">f(\mathbf{x})=A_{-1}\mathbf{x},A \in \mathbb{R}^{nxn}</script><p>如果A可以做特征值分解，那么f的condition number定义为</p><script type="math/tex; mode=display">\max{i,j}\left|\frac{\lambda_i}{\lambda_j}\right|</script><h2 id="Optimization"><a href="#Optimization" class="headerlink" title="Optimization"></a>Optimization</h2><p>解出使函数f(x)取得最大值/最小值的x的过程称为优化(optimization)。求f(x)最大值可以用求-f(x)的最小值实现，因此主要讨论求最小值。</p><h3 id="naive-gradient-descent"><a href="#naive-gradient-descent" class="headerlink" title="naive gradient descent"></a>naive gradient descent</h3><p>梯度(gradient)给出了函数增长最快的方向，那么沿着梯度的反方向就可以将函数值减少。梯度下降法(gradient descent)就是采用这个思路一种方法:</p><script type="math/tex; mode=display">x^{i+1} = x^{i} - \epsilon \nabla_xf(x)</script><p>其中ε是学习率(learning rate)。一般来说ε有两种取法</p><ul><li>设为一个固定的很小的常量</li><li>取多个ε，选其中让f(x-ε∇f(x))最小的那个ε。这种方法又称为line search</li></ul><h3 id="Beyond-the-Gradient-Jacobian-and-Hessian-Matrices"><a href="#Beyond-the-Gradient-Jacobian-and-Hessian-Matrices" class="headerlink" title="Beyond the Gradient: Jacobian and Hessian Matrices"></a>Beyond the Gradient: Jacobian and Hessian Matrices</h3><p>对于一个输入和输出都是向量的函数f，含有它的所有偏微分的矩阵称为雅可比矩阵(Jacobian matrix)</p><script type="math/tex; mode=display">f:\mathbb{R^m} \to \mathbb{R^n}</script><script type="math/tex; mode=display">J_{i,j}=\frac{\partial}{\partial x_j}f(x)_i 其中J \in \mathbb{R^{n\times m}}</script><p>同样的，含有f的所有二阶偏微分的矩阵称为海希矩阵(Hessian matrix)</p><script type="math/tex; mode=display">\mathbf{}{H}(f)(x)_{i,j}=\frac{\partial ^2}{\partial x_i \partial x_j}f(x)</script><p>海希矩阵就是梯度的雅可比矩阵。</p><p>当二阶偏微分连续时，微分运算符具有交换律，即</p><script type="math/tex; mode=display">H_{i,j}=H_{j,i}</script><p>机器学习中大部分函数f都具有连续的二阶偏微分，因此海希矩阵是实对称矩阵，因而可以做特征值分解，并且U是正交矩阵。</p><p>f对某个单位向量u方向u的二阶偏微分由</p><script type="math/tex; mode=display">u^{\top} Hu</script><p>给出。</p><p>在x0处对f做二阶泰勒展开</p><script type="math/tex; mode=display">f(x)\approx f(x^0)+(x-x^0)^{\top}g+\frac{1}{2}(x-x^0)^\top H(x-x^0)</script><p>H为海希矩阵，g为梯度。当采用学习率ε时，梯度下降的新坐标是x0-εg，带入泰勒展开，有</p><script type="math/tex; mode=display">f(x^0-\epsilon g) \approx f(x^0) - \epsilon g^{\top}g+\frac{1}{2}\epsilon^2g^{\top}Hg</script><p>当最后一项小于等于零时，ε可以任选。当最后一项大于零时，有</p><script type="math/tex; mode=display">\epsilon^*=\frac{g^{\top}g}{g^{\top}Hg}</script><p>时下降最快。最坏情况下，g与H重合，因此学习率的数量级由</p><script type="math/tex; mode=display">\frac{1}{\lambda_{\max}}</script><p>决定。</p><p>学习率设的过大或过小都会造成问题，这可以由海希矩阵解决。其中最简单的就是<strong>牛顿法</strong></p><p>牛顿法的步骤基本思路是，在x点做二阶泰勒展开。</p><script type="math/tex; mode=display">f(x)\approx f(x^0)+(x-x^0)^{\top}g+\frac{1}{2}(x-x^0)^\top H(x-x^0)</script><p>解出函数的critical point</p><script type="math/tex; mode=display">x^*=x^0-H(f)(x^0)^{-1}\nabla_xf(x^0)</script><p>f是正定二次函数时，一次就找到了最低点。当f不是二次函数但是可以用二次函数局部近似时，多次迭代x*即可。</p><p>牛顿法只适用于解最小值，而梯度下降没有这个限制。</p><p>只用到梯度的优化算法，如梯度下降，叫做一阶优化算法。使用海希矩阵的算法，如牛顿法，叫做二阶优化算法。</p><h3 id="Constrained-optimization"><a href="#Constrained-optimization" class="headerlink" title="Constrained optimization"></a>Constrained optimization</h3><p>有时我们希望在某集合S上而不是R^n上找出函数的最大/最小值，这叫做constrainted optimization(受限优化)。落在s中的点称为feasible point(可行点)</p><p>KKT方法是一个受限优化问题的通用方法。使用KKT方法，首先定义广义拉格朗日函数。</p><p>为了定义广义拉格朗日函数，首先需要将S定义为函数的集合</p><script type="math/tex; mode=display">\mathbb{S}=\{x| \forall i,g^{(i)}(x)=0 \text{ and } \forall j,h^{(j)}(x) \le0 \}</script><p>含有g(i)的方程称为equality constraints，含有h(j)的方程称为inequality constraints。</p><p>对于每个函数，定义λi和αj。他们叫做KKT乘数。</p><p>现在定义广义拉格朗日函数</p><script type="math/tex; mode=display">L(x,\lambda,\alpha)=f(x)+\sum_i \lambda_i g^{(i)}(x)+\sum_j \alpha_jh^{(j)}(x)</script><p>接下来在广义拉格朗日函数上使用非受限优化算法，就可以解决受限优化算法了。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML&quot; async&gt;&lt;/script&gt;&lt;/p&gt;
&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;本文的主体是机器学习中所用到的数值计算知识，因此奉行&lt;a href=&quot;https://en.wikipedia.org/wiki/Lazy_evaluation&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Lazy evaluation&lt;/a&gt;，对这些知识更深层次的探究只在&lt;strong&gt;绝对必要&lt;/strong&gt;时完成。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Machine learning" scheme="https://verrickt.github.io/categories/Machine-learning/"/>
    
    
      <category term="Machine learning" scheme="https://verrickt.github.io/tags/Machine-learning/"/>
    
      <category term="The flower book" scheme="https://verrickt.github.io/tags/The-flower-book/"/>
    
      <category term="Numberical computation" scheme="https://verrickt.github.io/tags/Numberical-computation/"/>
    
  </entry>
  
  <entry>
    <title>机器学习基础——概率论与信息论</title>
    <link href="https://verrickt.github.io/2020/06/26/machine-learning-basics-probability-and-information-theory/"/>
    <id>https://verrickt.github.io/2020/06/26/machine-learning-basics-probability-and-information-theory/</id>
    <published>2020-06-26T14:25:05.000Z</published>
    <updated>2020-06-27T12:01:07.788Z</updated>
    
    <content type="html"><![CDATA[<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML" async></script><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>本文的主体是机器学习中所用到的概率论知识，因此奉行<a href="https://en.wikipedia.org/wiki/Lazy_evaluation" target="_blank" rel="external">Lazy evaluation</a>，对这些知识更深层次的探究只在<strong>绝对必要</strong>时完成。本文假设你学过《概率论与数理统计》，仅指出花书的现有定义的不同，同时补充花书中特有的知识<br><a id="more"></a></p><h2 id="Probability"><a href="#Probability" class="headerlink" title="Probability"></a>Probability</h2><h3 id="Conditional-probability"><a href="#Conditional-probability" class="headerlink" title="Conditional probability"></a>Conditional probability</h3><script type="math/tex; mode=display">P(A|B)=\frac{P(AB)}{P(B)}</script><p>Chain rule/product rule of conditiona probability</p><script type="math/tex; mode=display">P(x^{(1)},...,x^{(n)})=P(x^{(1)})\prod_{i=2}^{n}P(x^{1}|x^{(1)},...,x^{(n)})</script><p>eg</p><script type="math/tex; mode=display">P(a,b,c)=P(a|b,c)P(b,c)</script><script type="math/tex; mode=display">P(b,c)=P(b|c)P(c)</script><script type="math/tex; mode=display">P(a,b,c)=P(a|b,c)P(b|c)P(c)</script><h3 id="Independence"><a href="#Independence" class="headerlink" title="Independence"></a>Independence</h3><p>Indenpendence:</p><script type="math/tex; mode=display">\forall x \in x,y \in y,p(x=x,y=y)=p(x=x)p(y=y)</script><p>则x,y互相独立。记作</p><script type="math/tex; mode=display">x \perp y</script><p>Conditional independent:</p><script type="math/tex; mode=display">\forall x \in x,y \in y ,z\in z,p(x=x,y=y|z=z)=p(x=x|z=z)p(y=y|z=z)</script><p>称x,y在z下相互独立，记作</p><script type="math/tex; mode=display">x \perp y \mid z</script><h3 id="Covariance"><a href="#Covariance" class="headerlink" title="Covariance"></a>Covariance</h3><script type="math/tex; mode=display">Cov(f(x),g(y))=\mathbb{E}[(f(x))-\mathbb{E}[f(x)])(g(x))-\mathbb{E}[g(x)]]</script><p>协方差值越高，意味着f和g的变化非常大，并且同时距离各自的均值很远。如果协方差是正值，那么f和g倾向于同时相对大的值。如果是负值，则一个取高值的同时另一个取低值。</p><p>向量</p><script type="math/tex; mode=display">x \in \mathbb{R}^n</script><p>的covariance matrix(协方差矩阵)是一个<code>nxn</code>的方阵，其中</p><script type="math/tex; mode=display">Cov(x)_{i,j}=Cov(x_i,x_j)</script><p>对于对角线的元素，</p><script type="math/tex; mode=display">Cov(x_i,x_i)=Var(x_i)</script><h3 id="Gaussian-Distribution"><a href="#Gaussian-Distribution" class="headerlink" title="Gaussian Distribution"></a>Gaussian Distribution</h3><p>高斯分布，又称正态分布。<br>概率密度函数(PDF):</p><script type="math/tex; mode=display">\mathcal{N}(x;\mu,\sigma)=\sqrt{\frac{1}{2 \pi \sigma^2}}exp(-\frac{1}{2\sigma^2}(x-\mu)^2)</script><p>计算概率密度时，经常要取σ的平方倒数，工程中常使用另一个参数β∈(0,∞)表示高斯分布的<strong>精准度</strong></p><script type="math/tex; mode=display">\mathcal{N}(x;\mu,\beta^{-1})=\sqrt{\frac{\beta}{2 \pi \sigma^2}}exp(-\frac{1}{2}\beta(x-\mu)^2)</script><p>高斯分布的特点</p><ul><li>现实中很多复杂的系统可以由高斯分布建模（<strong>中心极限定理</strong>）</li><li>在方差相同的所有分布中，高斯分布在实数范围上的“不确定度”最高。换句话说，高斯分布是所有分布中对样本做出最少先验假设的</li></ul><p>N维高斯分布：</p><script type="math/tex; mode=display">\mathcal{N}(x;\mu,\Sigma)=\sqrt{\frac{1}{(2\pi)^n det(\Sigma)}}exp(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu))</script><p>其中Σ是正定对称矩阵。μ是矢量形式的分布均值，Σ给出分布的协方差矩阵。为了便于计算，对于N维高斯分布，常用<strong>准确度矩阵β</strong>作为参数：</p><script type="math/tex; mode=display">\mathcal{N}(x;\mu,\beta^{-1})=\sqrt{\frac{det(\beta)}{(2\pi)^n }}exp(-\frac{1}{2}(x-\mu)^T\beta(x-\mu))</script><p>实践上通常将协方差矩阵固定为对角阵。更简单的方式是将<strong>isotropic matrix</strong>作为协方差矩阵，其中<strong>isotropic matrix</strong>指标量数乘单位矩阵的结果。</p><h3 id="Dirac-delta-distribution-amp-empirical-distribution"><a href="#Dirac-delta-distribution-amp-empirical-distribution" class="headerlink" title="Dirac delta distribution &amp; empirical distribution"></a>Dirac delta distribution &amp; empirical distribution</h3><p>有时我们希望所有的概率密度都聚集在一个点附近。这可以通过Dirac delta函数</p><script type="math/tex; mode=display">$\delta(x)$$$实现：$$p(x)=\delta(x-\mu)</script><p>Dirac delta分布常常被用作empirical distribution（经验分布）的一个组件：</p><script type="math/tex; mode=display">\hat{p}(x)=\frac{1}{m}\sum_{i=1}^m\delta(x-x^{(i)})</script><p>empirical distribution在全部m个点</p><script type="math/tex; mode=display">x^{(1)},...x^{(m)}</script><p>上放置</p><script type="math/tex; mode=display">\frac{1}{m}</script><p>概率密度</p><h3 id="Mixture-distribution"><a href="#Mixture-distribution" class="headerlink" title="Mixture distribution"></a>Mixture distribution</h3><p>用其他简单的概率分布来定义概率分布是十分普遍的，混合分布（mixture distribution)就是这样一种方式。混合分布由好几个组件(component)组成。每次采样时，由一个多重分布的结果选择组件标识(component identity)，由此最终结果是由哪一个分布给出的。</p><script type="math/tex; mode=display">P(x)=\sum_iP(c=i)P(x\mid c=i)</script><p>其中P(c)是所有组件上的多重分布。</p><p>一种常见且强大的混合模型是高斯混合模型。高斯混合模型所有组件都是高斯分布，他们具有不同的参数<code>μ</code>和<code>Σ</code>。有些分布可以增加限制，如所有组件共用协方差矩阵等。</p><p>高斯混合分布是概率密度的通用近似方式。具有足够分量的高斯混合模型可以用任何特定的非零误差量来近似任何平滑密度。</p><p>PS，如果用高斯分布来做分类问题，让两个分布共用协方差矩阵Σ通常效果会比使用各自的协方差矩阵效果好。</p><h3 id="Latent-variable"><a href="#Latent-variable" class="headerlink" title="Latent variable"></a>Latent variable</h3><p>隐含变量（Latent variable)指无法直接观测的随机变量。混合分布中的组件标识变量c就是隐含变量。</p><script type="math/tex; mode=display">P(x,c)=P(x\mid c)P(c)</script><p>隐含变量的分布<code>P(c)</code>和条件分布<code>P(x|c)</code>共同决定了<code>P(x)</code>的分布。尽管<code>P(x)</code>可以在没有隐含变量的条件下被计算出来。</p><h3 id="Useful-properties-of-Common-Functions"><a href="#Useful-properties-of-Common-Functions" class="headerlink" title="Useful properties of Common Functions"></a>Useful properties of Common Functions</h3><p>logistic sigmoid:</p><script type="math/tex; mode=display">\sigma(x)=\frac{1}{1+exp(-x)}</script><p>softplus:</p><script type="math/tex; mode=display">\zeta(x)=log(1+exp(x))</script><blockquote><p>why the name softplus?<br>It’s a “softeded” version of x^=max(0,x)</p></blockquote><h3 id="Bayes’-Rule"><a href="#Bayes’-Rule" class="headerlink" title="Bayes’ Rule"></a>Bayes’ Rule</h3><p>已知<code>P(y|x)</code>和<code>P(x)</code>求<code>P(x|y)</code>时可以使用贝叶斯公式：</p><script type="math/tex; mode=display">P(x|y)=\frac{P(x)P(y \mid x)}{P(y)}</script><p>其中</p><script type="math/tex; mode=display">P(y)=\sum_xP(y \mid x)P(x)</script><h3 id="prior-probability-and-posterior-probability"><a href="#prior-probability-and-posterior-probability" class="headerlink" title="prior-probability and posterior-probability"></a>prior-probability and posterior-probability</h3><ul><li>prior-probability<br>即先验概率。指根据以往经验和分析得到的概率</li><li>posterior-probability<br>后验概率是在考虑和给出相关证据或数据后所得到的条件概率</li></ul><p>考虑bayes’ Rule</p><script type="math/tex; mode=display">P(\theta \mid x)=\frac{P(x\mid\theta)P(\theta)}{P(x)}</script><ul><li>θ：parameter</li><li>x：observed value</li><li>P(x)：evidence</li><li>P(θ)：prior</li><li>P(x|θ)：likelihood</li><li>P(θ|x)：posterior</li></ul><h3 id="PDF-of-y-where-y-g-x"><a href="#PDF-of-y-where-y-g-x" class="headerlink" title="PDF of y where y=g(x)"></a>PDF of y where y=g(x)</h3><p>假设有现随机变量x，y，其中y=g(x)，求y的PDF</p><p>PDF根据pdf的定义,</p><script type="math/tex; mode=display">P(x)_{x \in \delta}=\int_{x}p(x)dx</script><p>p(x)dx为x落在某一邻域δ内的概率。现保留该属性，则有</p><script type="math/tex; mode=display">|p_y(g(x))dy|=|p_x(x)dx|</script><script type="math/tex; mode=display">p_y(y)=p_x(g^{-1}(y))\mid \frac{\partial x}{\partial y} \mid</script><script type="math/tex; mode=display">p_x(x)=p_y(g(x))\mid \frac{\partial g(x)}{\partial y} \mid</script><p>考虑高维情况，x与y为向量，定义雅可比矩阵J，其中</p><script type="math/tex; mode=display">J_{i,j}=\frac{\partial x_i}{\partial y_j}</script><p>则</p><script type="math/tex; mode=display">p_x(x)=p_y(g(x))\mid \det \left\{ \frac{\partial g(x)}{\partial x}\right\} \mid</script><h2 id="Information-theory"><a href="#Information-theory" class="headerlink" title="Information theory"></a>Information theory</h2><p>basic assemption:</p><ul><li>Likely events should have low information content, and in the extreme case, events that are guaranteed to happen should have no information content whatsoever.<ul><li>Less likely events should have higher information content.</li><li>Independent events should have additive information. For example, finding out that a tossed coin has come up as heads twice should convey twice as much information as finding out that a tossed coin has come up as heads once.</li></ul></li></ul><h3 id="Self-information-amp-Shanon-entropy"><a href="#Self-information-amp-Shanon-entropy" class="headerlink" title="Self-information &amp; Shanon entropy"></a>Self-information &amp; Shanon entropy</h3><p>单一事件所含的信息，单位为nat</p><script type="math/tex; mode=display">I(x)=-\log P(x)</script><p>整个概率分布上的不确定性，即香农熵</p><script type="math/tex; mode=display">H(x)=E_{x\sim P}[I(x)]=-\mathbb{E}_{x\sim P}[\log P(x)]</script><p>也记作H(P).</p><p>当P(x)和Q(x)为相同随机变量x的分部时，两种分布间的“距离”用<br>Kullback-Leibler (KL) divergence定义：</p><script type="math/tex; mode=display">D_{KL}(P\|Q)=\mathbb{E}_{x\sim P}\left[\log\frac{P(x)}{Q(x)}\right]=\mathbb{E}_{x\sim p}[\log P(x) - \log Q(x)]</script><p>the value means the extra amount of information<br>needed to send a message containing symbols drawn from probability distribution P, when we use a code that was designed to minimize the length of messages drawn from probability distribution .</p><p>KL divergence</p><ul><li>non-negarive</li><li>not symmetric<script type="math/tex; mode=display">D_{KL}(P\|Q)\not ={D_{KL}(Q\|P)}</script></li></ul><p>与KL divergence密切相关的一种度量是cross-entropy<br>定义为</p><script type="math/tex; mode=display">H(P,Q) = H(P)+D_{KL}(P\|Q)=-\mathbb{E}_{x\sim P}\log Q(x)</script><p>Minimizing the cross-entropy with respect to Q is equivalent to minimizing the KL divergence, because Q does not participate in the omitted term.</p><h3 id="Structed-Probabilistic-Models"><a href="#Structed-Probabilistic-Models" class="headerlink" title="Structed Probabilistic Models"></a>Structed Probabilistic Models</h3><p>机器学习中的参数成千上万，使用含有这么多参数的分布不切实际。根据条件概率的乘法公式，可以把大分布拆成小分布的乘积（这一过程叫factorization)。当使用CS中的图来表示这种<code>factorization</code>时，就把模型称为structured probabilistic model或graphical model。structed probabilistic model分为两类，分别使用DAG和UAG。</p><h4 id="DAG"><a href="#DAG" class="headerlink" title="DAG"></a>DAG</h4><p>Directed models use graphs with directed edges, and they represent factorizations into conditional probability distributions,  Specifically, a directed model contains one factor for every random variable xi in the distribution</p><script type="math/tex; mode=display">p(\mathbf{x})=\prod_ip(x_i|Pa\mathcal{G}(x_i))</script><p>where</p><script type="math/tex; mode=display">Pa\mathcal{G}(x_i)</script><p>is the parents of xi.</p><h4 id="UAG"><a href="#UAG" class="headerlink" title="UAG"></a>UAG</h4><p>Undirected models use graphs with undirected edges, and they represent<br>factorizations into a set of functions; unlike in the directed case, these functions are usually not probability distributions of any kind. <strong>Any</strong> set of nodes that are all <strong>connected to each other</strong> in G is called a <strong><a href="https://en.wikipedia.org/wiki/Clique_(graph_theory)" target="_blank" rel="external">clique</a></strong>. Each clique Ci in an undirected model is associated with a factor φi . These factors are just functions, not probability distributions. The output of each factor must be non-negative, but there is no constraint that the factor must sum or integrate to 1 like a probability distribution.</p><p>The probability of a configuration of random variables is proportional to the<br>product of all of these factors—assignments that result in larger factor values are more likely. Of course, there is no guarantee that this product will sum to 1. We therefore divide by a normalizing constant Z, defined to be the sum or integral over all states of the product of the φ functions, in order to obtain a normalized probability distribution:</p><script type="math/tex; mode=display">p(x)=\frac{1}{Z}\prod_i\phi^{(i)}(\mathcal{C}^{(i)})</script><p>DAG和UAG都是描述概率分布的方法，他们并不是互斥的概率分布。使用DAG还是UAG并不是概率分布的属性，而是某种特定<strong>描述方式</strong>的属性</p>]]></content>
    
    <summary type="html">
    
      &lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML&quot; async&gt;&lt;/script&gt;

&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;本文的主体是机器学习中所用到的概率论知识，因此奉行&lt;a href=&quot;https://en.wikipedia.org/wiki/Lazy_evaluation&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Lazy evaluation&lt;/a&gt;，对这些知识更深层次的探究只在&lt;strong&gt;绝对必要&lt;/strong&gt;时完成。本文假设你学过《概率论与数理统计》，仅指出花书的现有定义的不同，同时补充花书中特有的知识&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Machine learning" scheme="https://verrickt.github.io/categories/Machine-learning/"/>
    
    
      <category term="Machine learning" scheme="https://verrickt.github.io/tags/Machine-learning/"/>
    
      <category term="The flower book" scheme="https://verrickt.github.io/tags/The-flower-book/"/>
    
      <category term="Probability" scheme="https://verrickt.github.io/tags/Probability/"/>
    
      <category term="Information theory" scheme="https://verrickt.github.io/tags/Information-theory/"/>
    
  </entry>
  
  <entry>
    <title>机器学习基础——线性代数</title>
    <link href="https://verrickt.github.io/2020/06/24/machine-learning-basics-linear-algebra/"/>
    <id>https://verrickt.github.io/2020/06/24/machine-learning-basics-linear-algebra/</id>
    <published>2020-06-24T01:38:56.000Z</published>
    <updated>2020-06-27T12:00:34.274Z</updated>
    
    <content type="html"><![CDATA[<p><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML" async></script></p><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>本文的主体是机器学习中所用到的线性代数知识，因此奉行<a href="https://en.wikipedia.org/wiki/Lazy_evaluation" target="_blank" rel="external">Lazy evaluation</a>，对这些知识更深层次的探究只在<strong>绝对必要</strong>时完成。<br><a id="more"></a></p><h2 id="Concepts"><a href="#Concepts" class="headerlink" title="Concepts"></a>Concepts</h2><h3 id="Tensor"><a href="#Tensor" class="headerlink" title="Tensor"></a>Tensor</h3><p>矩阵是向量的延申，而Tensor(张量)则是矩阵的延申。<br>向量可用一维数组表示，矩阵可用二维数组表示，而张量可用三维数组表示。<br>表示张量A的一个分量</p><script type="math/tex; mode=display">A_{i,j,k}</script><h3 id="Identify-matrix"><a href="#Identify-matrix" class="headerlink" title="Identify matrix"></a>Identify matrix</h3><p>Identity matrix为单位矩阵的另一名称。一般用I表示，而单位矩阵用E表示。</p><h3 id="matrix-inversion"><a href="#matrix-inversion" class="headerlink" title="matrix inversion"></a>matrix inversion</h3><p>花书定义：</p><script type="math/tex; mode=display">A^{-1}A=I</script><p>并<strong>未限定</strong>A为方阵，并且是左逆</p><p>花书也定义了右逆</p><script type="math/tex; mode=display">AA^{-1}=I</script><p>同样<strong>未限定</strong>A为方阵</p><p>而后说明，<strong>方阵</strong>的左逆和右逆是相同的</p><blockquote><p>For square matrices, the left inverse and right inverse are equal</p></blockquote><p>而课本认为<strong>非方阵</strong>没有逆矩阵。对左逆和右逆也不做显式区分。</p><p>在花书接下来提到逆矩阵时，这可能是一个坑点</p><h3 id="Singular-matrix"><a href="#Singular-matrix" class="headerlink" title="Singular matrix"></a>Singular matrix</h3><p>非<strong>满秩</strong>的<strong>方阵</strong>称为奇异矩阵。</p><h3 id="Norms"><a href="#Norms" class="headerlink" title="Norms"></a>Norms</h3><p>又称范式。范式是一个将向量映射为非负数量的函数，用以描述向量的“大小”。</p><p>正式定义：</p><ul><li>f(x) = 0 ⇒ x = 0</li><li>f(x+y) ≤ f(x) + f(y)(三角不等式)</li><li>∀α∈R,f(αx) = |α|f(x)</li></ul><p>常用的范式:</p><ul><li>L2 norm<br>向量与原点的距离</li></ul><script type="math/tex; mode=display">||x||_2=(\sum_{i} |x_i|^2)^{\frac{1}{2}}</script><ul><li><p>Squared L2 norm<br>L2 norm的平方形式，某些情况下易于分析和计算</p></li><li><p>L1 norm</p><script type="math/tex; mode=display">||x||_1=\sum_{i} |x_i|</script></li><li><p>max norm</p><script type="math/tex; mode=display">||x||_{\infty }={\max} |x_i|</script></li><li><p>Frobenius norm<br>计算矩阵的“大小”</p></li></ul><script type="math/tex; mode=display">||A||_F=\sqrt{(\sum_{i,j} A_{i,j}^2)}</script><h2 id="Operations"><a href="#Operations" class="headerlink" title="Operations"></a>Operations</h2><h3 id="Decomposition"><a href="#Decomposition" class="headerlink" title="Decomposition"></a>Decomposition</h3><p>将矩阵分解为某些更基础的成分可以更好的帮助我们分析其中的一般规律。</p><h4 id="Eigendecomposition"><a href="#Eigendecomposition" class="headerlink" title="Eigendecomposition"></a>Eigendecomposition</h4><p>基于特征值和特征向量分解。与课本相同，不再赘述。</p><h4 id="Singular-value-decomposition-SVD"><a href="#Singular-value-decomposition-SVD" class="headerlink" title="Singular value decomposition(SVD)"></a>Singular value decomposition(SVD)</h4><p>又称奇异值分解。对矩阵的形状没有要求，且任意实矩阵都可以做奇异值分解，因而比特征值分解更具通用性。</p><p>对于一个<code>m*n</code>的矩阵A，奇异值分解希望将其分解为三个矩阵的乘积：</p><script type="math/tex; mode=display">A=UDV^T</script><ul><li>U是大小<code>m*m</code>的正交矩阵</li><li>D是大小<code>m*n</code>的对角矩阵</li><li>V是大小<code>n*n</code>的正交矩阵</li></ul><p>D中对角线的元素称为A的奇异值。U的列向量称为A的<strong>左</strong>奇异向量，V的列向量称为A的<strong>右</strong>奇异向量</p><p>A的奇异值分解可以理解为「A的函数」的特征值分解：</p><ul><li>A的左奇异值向量是AA^T的特征向量。</li><li>A的右奇异值向量是A^TA的特征向量</li><li>A的非零奇异值是A^TA的特征值，也是AA^T的特征值</li></ul><p>SVD的一个应用：部分场景下将矩阵求逆推广到非方阵，如下</p><h3 id="The-Moore-Penrose-Pseudoinverse"><a href="#The-Moore-Penrose-Pseudoinverse" class="headerlink" title="The Moore-Penrose Pseudoinverse"></a>The Moore-Penrose Pseudoinverse</h3><p>又称摩尔－彭若斯广义逆（好长的名字…）</p><script type="math/tex; mode=display">A^+=\lim_{\alpha \searrow 0}(A^TA+\alpha I)^{-1}A^{T}</script><p>实际上，一般用</p><script type="math/tex; mode=display">A^+=VD^+U^T</script><p>来计算广义逆</p><p>U，D，V是A的奇异值分解。D的广义逆D+由非零元素取倒数，然后转置得到。</p><p>当A是Ax=y的系数矩阵时，若x有解，则x=A+y是所有解中具有最小L2范式的那一个。</p><p>若x无解，Ax给出了L2范式中y-Ax的最小值</p><h3 id="Trace"><a href="#Trace" class="headerlink" title="Trace"></a>Trace</h3><p>又称迹，是矩阵主对角线元素的和。</p><script type="math/tex; mode=display">\mathrm{Tr}(A)=\sum_i{A_{i,i}}</script><p>迹和矩阵乘法可以代替一些需要求和符号的操作：</p><script type="math/tex; mode=display">||A||_F=\sqrt{\mathrm{Tr(AA^T)}}</script><p>矩阵的迹具有循环不变性：</p><script type="math/tex; mode=display">\mathrm{Tr(ABC)=Tr(CAB)=Tr(BCA)}</script><p>更通用来说，</p><script type="math/tex; mode=display">\mathrm{Tr}(\prod_{i=1}^nF^{(i)})=\mathrm{Tr}(F^{(n)}\prod_{i=1}^{n-1}F^{(i)})</script>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML&quot; async&gt;&lt;/script&gt;&lt;/p&gt;
&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;本文的主体是机器学习中所用到的线性代数知识，因此奉行&lt;a href=&quot;https://en.wikipedia.org/wiki/Lazy_evaluation&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Lazy evaluation&lt;/a&gt;，对这些知识更深层次的探究只在&lt;strong&gt;绝对必要&lt;/strong&gt;时完成。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Machine learning" scheme="https://verrickt.github.io/categories/Machine-learning/"/>
    
    
      <category term="Machine learning" scheme="https://verrickt.github.io/tags/Machine-learning/"/>
    
      <category term="Linear algebra" scheme="https://verrickt.github.io/tags/Linear-algebra/"/>
    
      <category term="The flower book" scheme="https://verrickt.github.io/tags/The-flower-book/"/>
    
  </entry>
  
  <entry>
    <title>HTML渲染为UWP的原生控件</title>
    <link href="https://verrickt.github.io/2020/04/21/render-html-natively-in-uwp/"/>
    <id>https://verrickt.github.io/2020/04/21/render-html-natively-in-uwp/</id>
    <published>2020-04-21T13:24:32.000Z</published>
    <updated>2020-07-11T14:18:07.693Z</updated>
    
    <content type="html"><![CDATA[<h3 id="没啥用的前言"><a href="#没啥用的前言" class="headerlink" title="没啥用的前言"></a>没啥用的前言</h3><p>说着再做UWP就剁手，我还是开了一个新坑🤣<br>这次是<a href="http://bog.ac" target="_blank" rel="external">B岛</a>的<a href="https://github.com/Verrickt/BogNMB.UWP" target="_blank" rel="external">UWP端</a><br>论坛客户端的一个老大难问题是内容的呈现。论坛一般以网页端为主，网页做好，论坛活跃起来后之后才会开发客户端/有开发者愿意做第三方的客户端。因此，API绝大多数情况是为网页端为一等公民的。此外，各个UI框架展示内容的格式也各有不同。以上两个原因导致HTML被选做富文本展示的通用语言。</p><a id="more"></a><p>对于客户端来说，HTML的呈现就成了问题。可以嵌入浏览器来渲染HTML，但存在两个难以解决的问题</p><ul><li>与应用原生部分交互困难，</li><li>可能有性能问题。</li></ul><p>因此，客户端的做法一般是绕过WebView，将HTML直接渲染为原生控件。那么问题来了，怎么做呢？Android的TextView可以渲染部分HTML，但UWP里就没有相应的API了。//@微软，出来挨打</p><p>先来看看手上有什么工具。首先是原生的XAML控件。Windows SDK 1903与.Net starndard 2.0兼容。又有一大堆.NET Standard 2.0的类库可以用了。  </p><p>API返回的结果是<br><figure class="highlight html"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">报个BUG。主岛的API p模式的data2，文档说小于1按1处理，实际上取0时返回的是'['，<span class="tag">&lt;<span class="name">br</span> /&gt;</span><span class="tag">&lt;<span class="name">br</span> /&gt;</span>curl  http://bog.ac/api/p/0/0<span class="tag">&lt;<span class="name">br</span> /&gt;</span>]</div></pre></td></tr></table></figure></p><h3 id="尝试搜索"><a href="#尝试搜索" class="headerlink" title="尝试搜索"></a>尝试搜索</h3><p>搜索一下，找到了<a href="https://github.com/VincentH-Net/WinRT-RichTextBlock.Html2Xaml" target="_blank" rel="external">WinRT-RichTextBlock.Html2Xaml</a>，它能把HTML渲染到<code>RichTextBlock</code>上，但是很遗憾，它不支持UWP。继续搜索，找到HTML2XAML的一个支持UWP的<a href="https://github.com/XeonKHJ/RichTextBlock.Html2Xaml" target="_blank" rel="external">fork</a>，使用后发现有不支持的标签。查看他的代码，似乎用到了<code>xslt</code>。面对HTML已经够头疼了，还是别引入另一个标记语言了。出师不利。</p><p>继续搜索，找到了一个<a href="https://docs.microsoft.com/en-us/windows/communitytoolkit/controls/markdowntextblock" target="_blank" rel="external">MarkdownTextBlock</a>的库。我以前做另一个论坛的<a href="https://github.com/Verrickt/COLG-UWP" target="_blank" rel="external">客户端</a>时用过它。当时是先想办法把<a href="https://github.com/Verrickt/COLG-UWP/blob/84a008d11518c1d7074be3c942e26e686e8cef5f/Colg%20UWP/Service/Html2Markdown.cs" target="_blank" rel="external">HTML转成Markdown</a>，再用它来渲染。但是在处理多级嵌套引用(<code>&lt;quote&gt;</code>)的时候会出错。<del>况且时隔这么久我已经看不懂当年写的代码了🤣</del></p><p>通过这两次搜索我们得到了以下信息：</p><ol><li><code>RichTextBlock</code>很可能能够作为我们渲染的容器</li><li>HTML标签和使用的原生控件有关</li><li>结构化的输出处理起来更方便，如果能把HTML转化为DOM树，靠dfs就可以实现转换。</li></ol><p>第一步，先要把HTML结构化。</p><h3 id="结构化HTML"><a href="#结构化HTML" class="headerlink" title="结构化HTML"></a>结构化HTML</h3><p>要把某种语言结构化，Parser是不二选择。而HTML的Parser因为经常面对残缺的HTML，通常支持将残缺的片段补齐。<a href="https://github.com/AngleSharp/AngleSharp" target="_blank" rel="external">AngleSharp</a>就是一个基于.NET Standard的HTML parser。<br>第一步搞定。有了结构，接下来就顺手多了。</p><h3 id="遍历DOM树"><a href="#遍历DOM树" class="headerlink" title="遍历DOM树"></a>遍历DOM树</h3><p>上面把HTML转成Markdown的源函数，里面的一大堆分支看的云里雾里。加上奇奇怪怪的边界情况后更是让人头疼。有没有一种代码的组织方法能让我针对一个标签写一个函数？答案是：<a href="https://en.wikipedia.org/wiki/Visitor_pattern" target="_blank" rel="external">Visitor pattern</a>. 百科上写的详细的多，我就只举一个例子。假设有<code>&lt;p&gt;</code>,<code>&lt;img&gt;</code>,<code>&lt;a&gt;</code>标签需要解析。定义<code>INode</code>作为所有DOM元素的接口,<code>IVisitor</code>是要对元素访问的接口。<code>INode</code>的实现者通过<code>visitor.Visit(this)</code>把控制流返还给<code>Visitor</code>。只需在<code>visitor</code>上实现对各个类的<code>Visit</code>方法，就达成目的。<br><figure class="highlight csharp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">interface</span> <span class="title">INode</span></div><div class="line">&#123;</div><div class="line">    <span class="function"><span class="keyword">void</span> <span class="title">Accept</span>(<span class="params">IVisitor visitor</span>)</span>;</div><div class="line">&#125;</div><div class="line"><span class="keyword">interface</span> <span class="title">IVisitor</span></div><div class="line">&#123;</div><div class="line">    <span class="function"><span class="keyword">void</span> <span class="title">Visit</span>(<span class="params">ANode node</span>)</span>;</div><div class="line">    <span class="function"><span class="keyword">void</span> <span class="title">Visit</span>(<span class="params">PNode node</span>)</span>;</div><div class="line">    <span class="function"><span class="keyword">void</span> <span class="title">Visit</span>(<span class="params">ImgNode node</span>)</span>;</div><div class="line">&#125;</div><div class="line"><span class="keyword">class</span> <span class="title">PNode</span>:<span class="title">INode</span></div><div class="line">&#123;</div><div class="line">    <span class="keyword">public</span> IReadonlyList&lt;INode&gt; Children&#123;<span class="keyword">get</span>;&#125;</div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">Accept</span>(<span class="params">IVisitor visitor</span>)</span></div><div class="line"><span class="function">    </span>&#123;</div><div class="line">        visitor.Visit(<span class="keyword">this</span>);</div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"><span class="keyword">class</span> <span class="title">ANode</span>:<span class="title">INode</span></div><div class="line">&#123;</div><div class="line">    <span class="keyword">public</span> <span class="keyword">string</span> Href&#123;<span class="keyword">get</span>;&#125;</div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">Accept</span>(<span class="params">IVisitor visitor</span>)</span></div><div class="line"><span class="function">    </span>&#123;</div><div class="line">        visitor.Visit(<span class="keyword">this</span>);</div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"><span class="keyword">class</span> <span class="title">ImgNode</span>:<span class="title">INode</span></div><div class="line">&#123;</div><div class="line">    <span class="keyword">public</span> <span class="keyword">string</span> src&#123;<span class="keyword">get</span>;&#125;</div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">Accept</span>(<span class="params">IVisitor visitor</span>)</span></div><div class="line"><span class="function">    </span>&#123;</div><div class="line">        visitor.Visit(<span class="keyword">this</span>);</div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title">DOMVisitor</span>:<span class="title">IVisitor</span></div><div class="line">&#123;</div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">Visit</span>(<span class="params">PNode node</span>)</span></div><div class="line"><span class="function">    </span>&#123;</div><div class="line">        <span class="keyword">foreach</span>(<span class="keyword">var</span> item <span class="keyword">in</span> node.Children)</div><div class="line">            item.Accept(<span class="keyword">this</span>);</div><div class="line">    &#125;</div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">Visit</span>(<span class="params">ANode node</span>)</span></div><div class="line"><span class="function">    </span>&#123;</div><div class="line">        <span class="comment">//new Hyper link</span></div><div class="line">    &#125;</div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">Visit</span>(<span class="params">ImgNode node</span>)</span></div><div class="line"><span class="function">    </span>&#123;</div><div class="line">        <span class="comment">//new Image</span></div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p><h3 id="RichTextBlock"><a href="#RichTextBlock" class="headerlink" title="RichTextBlock"></a>RichTextBlock</h3><p>接下来考虑如何呈现。根据<a href="https://docs.microsoft.com/en-us/uwp/api/Windows.UI.Xaml.Controls.RichTextBlock" target="_blank" rel="external">文档</a>，<code>RichTextBlock</code>可以包含多个<code>Block</code>，一个<code>Block</code>又可以包含若干<code>Inline</code>。与HTML标签刚好对应！<code>InlineUIContainer</code>自己是<code>Inline</code>，但<code>Child</code>属性可以塞下任何<code>UIElement</code>。如果塞进去另一个<code>RichTextBlock</code>就实现了对引用<code>&lt;quote&gt;</code>呈现。这样可以实现任意级引用<code>&lt;quote&gt;</code>的呈现。具体实现上，提供一个<code>Stack&lt;Block&gt;</code>供使用。转化为<code>Inline</code>的元素每次添加到栈顶的<code>Block</code>中。遇到转化为<code>Block</code>的元素则压栈。最后把<code>Block</code>按照先后顺序加入<code>RichTextBlock</code>即可。</p><p>下面是效果，具体代码请参考<a href="https://github.com/Verrickt/BogNMB.UWP/tree/develop/HTMLParser" target="_blank" rel="external">HTML Parser</a>和<a href="https://github.com/Verrickt/BogNMB.UWP/blob/develop/BogNMB.UWP/CustomControl/RichTextBlockRenderer.cs" target="_blank" rel="external">RichTextBlockRenderer</a></p><p><img src="https://user-images.githubusercontent.com/11483783/79775074-48d45800-8366-11ea-839d-83283cbb3b4a.jpg" alt="picture"></p><p>搞定了一个困扰多年的难题，可喜可贺(＾o＾)ﾉ</p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;没啥用的前言&quot;&gt;&lt;a href=&quot;#没啥用的前言&quot; class=&quot;headerlink&quot; title=&quot;没啥用的前言&quot;&gt;&lt;/a&gt;没啥用的前言&lt;/h3&gt;&lt;p&gt;说着再做UWP就剁手，我还是开了一个新坑🤣&lt;br&gt;这次是&lt;a href=&quot;http://bog.ac&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;B岛&lt;/a&gt;的&lt;a href=&quot;https://github.com/Verrickt/BogNMB.UWP&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;UWP端&lt;/a&gt;&lt;br&gt;论坛客户端的一个老大难问题是内容的呈现。论坛一般以网页端为主，网页做好，论坛活跃起来后之后才会开发客户端/有开发者愿意做第三方的客户端。因此，API绝大多数情况是为网页端为一等公民的。此外，各个UI框架展示内容的格式也各有不同。以上两个原因导致HTML被选做富文本展示的通用语言。&lt;/p&gt;
    
    </summary>
    
    
      <category term="UWP" scheme="https://verrickt.github.io/tags/UWP/"/>
    
      <category term="HTML" scheme="https://verrickt.github.io/tags/HTML/"/>
    
  </entry>
  
  <entry>
    <title>PAT 1097 Deduplication on a Linked List</title>
    <link href="https://verrickt.github.io/2020/04/03/PAT-1097-Deduplication-on-a-Linked-List/"/>
    <id>https://verrickt.github.io/2020/04/03/PAT-1097-Deduplication-on-a-Linked-List/</id>
    <published>2020-04-03T05:31:51.000Z</published>
    <updated>2020-04-03T06:15:54.728Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Deduplication-on-a-Linked-List"><a href="#Deduplication-on-a-Linked-List" class="headerlink" title="Deduplication on a Linked List"></a>Deduplication on a Linked List</h2><p>Given a singly linked list L with integer keys, you are supposed to remove the nodes with duplicated absolute values of the keys. That is, for each value K, only the first node of which the value or absolute value of its key equals K will be kept. At the mean time, all the removed nodes must be kept in a separate list. For example, given L being <code>21→-15→-15→-7→15</code>, you must output <code>21→-15→-7</code>, and the removed list <code>-15→15</code>.</p><a id="more"></a><p><strong>Input Specification</strong></p><p>Each input file contains one test case. For each case, the first line contains the address of the first node, and a positive N (≤10^​5​​) which is the total number of nodes. The address of a node is a 5-digit nonnegative integer, and NULL is represented by −1.</p><p>Then N lines follow, each describes a node in the format:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">Address Key Next</div></pre></td></tr></table></figure></p><p>where Address is the position of the node, Key is an integer of which absolute value is no more than 10​^4​​, and Next is the position of the next node.</p><p><strong>Output Specification</strong></p><p>For each case, output the resulting linked list first, then the removed list. Each node occupies a line, and is printed in the same format as in the input.</p><p><strong>Sample Input</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">00100 5</div><div class="line">99999 -7 87654</div><div class="line">23854 -15 00000</div><div class="line">87654 15 -1</div><div class="line">00000 -15 99999</div><div class="line">00100 21 23854</div></pre></td></tr></table></figure></p><p><strong>Sample Output</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">00100 21 23854</div><div class="line">23854 -15 99999</div><div class="line">99999 -7 -1</div><div class="line">00000 -15 87654</div><div class="line">87654 15 -1</div></pre></td></tr></table></figure></p><hr><p>类似<a href="https://www.liuchuo.net/archives/2118" target="_blank" rel="external">这样</a>的骚操作网上有很多，这里不在引用。本文主要考虑常规思路。思路非常简单，便利链表，如果曾经出现过就插入另一个链表，没出现过就继续，最后得到两个链表，分别是去重后的和重复的。看似比较简单，想把所有情况考虑全也是要花点心思的。</p><ul><li>重复部分的链表是和原链表顺序是一样的，采用尾插法。需要头、尾两个指针。</li><li>原链表的头节点是第一个节点，不可能被放入重复链表，那么去重后的链表头节点不变。</li><li>遍历过程中需要将节点删除(即插入重复链表)，所以需要<code>prev</code>指针记录去重链表的上一个节点地址，以便修改<code>next</code>指针。</li></ul><p>如果你像我一样以为上面的描述就把所有的情况都考虑完全了，那么欢迎你和我来到BUG的海洋😉</p><p>先来看AC的代码</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"vector"</span></span></div><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"iostream"</span></span></div><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"algorithm"</span></span></div><div class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</div><div class="line"><span class="class"><span class="keyword">struct</span> <span class="title">node</span></span></div><div class="line"><span class="class">&#123;</span></div><div class="line"><span class="keyword">int</span> addr;</div><div class="line"><span class="keyword">int</span> key; </div><div class="line"><span class="keyword">int</span> next;</div><div class="line">&#125;;</div><div class="line">node nodes[<span class="number">100086</span>];</div><div class="line"><span class="keyword">bool</span> <span class="built_in">map</span>[<span class="number">10024</span>];</div><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">print</span><span class="params">(<span class="keyword">int</span> head)</span></span></div><div class="line"><span class="function"></span>&#123;</div><div class="line"><span class="keyword">auto</span> cur = head;</div><div class="line"><span class="keyword">while</span> (cur != <span class="number">-1</span>)</div><div class="line">&#123;</div><div class="line"><span class="keyword">if</span> (nodes[cur].next != <span class="number">-1</span>)</div><div class="line"><span class="built_in">printf</span>(<span class="string">"%05d %d %05d\n"</span>, cur, nodes[cur].key, nodes[cur].next);</div><div class="line"><span class="keyword">else</span> <span class="built_in">printf</span>(<span class="string">"%05d %d -1\n"</span>, cur, nodes[cur].key);</div><div class="line">cur = nodes[cur].next;</div><div class="line">&#125;</div><div class="line">&#125;</div><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></div><div class="line"><span class="function"></span>&#123;</div><div class="line"><span class="keyword">int</span> head, n;</div><div class="line"><span class="built_in">cin</span> &gt;&gt; head &gt;&gt; n;</div><div class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n; i++)</div><div class="line">&#123;</div><div class="line"><span class="keyword">int</span> id;</div><div class="line"><span class="built_in">cin</span> &gt;&gt; id;</div><div class="line">nodes[id].addr = id;</div><div class="line"><span class="built_in">cin</span> &gt;&gt; nodes[id].key &gt;&gt; nodes[id].next;</div><div class="line">&#125;</div><div class="line">fill(<span class="built_in">map</span>, <span class="built_in">map</span> + <span class="number">10024</span>, <span class="literal">false</span>);</div><div class="line"><span class="keyword">int</span> dupli_head = <span class="number">-1</span>;</div><div class="line"><span class="keyword">int</span> dupli_tail = <span class="number">-1</span>;</div><div class="line"><span class="keyword">int</span> cur = head;</div><div class="line"><span class="keyword">int</span> prev = <span class="number">-1</span>;</div><div class="line"><span class="keyword">while</span> (cur != <span class="number">-1</span>)</div><div class="line">&#123;</div><div class="line"><span class="keyword">while</span> (<span class="built_in">map</span>[<span class="built_in">abs</span>(nodes[cur].key)])</div><div class="line">&#123;</div><div class="line"><span class="keyword">int</span> next = nodes[cur].next;</div><div class="line"><span class="keyword">if</span> (dupli_head == <span class="number">-1</span>) dupli_head = dupli_tail = cur;</div><div class="line"><span class="keyword">else</span></div><div class="line">&#123;</div><div class="line">nodes[dupli_tail].next = cur;</div><div class="line">dupli_tail = cur;</div><div class="line">nodes[cur].next = <span class="number">-1</span>;</div><div class="line">&#125;</div><div class="line">cur = next;</div><div class="line"><span class="keyword">if</span> (cur == <span class="number">-1</span>) <span class="keyword">break</span>;</div><div class="line">&#125;</div><div class="line"><span class="keyword">if</span> (cur == <span class="number">-1</span>) &#123; nodes[prev].next = <span class="number">-1</span>; <span class="keyword">break</span>; &#125;</div><div class="line"><span class="keyword">if</span> (prev != <span class="number">-1</span>) nodes[prev].next = cur;</div><div class="line"><span class="built_in">map</span>[<span class="built_in">abs</span>(nodes[cur].key)] = <span class="literal">true</span>;</div><div class="line">prev = cur;</div><div class="line">cur = nodes[cur].next;</div><div class="line">&#125;</div><div class="line">print(head);</div><div class="line">print(dupli_head);</div><div class="line"><span class="keyword">return</span> <span class="number">0</span>;</div><div class="line">&#125;</div></pre></td></tr></table></figure><blockquote><p>为啥<code>while</code>循环里又套了个<code>while</code>循环，用<code>if</code>不就够了么？</p></blockquote><p>此乃第一坑😉<br>上文说到</p><blockquote><p><code>prev</code>指针记录去重链表的上一个节点地址</p></blockquote><p>那么<code>prev</code>的<code>next</code>应该是<strong>去重链表</strong>的下一个地址。而<code>if</code>得到的仅仅是下一个地址，此时还不知道他是否属于<strong>去重链表</strong>。这时不能做<code>prev.next=cur</code>。那什么时候可以呢？当然是<code>map[abs(nodes[cur].key)</code>为假啦。</p><blockquote><p>好多<code>-1</code>看的我头晕</p></blockquote><p>恭喜你，进入了第二坑😉</p><ol><li><p><code>dupli_head</code>和<code>dupli_tail</code>是头尾指针，<code>-1</code>作为链表为空时的特殊值。</p></li><li><p>插入<code>dupli</code>链表后，要把<code>cur.next</code>置为<code>-1</code>，表示这是新链表的最后一个节点。为防止链表断裂，用<code>next=cur.next</code>记住下一个地址。</p></li><li><p>插入重复链表是在<code>while</code>循环里进行的，那么就有可能在这里耗光所有的元素。此即<code>if (cur == -1) break;</code>。</p></li><li><p>当在<code>while</code>循环里耗尽所有元素后，已经不可能有新的元素可以作为<code>prev.next</code>，因此<code>if (cur == -1) { nodes[prev].next = -1; break; }</code></p></li><li>上文说过，原链表的头节点一定是去重链表的头节点。那么当刚访问过头节点时，是不需要对<code>prev</code>操作的。即<code>if (prev != -1) nodes[prev].next = cur;</code>。当然，你也可以直接跳过头节点<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">map</span>[head]=<span class="literal">true</span>;</div><div class="line">cur = nodes[head].next;</div><div class="line">prev = head;</div></pre></td></tr></table></figure></li></ol><p>怎么样，还觉得考虑所有情况简单么😉</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Deduplication-on-a-Linked-List&quot;&gt;&lt;a href=&quot;#Deduplication-on-a-Linked-List&quot; class=&quot;headerlink&quot; title=&quot;Deduplication on a Linked List&quot;&gt;&lt;/a&gt;Deduplication on a Linked List&lt;/h2&gt;&lt;p&gt;Given a singly linked list L with integer keys, you are supposed to remove the nodes with duplicated absolute values of the keys. That is, for each value K, only the first node of which the value or absolute value of its key equals K will be kept. At the mean time, all the removed nodes must be kept in a separate list. For example, given L being &lt;code&gt;21→-15→-15→-7→15&lt;/code&gt;, you must output &lt;code&gt;21→-15→-7&lt;/code&gt;, and the removed list &lt;code&gt;-15→15&lt;/code&gt;.&lt;/p&gt;
    
    </summary>
    
    
      <category term="PAT" scheme="https://verrickt.github.io/tags/PAT/"/>
    
  </entry>
  
  <entry>
    <title>PAT 1021 Deepest Root</title>
    <link href="https://verrickt.github.io/2020/03/23/PAT-1021-Deepest-Root/"/>
    <id>https://verrickt.github.io/2020/03/23/PAT-1021-Deepest-Root/</id>
    <published>2020-03-23T13:32:29.000Z</published>
    <updated>2020-03-23T14:29:30.905Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Deepest-Root"><a href="#Deepest-Root" class="headerlink" title="Deepest Root"></a>Deepest Root</h2><p>A graph which is connected and acyclic can be considered a tree. The height of the tree depends on the selected root. Now you are supposed to find the root that results in a highest tree. Such a root is called <strong>the deepest root.</strong></p><a id="more"></a><p><strong>Input Specification</strong></p><p>Each input file contains one test case. For each case, the first line contains a positive integer N (≤10​^4​​) which is the number of nodes, and hence the nodes are numbered from 1 to N. Then N−1 lines follow, each describes an edge by given the two adjacent nodes’ numbers.</p><p><strong>Output Specification</strong></p><p>For each test case, print each of the deepest roots in a line. If such a root is not unique, print them in increasing order of their numbers. In case that the given graph is not a tree, print Error: K components where K is the number of connected components in the graph.</p><p><strong>Sample Input 1</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">5</div><div class="line">1 2</div><div class="line">1 3</div><div class="line">1 4</div><div class="line">2 5</div></pre></td></tr></table></figure><p><strong>Sample Output 1</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td></tr></table></figure></p><p><strong>Sample Input 2</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">5</div><div class="line">1 3</div><div class="line">1 4</div><div class="line">2 5</div><div class="line">3 4</div></pre></td></tr></table></figure></p><p><strong>Sample Output 2</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">Error: 2 components</div></pre></td></tr></table></figure></p><hr><p>去年连答案都看不懂的题在今天发愣的时候突然想出来，真是可喜可贺。</p><p>题目要求是求树高，但其实树高其实和求路径长度是一致的：在简单图里，只要定了起点和终点，路径就是从根到叶节点的轨迹。求<strong>某点</strong>的最大路径长度和全图强连通分量可以用dfs一并解决，问题转化为求<strong>全图</strong>的最大路径长度</p><p>简单粗暴的穷举会被几个测试点卡时间，所以需要一些“聪明”的小办法。仔细思考，穷举被卡时间的原因应该是做了大量重复的计算。如果能区分哪些计算是不必要的，就可以解决问题。沿着这个思路，考虑两条最长路径是否有关系。考虑<code>十</code>字型的图，两条最长路径相交于图上一点，这么看来最长路径之间很可能有交点。就从交点出发吧。</p><p>考虑连通图的情况。若<code>A</code>为图上一点，<code>B</code>为图上另一点。两点间的最长路径，记作<code>MaxPath(A,B)</code></p><p>设<code>S</code>为图上任意一点，从<code>S</code>出发的最长路径的终点为<code>P</code>。那么<code>P</code>点的出度一定为0：若出度不为0，总可以找到更长的路径。从而与<code>S</code>，<code>P</code>之间路径为最长矛盾。</p><p>设从<code>P</code>点出发的最长路径的终点为<code>Q</code>，则<code>MaxPath(P,Q)</code>一定经过<code>S</code>点：假设这条路不经过<code>S</code>点。设<code>T</code>为属于<code>Path(P,Q)</code>且不属于<code>Path(P,S)</code>的任意一点。因为图联通，则<code>S</code>，<code>T</code>间存在长度至少为1的路径。那么<code>S</code>，<code>P</code>间经过<code>T</code>的路径长度比<code>Path(S,P)</code>更长，与<code>Path(S,P)</code>是<code>S</code>和<code>P</code>点间最长路径的假设矛盾。所以<code>MaxPath(P,Q)</code>一定经过S点。而<code>P</code>点和<code>Q</code>点必定一个入度为0，一个出度为0(<code>P</code>，<code>Q</code>位于图的两端)。因此，<code>Path(P,Q)</code>即为整张图的最长路径。</p><p>回到本题，要求所有最长路径的起始节点，任选一个节点<code>S</code>，所有使路径最长的<code>P</code>都是路径的起始节点。需要注意的是，当<code>Path(P，Q)</code>在最长路径时，<code>P</code>点和<code>Q</code>点都可以作为节点。这是因为路径是可以反向的。求出所有的<code>P</code>，<code>Q</code>去重排序后即可。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"cstdio"</span></span></div><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"vector"</span></span></div><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"algorithm"</span></span></div><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"set"</span></span></div><div class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</div><div class="line"><span class="keyword">int</span> graph[<span class="number">10086</span>][<span class="number">10086</span>];</div><div class="line"><span class="keyword">int</span> n;</div><div class="line"><span class="keyword">int</span> max_depth = <span class="number">-1</span>;</div><div class="line"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; roots;</div><div class="line"><span class="built_in">set</span>&lt;<span class="keyword">int</span>&gt; ans = <span class="built_in">set</span>&lt;<span class="keyword">int</span>&gt;();</div><div class="line"><span class="keyword">bool</span> visited[<span class="number">10086</span>];</div><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">dfs</span><span class="params">(<span class="keyword">int</span> source, <span class="keyword">int</span> depth)</span></span></div><div class="line"><span class="function"></span>&#123;</div><div class="line">visited[source] = <span class="literal">true</span>;</div><div class="line"><span class="keyword">if</span> (depth &gt; max_depth)</div><div class="line">&#123;</div><div class="line">max_depth = depth;</div><div class="line">roots.clear();</div><div class="line">&#125;</div><div class="line"><span class="keyword">if</span> (depth == max_depth) roots.push_back(source);</div><div class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= n; i++)</div><div class="line"><span class="keyword">if</span> (graph[source][i] == <span class="number">1</span> &amp;&amp; !visited[i])</div><div class="line">dfs(i, depth + <span class="number">1</span>);</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></div><div class="line"><span class="function"></span>&#123;</div><div class="line"><span class="built_in">scanf</span>(<span class="string">"%d"</span>, &amp;n);</div><div class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt; n; i++)</div><div class="line">&#123;</div><div class="line"><span class="keyword">int</span> j, k;</div><div class="line"><span class="built_in">scanf</span>(<span class="string">"%d %d"</span>, &amp;j, &amp;k);</div><div class="line">graph[j][k] = graph[k][j] = <span class="number">1</span>;</div><div class="line">&#125;</div><div class="line">fill(visited, visited + <span class="number">10086</span>, <span class="literal">false</span>);</div><div class="line"><span class="keyword">int</span> cnt = <span class="number">0</span>;</div><div class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= n; i++)</div><div class="line">&#123;</div><div class="line"><span class="keyword">if</span> (!visited[i]) &#123; dfs(i, <span class="number">0</span>); cnt++; &#125;</div><div class="line">&#125;</div><div class="line"><span class="keyword">if</span> (cnt != <span class="number">1</span>) &#123; <span class="built_in">printf</span>(<span class="string">"Error: %d components\n"</span>, cnt); <span class="keyword">return</span> <span class="number">0</span>; &#125;</div><div class="line"><span class="keyword">for</span> (<span class="keyword">auto</span> i : roots) ans.insert(i);</div><div class="line">fill(visited, visited + <span class="number">10086</span>, <span class="literal">false</span>);</div><div class="line">dfs(roots[<span class="number">0</span>], <span class="number">0</span>);</div><div class="line"><span class="keyword">for</span> (<span class="keyword">auto</span> i : roots) ans.insert(i);</div><div class="line"><span class="keyword">for</span> (<span class="keyword">auto</span> i : ans) <span class="built_in">printf</span>(<span class="string">"%d\n"</span>, i);</div><div class="line"><span class="keyword">return</span> <span class="number">0</span>;</div><div class="line">&#125;</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Deepest-Root&quot;&gt;&lt;a href=&quot;#Deepest-Root&quot; class=&quot;headerlink&quot; title=&quot;Deepest Root&quot;&gt;&lt;/a&gt;Deepest Root&lt;/h2&gt;&lt;p&gt;A graph which is connected and acyclic can be considered a tree. The height of the tree depends on the selected root. Now you are supposed to find the root that results in a highest tree. Such a root is called &lt;strong&gt;the deepest root.&lt;/strong&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="PAT" scheme="https://verrickt.github.io/tags/PAT/"/>
    
  </entry>
  
  <entry>
    <title>重学「冒泡排序」</title>
    <link href="https://verrickt.github.io/2020/03/17/yeah-that-naive-bubble-sort/"/>
    <id>https://verrickt.github.io/2020/03/17/yeah-that-naive-bubble-sort/</id>
    <published>2020-03-17T06:22:12.000Z</published>
    <updated>2020-03-17T09:47:08.570Z</updated>
    
    <content type="html"><![CDATA[<p>PAT里链表题有各式各样的<a href="https://www.liuchuo.net/archives/2116" target="_blank" rel="external">骚操作</a>。这些非常规操作易学易用，但是习惯了这些后，反而对题目真正想考察的知识生疏了。今天就碰到一道这样的题，想用正经的算法写却写不出来。希望大家以我为戒，不要过多的学习这些「奇技淫巧」<br><a id="more"></a></p><hr><p><a href="https://pintia.cn/problem-sets/994805342720868352/problems/994805425780670464" target="_blank" rel="external">Linked List Sorting</a>这道题很常规，是个很一般的链表排序题。链表不能随机访问，因此算法上的选择限制很大，只有冒泡、插入、选择排序可以选。我选了冒泡练手，没想到没做出来。</p><p>冒泡排序的关键步骤是交换两个相邻的逆序元素。每完成一趟后都有一个元素被顺利的放到了最终位置上。</p><p>按我的想法，<code>i in (0,n),j in (i+1,n)</code>，每次<code>j</code>走到<code>n</code>时，就把「最小」的元素放到了<code>i</code>的位置，依次增加<code>i</code>就得到了升序排列。<strong>很遗憾，这是错的</strong><br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n; i++)</div><div class="line"><span class="keyword">for</span> (<span class="keyword">int</span> j = i + <span class="number">1</span>; j &lt; n; j++)</div><div class="line"><span class="keyword">if</span> (n[j] &lt; n[j<span class="number">-1</span>]) swap(n[j], n[j<span class="number">-1</span>]);</div></pre></td></tr></table></figure></p><p>在<code>[5,3,1,2,6,7]</code>上考虑这段代码，<code>if</code>语句能保证，只要<code>n[j]</code>比<code>n[j-1]</code>大，就把这个大的往<strong>后</strong>移。。而对于最小值<code>1</code>，仅仅在<code>j=2</code>的时候被<code>swap</code>到了<code>n[1]</code>，并没有真正的移动到<code>n[0]</code>这个我所期待的位置。观察<code>j</code>扫描的方向也是向<strong>后</strong>的。</p><p>再看另一种写法<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n; i++)</div><div class="line"><span class="keyword">for</span> (<span class="keyword">int</span> j = n<span class="number">-1</span>; j &gt; i; j--)</div><div class="line"><span class="keyword">if</span> (n[j] &lt; n[j<span class="number">-1</span>]) swap(n[j], n[j<span class="number">-1</span>]);</div></pre></td></tr></table></figure></p><p>这时<code>if</code>语句能保证的是只要<code>n[j]</code>比<code>n[j-1]</code>小，就把这个小的往<strong>前</strong>移。在<code>[5,3,1,2,6,7]</code>，第一轮之后<code>1</code>就被放在了<code>n[0]</code>上。注意到<code>j</code>的移动方向是向<strong>前</strong>的。</p><p>我们想得到升序的序列，那么就要把大的往后移，小的往前移。<code>j</code>从前往<strong>后</strong>移动的过程中，我们是把最大的元素往<strong>后移一位</strong>，在下一次比较时最大的元素还会继续被移往后方，最终放在<code>a[n-1]</code>同理，<code>j</code>从后往<strong>前</strong>的过程中，最小的元素会被一直前移，直到<code>a[0]</code>。</p><p>因此一趟完成后到底是最大的元素被放到最后了还是最小的元素被放到最前了，要看<code>j</code>移动的方向。在链表中从前往后是方便的，因而最大的元素会被先放到最后。所以我们需要做的是，每一趟减少<code>j</code><strong>移动结束</strong>的范围。</p><p>想到这里基本上就能理清了。加上处理头节点的一些细节后，不难写出代码<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"cstdio"</span></span></div><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"vector"</span></span></div><div class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</div><div class="line"><span class="class"><span class="keyword">struct</span> <span class="title">node</span> &#123;</span></div><div class="line"><span class="keyword">int</span> addr;</div><div class="line"><span class="keyword">int</span> key;</div><div class="line"><span class="keyword">int</span> next;</div><div class="line">&#125;;</div><div class="line"><span class="built_in">vector</span>&lt;node&gt; nodes = <span class="built_in">vector</span>&lt;node&gt;(<span class="number">100086</span>);</div><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></div><div class="line"><span class="function"></span>&#123;</div><div class="line"><span class="keyword">int</span> n, head;</div><div class="line"><span class="built_in">scanf</span>(<span class="string">"%d %d"</span>, &amp;n, &amp;head);</div><div class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n; i++)</div><div class="line">&#123;</div><div class="line"><span class="keyword">int</span> addr, key, next;</div><div class="line"><span class="built_in">scanf</span>(<span class="string">"%d %d %d"</span>, &amp;addr, &amp;key, &amp;next);</div><div class="line">nodes[addr] = node(&#123; addr,key,next &#125;);</div><div class="line">&#125;</div><div class="line"><span class="keyword">int</span> newHead = head;</div><div class="line"><span class="keyword">int</span> end = <span class="number">-1</span>;</div><div class="line"><span class="keyword">while</span> (end != newHead)</div><div class="line">&#123;</div><div class="line"><span class="keyword">int</span> t = newHead;</div><div class="line"><span class="keyword">int</span> pre = <span class="number">-1</span>;</div><div class="line"><span class="keyword">while</span> (nodes[t].next != end)</div><div class="line">&#123;</div><div class="line"><span class="keyword">int</span> next = nodes[t].next;</div><div class="line"><span class="keyword">if</span> (nodes[t].key &gt; nodes[next].key)</div><div class="line">&#123;</div><div class="line"><span class="keyword">if</span> (pre != <span class="number">-1</span>)</div><div class="line">nodes[pre].next = next;</div><div class="line"><span class="keyword">else</span></div><div class="line">newHead = next;</div><div class="line">nodes[t].next = nodes[next].next;</div><div class="line">nodes[next].next = t;</div><div class="line">&#125;</div><div class="line">pre = t;</div><div class="line">t = next;</div><div class="line">&#125;</div><div class="line">end = t;</div><div class="line">&#125;</div><div class="line"><span class="keyword">int</span> h = newHead;</div><div class="line"><span class="keyword">int</span> cnt = <span class="number">0</span>;</div><div class="line"><span class="keyword">while</span> (h != <span class="number">-1</span>)</div><div class="line">&#123;</div><div class="line">cnt++; h = nodes[h].next;</div><div class="line">&#125;</div><div class="line"><span class="keyword">if</span> (cnt == <span class="number">0</span>)</div><div class="line"><span class="built_in">printf</span>(<span class="string">"0 -1\n"</span>);</div><div class="line"><span class="keyword">else</span></div><div class="line"><span class="built_in">printf</span>(<span class="string">"%d %05d\n"</span>, cnt, newHead);</div><div class="line">h = newHead;</div><div class="line"><span class="keyword">while</span> (h != <span class="number">-1</span>)</div><div class="line">&#123;</div><div class="line"><span class="keyword">if</span> (nodes[h].next == <span class="number">-1</span>)</div><div class="line"><span class="built_in">printf</span>(<span class="string">"%05d %d -1\n"</span>, nodes[h].addr, nodes[h].key);</div><div class="line"><span class="keyword">else</span></div><div class="line"><span class="built_in">printf</span>(<span class="string">"%05d %d %05d\n"</span>, nodes[h].addr, nodes[h].key, nodes[h].next);</div><div class="line">h = nodes[h].next;</div><div class="line">&#125;</div><div class="line"><span class="keyword">return</span> <span class="number">0</span>;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p><p>欢天喜地的去提交，发现有一个点超时了。诚然，冒泡排序的时间复杂度是<code>Θ(n^2)</code>，非常规操作里用<code>std::sort</code>处理后输出的时间复杂度是<code>Θ(nlogn)</code>，超时是理所应当。放眼望去绝大多数AC的代码都用的与这种投机取巧的方法。不知道这是不是违背出题人的初衷呢？</p><p>当然，这问题轮不到我评头论足，我只是个连冒泡排序都忘得一干二净的渣渣🙂</p><hr><p>总结：</p><ul><li>冒泡排序一趟后被放到最终位置的元素与扫描方向有关</li><li>基本功要打扎实，切忌「眼高手低」</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;PAT里链表题有各式各样的&lt;a href=&quot;https://www.liuchuo.net/archives/2116&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;骚操作&lt;/a&gt;。这些非常规操作易学易用，但是习惯了这些后，反而对题目真正想考察的知识生疏了。今天就碰到一道这样的题，想用正经的算法写却写不出来。希望大家以我为戒，不要过多的学习这些「奇技淫巧」&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="PAT" scheme="https://verrickt.github.io/tags/PAT/"/>
    
  </entry>
  
  <entry>
    <title>PAT-1038 Recover the Smallest Number</title>
    <link href="https://verrickt.github.io/2020/03/16/PAT-1038-Recover-the-Smallest-Number/"/>
    <id>https://verrickt.github.io/2020/03/16/PAT-1038-Recover-the-Smallest-Number/</id>
    <published>2020-03-16T04:04:00.000Z</published>
    <updated>2020-03-17T06:23:51.266Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Recover-the-Smallest-Number"><a href="#Recover-the-Smallest-Number" class="headerlink" title="Recover the Smallest Number"></a>Recover the Smallest Number</h1><p>Given a collection of number segments, you are supposed to recover the smallest number from them. For example, given <code>{ 32, 321, 3214, 0229, 87 }</code>, we can recover many numbers such like <code>32-321-3214-0229-87</code> or <code>0229-32-87-321-3214</code> with respect to different orders of combinations of these segments, and the smallest number is <code>0229-321-3214-32-87</code>.<br><a id="more"></a><br><strong>Input Specification</strong></p><p>Each input file contains one test case. Each case gives a positive integer N (≤10​^4​​) followed by N number segments. Each segment contains a non-negative integer of no more than 8 digits. All the numbers in a line are separated by a space. </p><p><strong>Output Specification</strong></p><p>For each test case, print the smallest number in one line. Notice that the first digit must not be zero.</p><p><strong>Sample Input</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">5 32 321 3214 0229 87</div></pre></td></tr></table></figure></p><p><strong>Sample Output</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">22932132143287</div></pre></td></tr></table></figure></p><hr><p>这道题以前就没做对，这次也没做对，记录一下。</p><p>两次读完题的反应完全一致：谁“小”把谁排前面。</p><ol><li>若两个数位数相同，按数值比较</li><li>若两个数位数不同，位数少的数字补0至位数相同，按数值比较。</li></ol><p>我的cmp是这么写的<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">bool</span> <span class="title">cmp</span><span class="params">(<span class="built_in">string</span> a, <span class="built_in">string</span> b)</span></span></div><div class="line"><span class="function"></span>&#123;</div><div class="line"><span class="built_in">string</span> p1 = a, p2 = b;</div><div class="line"><span class="keyword">int</span> diff = p1.size() - p2.size();</div><div class="line"><span class="keyword">if</span> (diff &lt; <span class="number">0</span>) swap(p1, p2);</div><div class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="built_in">abs</span>(diff); i++) p2.push_back(<span class="string">'0'</span>);</div><div class="line"><span class="keyword">return</span> p1 &lt; p2;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p><p>Debug模式在运行时报错，<code>Assert failed: Invalid comparor</code>. 根据<a href="https://en.cppreference.com/w/cpp/named_req/Compare" target="_blank" rel="external">cppreference</a>，<code>comparer</code>需满足</p><ul><li>反自反.对于所有的<code>a</code>,<code>comp(a,a)==false</code></li><li>反对称.若<code>comp(a,b)==true</code>，则<code>comp(b,a)==false</code></li><li>传递 若<code>comp(a,b)==true</code>，<code>comp(b,c)==true</code>,则<code>comp(a,c)==true</code>.</li></ul><p>我们的<code>comparer</code>违背了反对称性。<code>comp(9,10)===comp(10,9)</code></p><p>查到<code>string</code>的<code>&lt;</code>操作符按照<a href="https://en.wikipedia.org/wiki/Lexicographical_order" target="_blank" rel="external">字典序</a>进行比较。而我们辛辛苦苦的补<code>0</code>，就是要按字典序比较。</p><p>事实证明字典序也不对。考虑<code>32</code>和<code>321</code>，按照字典序，<code>32</code>&lt;<code>321</code>，但这样拼接的<code>32321</code>却比<code>32132</code>要大。似乎还要考虑字符串的首位和末位。</p><p>查了别人的代码，最好的解决办法是，把问题原样丢给C++<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">bool</span> <span class="title">cmp</span><span class="params">(<span class="built_in">string</span> s1, <span class="built_in">string</span> s2)</span></span></div><div class="line"><span class="function"></span>&#123;</div><div class="line"><span class="keyword">return</span> s1 + s2 &gt; s2 + s1;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p><p>我居然还想了半天，整个人都不好了😶</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"string"</span></span></div><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"vector"</span></span></div><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"iostream"</span></span></div><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"algorithm"</span></span></div><div class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</div><div class="line"><span class="function"><span class="keyword">bool</span> <span class="title">cmp</span><span class="params">(<span class="built_in">string</span> s1, <span class="built_in">string</span> s2)</span></span></div><div class="line"><span class="function"></span>&#123;</div><div class="line"><span class="comment">//sort by lexicographical order doesn't work here. 33&lt;321,but 33321 &gt; 32133. :-(</span></div><div class="line"><span class="comment">//but we can throw the problem at cpp :-)</span></div><div class="line"><span class="keyword">auto</span> p1 = s1;</div><div class="line"><span class="keyword">auto</span> p2 = s2;</div><div class="line"><span class="keyword">return</span> p1+p2 &lt; p2+p1;</div><div class="line">&#125;</div><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></div><div class="line"><span class="function"></span>&#123;</div><div class="line"><span class="keyword">int</span> n;</div><div class="line"><span class="built_in">cin</span> &gt;&gt; n;</div><div class="line"><span class="built_in">vector</span>&lt;<span class="built_in">string</span>&gt; v=<span class="built_in">vector</span>&lt;<span class="built_in">string</span>&gt;(n);</div><div class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n; i++) <span class="built_in">cin</span> &gt;&gt; v[i];</div><div class="line">sort(v.begin(), v.end(), cmp);</div><div class="line"><span class="built_in">string</span> res;</div><div class="line"><span class="keyword">for</span> (<span class="keyword">auto</span> i : v) res.append(i);</div><div class="line"><span class="keyword">int</span> i = <span class="number">0</span>; <span class="keyword">while</span> (res[i] == <span class="string">'0'</span>) i++;</div><div class="line">res = res.substr(i);</div><div class="line"><span class="keyword">if</span> (res.size() == <span class="number">0</span>) res = <span class="string">"0"</span>;</div><div class="line"><span class="built_in">cout</span> &lt;&lt; res &lt;&lt; <span class="built_in">endl</span>;</div><div class="line"><span class="keyword">return</span> <span class="number">0</span>;</div><div class="line">&#125;</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Recover-the-Smallest-Number&quot;&gt;&lt;a href=&quot;#Recover-the-Smallest-Number&quot; class=&quot;headerlink&quot; title=&quot;Recover the Smallest Number&quot;&gt;&lt;/a&gt;Recover the Smallest Number&lt;/h1&gt;&lt;p&gt;Given a collection of number segments, you are supposed to recover the smallest number from them. For example, given &lt;code&gt;{ 32, 321, 3214, 0229, 87 }&lt;/code&gt;, we can recover many numbers such like &lt;code&gt;32-321-3214-0229-87&lt;/code&gt; or &lt;code&gt;0229-32-87-321-3214&lt;/code&gt; with respect to different orders of combinations of these segments, and the smallest number is &lt;code&gt;0229-321-3214-32-87&lt;/code&gt;.&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="PAT" scheme="https://verrickt.github.io/tags/PAT/"/>
    
  </entry>
  
  <entry>
    <title>PAT 1007 Maximum Subsequence Sum</title>
    <link href="https://verrickt.github.io/2020/03/11/PAT-1007-Maximum-Subsequence-Sum/"/>
    <id>https://verrickt.github.io/2020/03/11/PAT-1007-Maximum-Subsequence-Sum/</id>
    <published>2020-03-11T14:12:04.000Z</published>
    <updated>2020-03-11T14:57:10.182Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Maximum-Subsequence-Sum"><a href="#Maximum-Subsequence-Sum" class="headerlink" title="Maximum Subsequence Sum"></a>Maximum Subsequence Sum</h2><p>Given a sequence of K integers { N​1​​, N​2​​, …, N​K​​ }. A continuous subsequence is defined to be { N​i​​, N​i+1​​, …, N​j​​ } where 1≤i≤j≤K. The Maximum Subsequence is the continuous subsequence which has the largest sum of its elements. For example, given sequence { -2, 11, -4, 13, -5, -2 }, its maximum subsequence is { 11, -4, 13 } with the largest sum being 20.</p><p>Now you are supposed to find the largest sum, together with the first and the last numbers of the maximum subsequence.</p><a id="more"></a><p><strong>Input Specification</strong></p><p>Each input file contains one test case. Each case occupies two lines. The first line contains a positive integer K (≤10000). The second line contains K numbers, separated by a space.</p><p><strong>Output Specification</strong></p><p>For each test case, output in one line the largest sum, together with the first and the last numbers of the maximum subsequence. The numbers must be separated by one space, but there must be no extra space at the end of a line. In case that the maximum subsequence is not unique, output the one with the smallest indices i and j (as shown by the sample case). If all the K numbers are negative, then its maximum sum is defined to be 0, and you are supposed to output the first and the last numbers of the whole sequence. </p><p><strong>Sample Input</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">10</div><div class="line">-10 1 2 3 4 -5 -23 3 7 -21</div></pre></td></tr></table></figure><p><strong>Sample Output</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">10 1 4</div></pre></td></tr></table></figure></p><hr><p>分数线死活不出来，一为排解焦虑，二为准备复试，建了个小号重刷PAT。最大子列和这个问题自己想不出来，听别人的想法似懂非懂，抄的代码放久了还是看不懂，让我头疼不已。今天在往常的思路上多想了一步，迎刃而解。记录一番，以示庆祝。</p><p>根据历次的印象，应该用动规。根据历次的印象，应该用前一段的最大子列和与当前元素作为状态转换方程的参数。好了，到了这一步就卡住了。</p><p>以往我的思路是，<code>新的子列和=旧子列和+最后一项</code>。<code>旧子列和</code>和<code>最后一项</code>可正可负，但只要<code>最后一项</code>是正的，那么子列和就严格递增。可是这样的思路在后面是<code>-2,5</code>的时候就行不通了。以前走到这一步我就去求助搜索引擎了。</p><p>这次在上面的思路上稍微走了一步。加法是有交换律的。不妨把式子改写成<code>新的子列和=最后一项+旧子列和</code>。不考虑<code>最后一项</code>的正负，而去考虑<code>旧子列和</code>的正负。</p><ul><li>如果<code>旧子列和</code>为正，那么不管<code>最后一项</code>符号如何，新子列和一定比<code>最后一项</code>要大。</li><li>如果<code>旧子列和</code>为负，那么不管<code>最后一项</code>符号如何，新子列和一定比<code>最后一项</code>要小。</li></ul><p>如果一个<code>新子列和</code>比<code>最后一项</code>还要小，取仅含<code>最后一项</code>的子序列，则它比<code>新子列和</code>还要大。这时的<code>新子列和</code>并不是最大子列和。舍弃之。</p><p>把<code>最后一项</code>当作子列的第一项，重复上述的过程，则可以得到几个连续序列的<strong>正数</strong>和。最大子列和就藏在这些正数和当中。问题转化求几个数的最大值。这还用想？😛</p><p>针对这道题的要求改一改，就得到答案啦<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;vector&gt;</span></span></div><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cstdio&gt;</span></span></div><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></div><div class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</div><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></div><div class="line"><span class="function"></span>&#123;</div><div class="line"><span class="keyword">int</span> k;</div><div class="line"><span class="built_in">cin</span> &gt;&gt; k;</div><div class="line"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; v = <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;(k);</div><div class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; k; i++) <span class="built_in">scanf</span>(<span class="string">"%d"</span>, &amp;v[i]);</div><div class="line"><span class="keyword">int</span> i=<span class="number">0</span>, sum = <span class="number">0</span>;</div><div class="line"><span class="keyword">int</span> max = <span class="number">-1</span>;</div><div class="line">sum = <span class="number">0</span>;</div><div class="line"><span class="keyword">int</span> p = <span class="number">0</span>, q = k<span class="number">-1</span>;</div><div class="line"><span class="keyword">int</span> s = <span class="number">0</span>;</div><div class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;k;i++)</div><div class="line">&#123;</div><div class="line">sum += v[i];</div><div class="line"><span class="keyword">if</span> (sum&gt;max)</div><div class="line">&#123;</div><div class="line">max = sum;</div><div class="line">p = s;</div><div class="line">q = i;</div><div class="line">&#125;</div><div class="line"><span class="keyword">if</span> (sum&lt;<span class="number">0</span>)</div><div class="line">&#123;</div><div class="line">s = i+<span class="number">1</span>;</div><div class="line">sum = <span class="number">0</span>;</div><div class="line">&#125;</div><div class="line">&#125;</div><div class="line"><span class="keyword">if</span> (max &lt; <span class="number">0</span>) max = <span class="number">0</span>;</div><div class="line"><span class="built_in">printf</span>(<span class="string">"%d %d %d"</span>, max, v[p], v[q]);</div><div class="line"><span class="keyword">return</span> <span class="number">0</span>;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Maximum-Subsequence-Sum&quot;&gt;&lt;a href=&quot;#Maximum-Subsequence-Sum&quot; class=&quot;headerlink&quot; title=&quot;Maximum Subsequence Sum&quot;&gt;&lt;/a&gt;Maximum Subsequence Sum&lt;/h2&gt;&lt;p&gt;Given a sequence of K integers { N​1​​, N​2​​, …, N​K​​ }. A continuous subsequence is defined to be { N​i​​, N​i+1​​, …, N​j​​ } where 1≤i≤j≤K. The Maximum Subsequence is the continuous subsequence which has the largest sum of its elements. For example, given sequence { -2, 11, -4, 13, -5, -2 }, its maximum subsequence is { 11, -4, 13 } with the largest sum being 20.&lt;/p&gt;
&lt;p&gt;Now you are supposed to find the largest sum, together with the first and the last numbers of the maximum subsequence.&lt;/p&gt;
    
    </summary>
    
    
      <category term="PAT" scheme="https://verrickt.github.io/tags/PAT/"/>
    
  </entry>
  
  <entry>
    <title>「C++」踩坑</title>
    <link href="https://verrickt.github.io/2019/08/21/cpp-the-double-edged-sword/"/>
    <id>https://verrickt.github.io/2019/08/21/cpp-the-double-edged-sword/</id>
    <published>2019-08-21T14:41:50.000Z</published>
    <updated>2020-03-11T14:09:05.124Z</updated>
    
    <content type="html"><![CDATA[<p>因为PAT刷题的缘故,开始再次接触C++.用过OOP语言再回来用C十分不习惯,C++就顺手多了.C没有泛型,字符串也不好用,C++就好多了,STL的容器和模板完美结合,字符串类也堪用.</p><a id="more"></a><p>按照欲抑先扬的套路,下面就该说说缺点了.作为老资历的语言,C++有些部分真是让人摸不着头脑.比如非void函数可以不返回值<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">int do_something(int i)</div><div class="line">&#123;</div><div class="line">    int j = i+1;</div><div class="line">    // returns nothing;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p><p>我就在这上面被坑了.BST的构建函数忘记返回值,调了一个多小时.<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="function">node* <span class="title">insert</span><span class="params">(node* root,<span class="keyword">int</span> data)</span></span></div><div class="line"><span class="function"></span>&#123;</div><div class="line">    <span class="keyword">if</span>(root==<span class="literal">nullptr</span>)</div><div class="line">    &#123;</div><div class="line">        root = (node*)<span class="built_in">malloc</span>(<span class="keyword">sizeof</span>(node));</div><div class="line">        root-&gt;left = <span class="literal">nullptr</span>;</div><div class="line">        root-&gt;right = <span class="literal">nullptr</span>;</div><div class="line">        root-&gt;data=data;</div><div class="line">        <span class="keyword">return</span> root;</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">if</span>(data&lt;root-&gt;data)</div><div class="line">    &#123;</div><div class="line">        root-&gt;left = insert(root-&gt;left,data);</div><div class="line">    &#125; <span class="keyword">else</span> &#123;</div><div class="line">        root-&gt;right = insert(root-&gt;right,data);</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="comment">//return root</span></div><div class="line">    <span class="comment">//掉了这一句</span></div><div class="line">&#125;</div></pre></td></tr></table></figure></p><p>自己的机器上跑一直没问题,一提交就错.抓瞎搞了一个多小时,又被devcpp的智能提示搞得心态爆炸,切换回了Visual Studio开始调试.结果依旧,不过编译的输出里多了一行Warning<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">warning C4715: &apos;insert&apos;: not all control paths return a value</div></pre></td></tr></table></figure></p><p>把掉了的<code>return</code>语句加回去,AC了.</p><p><a href="https://softwareengineering.stackexchange.com/questions/153372/function-works-fine-without-return-value-in-c" target="_blank" rel="external">StackExchange</a>上有人问了同样的问题</p><quote> I forgot to write return 'a'; in function and return 0; in main function but its works fine in Code::Blocks.<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">#include &lt;iostream&gt;</div><div class="line">using namespace std;</div><div class="line">char show()</div><div class="line">&#123;</div><div class="line">    cout&lt;&lt;&quot;this is show function&quot;&lt;&lt;endl;</div><div class="line">&#125;</div><div class="line">int main()</div><div class="line">&#123;</div><div class="line">    show();</div><div class="line">&#125;</div></pre></td></tr></table></figure> i am using Code::Blocks 10.05 in Ubuntu 12.04. Why this happen and why does the same thing cause an error in TURBO C++?</quote><p>Answer:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">If a function is declared to return a value, and fails to do so, the result is undefined behavior (in C++). One possible result is seeming to work, which is pretty much what you&apos;re seeing here.</div><div class="line"></div><div class="line">As an aside, in C, you wouldn&apos;t actually have undefined behavior -- in C, you get undefined behavior only if you try to use the return value, and the function didn&apos;t specify a value to return.</div></pre></td></tr></table></figure><p>在其他的语言里,一般会直接报错编译失败,在C++里就变成了编译器警告.去devcpp鼓捣了一下,把warning打开了.我要是选个带VS的考场就不用折腾devcpp了.真的难用.</p><p>想到初学编程的时候,编辑器和IDE党的圣战.一方说平均水平高,一方说效率高,争起来无休无止的口水战.</p><p>不过现在嘛,因为</p><ol><li>好用的IDE成倍提升效率.</li><li>IDE降低了上下文切换的开销,但对工具链不熟悉还是会吃亏.</li></ol><p>搞熟了工具链的我选择IDE.</p><p>不过tab和space的扣税战到现在还没结束,IDE和编辑器的口水战不知道还要打多少年呢</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;因为PAT刷题的缘故,开始再次接触C++.用过OOP语言再回来用C十分不习惯,C++就顺手多了.C没有泛型,字符串也不好用,C++就好多了,STL的容器和模板完美结合,字符串类也堪用.&lt;/p&gt;
    
    </summary>
    
    
      <category term="CPP" scheme="https://verrickt.github.io/tags/CPP/"/>
    
  </entry>
  
</feed>
