<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Melchior on CLR</title>
  
  <subtitle>What one programmer can do in one month, two programmers can do in two months</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://verrickt.github.io/"/>
  <updated>2021-07-31T02:43:11.193Z</updated>
  <id>https://verrickt.github.io/</id>
  
  <author>
    <name>Verrickt</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>troubleshot-hyper-v</title>
    <link href="https://verrickt.github.io/2021/02/14/troubleshot-hyper-v/"/>
    <id>https://verrickt.github.io/2021/02/14/troubleshot-hyper-v/</id>
    <published>2021-02-13T16:31:50.000Z</published>
    <updated>2021-07-31T02:43:11.193Z</updated>
    
    <content type="html"><![CDATA[<figure class="highlight powershell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">(base) PS C:\Windows\system32&gt; Enable-WindowsOptionalFeature -Online -FeatureName Microsoft-Hyper-V -All                                                                    Enable-WindowsOptionalFeature : The source files could not be found.</div><div class="line">Use the <span class="string">"Source"</span> option to specify the location of the files that are required to restore the feature. <span class="keyword">For</span> more information on specifying a source location, see</div><div class="line">https://go.microsoft.com/fwlink/?LinkId=<span class="number">243077</span>.</div><div class="line">At line:<span class="number">1</span> char:<span class="number">1</span></div><div class="line">+ Enable-WindowsOptionalFeature -Online -FeatureName Microsoft-Hyper-V  ...</div><div class="line">+ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</div><div class="line">    + CategoryInfo          : NotSpecified: (:) [Enable-WindowsOptionalFeature], COMException</div><div class="line">    + FullyQualifiedErrorId : Microsoft.Dism.Commands.EnableWindowsOptionalFeatureCommand</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;figure class=&quot;highlight powershell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>ä½¿VSCodeçš„ä¸­çš„Jupyter Notebookè®¿é—®åŒcondaç¯å¢ƒä¸‹çš„ä¾èµ–</title>
    <link href="https://verrickt.github.io/2021/01/16/access-packages-from-jupyter--in-the-same-conda-environment-in-vscode/"/>
    <id>https://verrickt.github.io/2021/01/16/access-packages-from-jupyter--in-the-same-conda-environment-in-vscode/</id>
    <published>2021-01-16T07:33:16.000Z</published>
    <updated>2021-01-16T08:25:36.226Z</updated>
    
    <content type="html"><![CDATA[<p>ä½œä¸ºPythonçš„ä¸€ä¸ªä¾èµ–ç®¡ç†å·¥å…·ï¼ŒAnacondaå®ç°äº†Pipã€Œå…¨å±€å®‰è£…ã€å’Œvirtualenvã€Œæœ¬åœ°å®‰è£…ã€çš„æŠ˜è¡·ï¼Œå®ƒä½¿ç”¨ã€ŒæŒ‰åè®¿é—®ã€çš„æ–¹å¼å®ç°ç¯å¢ƒçš„éš”ç¦»ï¼Œä½¿ä½¿ç”¨è€…èƒ½å¤Ÿçµæ´»çš„éš”ç¦»å’Œå…±äº«ç¯å¢ƒã€‚</p><p>ç„¶è€Œè¦ä½¿ç”¨Anaconda+Jupyter+VSCodeè¿™ä¸€å¥—ç»„åˆæ‹³å¯æœ‰ä¸å°‘å‘ã€‚<br><a id="more"></a></p><p>æŒ‰ç…§å¸¸è¯†ï¼Œåœ¨ä¸€ä¸ªcondaç¯å¢ƒä¸‹å®‰è£…<code>Jupyter</code>ååº”è¯¥å¯ä»¥ç›´æ¥è®¿é—®å¯¹åº”ç¯å¢ƒä¸‹çš„åŒ…</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">conda create -n test python=3.8</div><div class="line">conda install pytorch</div></pre></td></tr></table></figure><p>åœ¨VSCodeä¸­é€‰æ‹©åä¸º<code>test</code>çš„condaç¯å¢ƒï¼Œæ–°å»ºJupyter notebookï¼Œæ‰§è¡Œ<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> torch</div></pre></td></tr></table></figure></p><p>æç¤ºæ‰¾ä¸åˆ°torchè¿™ä¸ªåŒ…<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">---------------------------------------------------------------------------</div><div class="line">ModuleNotFoundError                       Traceback (most recent call last)</div><div class="line">&lt;ipython-input-1-fbdd1ce2ac00&gt; in &lt;module&gt;</div><div class="line">----&gt; 1 import torch</div><div class="line"></div><div class="line">ModuleNotFoundError: No module named &apos;torch&apos;</div></pre></td></tr></table></figure></p><p>è€Œä»ç›´æ¥è¿è¡ŒPythonè§£é‡Šå™¨å´ä¸€åˆ‡æ­£å¸¸<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">Python 3.8.5 (default, Sep  3 2020, 21:29:08) [MSC v.1916 64 bit (AMD64)] :: Anaconda, Inc. on win32</div><div class="line">Type "help", "copyright", "credits" or "license" for more information.</div><div class="line"><span class="meta">&gt;</span>&gt;&gt; import torch</div><div class="line"><span class="meta">&gt;</span>&gt;&gt; torch.__version__</div><div class="line">'1.7.0'</div></pre></td></tr></table></figure></p><p>åœ¨notebookä¸­æ‰§è¡Œ<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> sys</div><div class="line">print(sys.executable)</div></pre></td></tr></table></figure></p><p>å‘ç°Jupyter notebookä½¿ç”¨äº†é»˜è®¤condaç¯å¢ƒä¸­çš„pythonä½œä¸ºkernel<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">C:\Users\anaconda3\python.exe</div></pre></td></tr></table></figure></p><p>è¦è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œéœ€è¦å°†<code>test</code>ç¯å¢ƒä¸‹çš„pythonæš´éœ²å‡ºæ¥ã€‚<br>æ‰§è¡Œ<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">conda activate test</div><div class="line">conda install nb_conda_kernels</div></pre></td></tr></table></figure></p><p>å®‰è£…å®Œæˆåé‡æ–°æ‰“å¼€VSCodeï¼Œåœ¨è¿è¡ŒJupyter notebookä¸­çš„ä»£ç ä¹‹å‰ï¼Œåœ¨å³ä¸Šè§’çš„Select Kernelä¸­åˆ‡æ¢åˆ°<code>test</code>ä¸­çš„Kernelï¼Œå³å¯æˆåŠŸå¯¼å…¥Pytorchã€‚</p><hr><p>åŸå› åˆ†æï¼š<br>VSCodeä¸­çš„Pythonæ‰©å±•åœ¨æ‰“å¼€<code>.ipynb</code>æ–‡ä»¶æ—¶ä½¿ç”¨çš„æ˜¯<code>base</code>ç¯å¢ƒä¸­çš„Jupyterï¼Œè¿™æ ·å½“ç„¶æ‰¾ä¸åˆ°<code>test</code>ç¯å¢ƒä¸­çš„pytorchäº†ã€‚è€Œåœ¨å®‰è£…äº†nb_conda_kernelsåï¼Œ<code>test</code>ç¯å¢ƒä¸­çš„ipykernelå‘<code>base</code>ä¸­çš„JupyteræŠ¥å‘Šäº†kernel specï¼Œä»è€Œä½¿å¾—<code>base</code>ä¸­çš„Jupyterå¯ä»¥é¡ºåˆ©çš„ä½¿ç”¨<code>test</code>ä¸­çš„ipykernelæ‰§è¡Œä»£ç ã€‚è€Œ<code>test</code>ä¸­çš„ipykernelè‡ªç„¶å¯ä»¥è®¿é—®åŒåœ¨<code>test</code>ä¸‹çš„pytorchäº†ã€‚æ›´å¤šè¯·å‚è§nb_conda_kernelsçš„<a href="https://github.com/Anaconda-Platform/nb_conda_kernels#Installation" target="_blank" rel="external">è¯´æ˜</a>ã€‚</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;ä½œä¸ºPythonçš„ä¸€ä¸ªä¾èµ–ç®¡ç†å·¥å…·ï¼ŒAnacondaå®ç°äº†Pipã€Œå…¨å±€å®‰è£…ã€å’Œvirtualenvã€Œæœ¬åœ°å®‰è£…ã€çš„æŠ˜è¡·ï¼Œå®ƒä½¿ç”¨ã€ŒæŒ‰åè®¿é—®ã€çš„æ–¹å¼å®ç°ç¯å¢ƒçš„éš”ç¦»ï¼Œä½¿ä½¿ç”¨è€…èƒ½å¤Ÿçµæ´»çš„éš”ç¦»å’Œå…±äº«ç¯å¢ƒã€‚&lt;/p&gt;
&lt;p&gt;ç„¶è€Œè¦ä½¿ç”¨Anaconda+Jupyter+VSCodeè¿™ä¸€å¥—ç»„åˆæ‹³å¯æœ‰ä¸å°‘å‘ã€‚&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Other" scheme="https://verrickt.github.io/categories/Other/"/>
    
    
      <category term="VSCode" scheme="https://verrickt.github.io/tags/VSCode/"/>
    
      <category term="Jupyter" scheme="https://verrickt.github.io/tags/Jupyter/"/>
    
      <category term="Anaconda" scheme="https://verrickt.github.io/tags/Anaconda/"/>
    
  </entry>
  
  <entry>
    <title>ä½¿ç”¨SJTUçš„Anacondaé•œåƒ</title>
    <link href="https://verrickt.github.io/2020/12/12/conda-mirror-from-sjtu/"/>
    <id>https://verrickt.github.io/2020/12/12/conda-mirror-from-sjtu/</id>
    <published>2020-12-12T06:07:56.000Z</published>
    <updated>2021-01-08T04:42:22.108Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://mirrors.tuna.tsinghua.edu.cn/" target="_blank" rel="external">æ¸…å</a>çš„Anacondaé•œåƒåœ¨åä¸œåœ°åŒºæ˜¯å¥½æ˜¯åï¼Œå¥½çš„æ—¶å€™ä¸€åˆ‡é¡ºåˆ©ï¼Œåçš„æ—¶å€™ä¸€ä¸ª24Kçš„åŒ…ä¸‹5åˆ†é’Ÿï¼›Anacondaçš„å®˜æ–¹æºåˆ™æ›´æƒ¨ï¼Œè¿repo.jsonéƒ½ä¸‹ä¸ä¸‹æ¥ï¼›<br>ä»Šå¤©åˆæ˜¯æ¸…åçš„æºå¤§å§¨å¦ˆçš„ä¸€å¤©ï¼šPytorchä¸‹è½½åˆ°100MBå°±è¶…æ—¶ï¼ŒæŠ˜è…¾äº†ä¸‰ä¸ªå°æ—¶ä¹Ÿè£…ä¸ä¸Šã€‚è€ŒåŒåœ¨åä¸œçš„<a href="https://mirrors.sjtug.sjtu.edu.cn/" target="_blank" rel="external">SJTU</a>çš„é•œåƒå°±å¾ˆç¨³å®šï¼Œè¿™æ¬¡å…¨çº¿æ¢æˆSJTUå•¦ã€‚<br><a id="more"></a><br>é¦–å…ˆæ‰§è¡Œ<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">conda config --set show_channel_urls yes</div></pre></td></tr></table></figure></p><p>åœ¨<code>/usr</code>ä¸‹ç”Ÿæˆ<code>.condarc</code>æ–‡ä»¶ï¼Œç„¶åå°†å…¶å†…å®¹æ›¿æ¢ä¸º<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">channels:</div><div class="line">  - defaults</div><div class="line">show_channel_urls: true</div><div class="line">channel_alias: https://anaconda.mirrors.sjtug.sjtu.edu.cn</div><div class="line">default_channels:</div><div class="line">  - https://anaconda.mirrors.sjtug.sjtu.edu.cn/pkgs/main</div><div class="line">  - https://anaconda.mirrors.sjtug.sjtu.edu.cn/pkgs/free</div><div class="line">  - https://anaconda.mirrors.sjtug.sjtu.edu.cn/pkgs/r</div><div class="line">  - https://anaconda.mirrors.sjtug.sjtu.edu.cn/pkgs/pro</div><div class="line">  - https://anaconda.mirrors.sjtug.sjtu.edu.cn/pkgs/msys2</div><div class="line">custom_channels:</div><div class="line">  conda-forge: https://anaconda.mirrors.sjtug.sjtu.edu.cn/cloud</div><div class="line">  msys2: https://anaconda.mirrors.sjtug.sjtu.edu.cn/cloud</div><div class="line">  bioconda: https://anaconda.mirrors.sjtug.sjtu.edu.cn/cloud</div><div class="line">  menpo: https://anaconda.mirrors.sjtug.sjtu.edu.cn/cloud</div><div class="line">  pytorch: https://anaconda.mirrors.sjtug.sjtu.edu.cn/cloud</div><div class="line">  simpleitk: https://anaconda.mirrors.sjtug.sjtu.edu.cn/cloud</div></pre></td></tr></table></figure></p><p>å†è¿è¡Œ<code>conda clean -i</code>æ¸…é™¤ç´¢å¼•ç¼“å­˜å³å¯ã€‚<br>ç»ˆäºè£…ä¸ŠPytorchäº†ï¼Œå¯å–œå¯è´ºğŸ˜‚</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://mirrors.tuna.tsinghua.edu.cn/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;æ¸…å&lt;/a&gt;çš„Anacondaé•œåƒåœ¨åä¸œåœ°åŒºæ˜¯å¥½æ˜¯åï¼Œå¥½çš„æ—¶å€™ä¸€åˆ‡é¡ºåˆ©ï¼Œåçš„æ—¶å€™ä¸€ä¸ª24Kçš„åŒ…ä¸‹5åˆ†é’Ÿï¼›Anacondaçš„å®˜æ–¹æºåˆ™æ›´æƒ¨ï¼Œè¿repo.jsonéƒ½ä¸‹ä¸ä¸‹æ¥ï¼›&lt;br&gt;ä»Šå¤©åˆæ˜¯æ¸…åçš„æºå¤§å§¨å¦ˆçš„ä¸€å¤©ï¼šPytorchä¸‹è½½åˆ°100MBå°±è¶…æ—¶ï¼ŒæŠ˜è…¾äº†ä¸‰ä¸ªå°æ—¶ä¹Ÿè£…ä¸ä¸Šã€‚è€ŒåŒåœ¨åä¸œçš„&lt;a href=&quot;https://mirrors.sjtug.sjtu.edu.cn/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;SJTU&lt;/a&gt;çš„é•œåƒå°±å¾ˆç¨³å®šï¼Œè¿™æ¬¡å…¨çº¿æ¢æˆSJTUå•¦ã€‚&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Other" scheme="https://verrickt.github.io/categories/Other/"/>
    
    
      <category term="Anaconda" scheme="https://verrickt.github.io/tags/Anaconda/"/>
    
  </entry>
  
  <entry>
    <title>åŠç›‘ç£å­¦ä¹ </title>
    <link href="https://verrickt.github.io/2020/08/04/semi-supervised-learning/"/>
    <id>https://verrickt.github.io/2020/08/04/semi-supervised-learning/</id>
    <published>2020-08-04T07:27:52.000Z</published>
    <updated>2021-01-11T12:07:29.647Z</updated>
    
    <content type="html"><![CDATA[<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML" async></script><h2 id="å‰è¨€"><a href="#å‰è¨€" class="headerlink" title="å‰è¨€"></a>å‰è¨€</h2><p>æœ¬æ–‡çš„æ˜¯å°æ¹¾å¤§å­¦2020æœºå™¨å­¦ä¹ <a href="https://youtu.be/fX_guE7JNnY" target="_blank" rel="external">Semi-supervised leraning</a>çš„ç¬”è®°ã€‚å¥‰è¡Œ<a href="why-semi-supervised.png">Lazy evaluation</a>ç­–ç•¥ï¼Œå¯¹è¿™äº›çŸ¥è¯†æ›´æ·±å±‚æ¬¡çš„æ¢ç©¶åªåœ¨<strong>ç»å¯¹å¿…è¦</strong>æ—¶å®Œæˆã€‚<br><a id="more"></a></p><p>ç°åœ¨æ˜¯æ‰€è°“çš„å¤§æ•°æ®æ—¶ä»£ï¼Œæ¯äººæ¯å¤©äº§ç”Ÿçš„æ•°æ®æˆåƒä¸Šä¸‡ã€‚å¯¹å•†ä¸šå…¬å¸æ¥è¯´ï¼Œæ•°æ®çš„æ”¶é›†å·²ç»ä¸æ˜¯é—®é¢˜ï¼Œä½†ç»™æ•°æ®æ·»åŠ æ ‡è®°çš„å·¥ä½œå› ä¸ºå…¶æ‰€éœ€è¦æ¶ˆè€—çš„äººåŠ›ç‰©åŠ›æˆä¸ºè€å¤§éš¾é—®é¢˜ï¼›è€Œåœ¨æœºå™¨å­¦ä¹ ä¸­ï¼Œå¸¦æ ‡è®°æ•°æ®çš„ä¸è¶³ä¼šç»™æ¨¡å‹å¼•å…¥biasã€‚ä¸€ä¸ªå¾ˆè‡ªç„¶çš„æƒ³æ³•æ˜¯ï¼Œèƒ½ä¸èƒ½ç”¨è¿™äº›ä¸å¸¦æ ‡è®°çš„æ•°æ®æ¥æé«˜æ¨¡å‹çš„æ³›åŒ–æ€§èƒ½å‘¢ï¼Ÿç­”æ¡ˆæ˜¯è‚¯å®šçš„ã€‚è¿™å°±æ˜¯åŠç›‘ç£å­¦ä¹ ã€‚</p><p><img src="/2020/08/04/semi-supervised-learning/introduction.png" alt="1"><br>åœ¨åŠç›‘ç£å­¦ä¹ ä¸­ï¼Œæ•°æ®åˆ†ä¸ºä¸¤å—ã€‚</p><script type="math/tex; mode=display">{(x^r,\hat y^r)}_{r=1}^R,{x^u}_{u=R}^{R+U}</script><p>å…¶ä¸­Ræ˜¯å¸¦æ ‡è®°æ•°æ®(labeled data)çš„æ•°é‡ï¼ŒUæ˜¯ä¸å¸¦æ ‡è®°æ•°æ®(unlabelled data)çš„æ•°é‡ã€‚Ué€šå¸¸è¿œè¿œå¤§äºUï¼›<br>æ ¹æ®testing dataæ˜¯å¦åŒ…æ‹¬unlabelled dataï¼Œå¯å°†åŠç›‘ç£å­¦ä¹ åˆ†ä¸ºä¸¤ç±»ã€‚</p><p><img src="/2020/08/04/semi-supervised-learning/why-semi-supervised.png" alt="2"><br>åŠç›‘ç£å­¦ä¹ ä¸ºä»€ä¹ˆæœ‰æ•ˆï¼Œä¸€ä¸ªæ¯”è¾ƒä»¤äººæ¥å—çš„è¯´æ³•æ˜¯ï¼Œæ— æ ‡ç­¾æ•°æ®çš„åˆ†å¸ƒå¯¹é—®é¢˜æœ‰äº›å¯å‘ã€‚<br>è€ŒåŠè‡ªåŠ¨å­¦ä¹ æœ‰æ²¡æœ‰ç”¨ï¼Œæœ‰å¤šå¤§ç”¨ï¼Œå–å†³äºå‡è®¾åšçš„å¥½ä¸å¥½ï¼›</p><h2 id="Semi-supervised-Learning-for-Generative-Model"><a href="#Semi-supervised-Learning-for-Generative-Model" class="headerlink" title="Semi-supervised Learning for Generative Model"></a>Semi-supervised Learning for Generative Model</h2><p>åœ¨<a href="../../../07/04/from-logistic-regression-to-neural-network/">äºŒå…ƒåˆ†ç±»</a>æ—¶ï¼Œæ›¾ç»ç”¨è¿‡ç”Ÿæˆæ¨¡å‹è§£å†³è¿™ä¸ªé—®é¢˜ã€‚<br><img src="/2020/08/04/semi-supervised-learning/recall.png" alt="4"><br>åœ¨åŠç›‘ç£å­¦ä¹ ä¸­ï¼Œåˆ†ç±»ä¾æ—§å¯ä»¥ç”¨ç”Ÿæˆæ¨¡å‹æ¥è§£å†³ã€‚<br><img src="/2020/08/04/semi-supervised-learning/semi-supervised-generative.png" alt="5"></p><p>å¤§ä½“çš„çš„æ€è·¯æ˜¯ï¼Œä½¿ç”¨unlabelled dataæ¥ä¿®æ­£å¯¹priorçš„ä¼°è®¡ï¼Œä»¥æœŸæ›´å°çš„æ³›åŒ–é”™è¯¯</p><p>å…·ä½“ç®—æ³•å¦‚ä¸‹:<br><img src="/2020/08/04/semi-supervised-learning/how-to-train-a-semi-supervised-generative-model.png" alt="6"></p><ol><li>åˆå§‹åŒ–å‚æ•°Î¸</li><li>å¯¹unlabelled dataè®¡ç®—å…¶å±äºæŸä¸€ä¸ªç±»åˆ«çš„åéªŒæ¦‚ç‡</li><li>æ›´æ–°å‚æ•°Î¸:<script type="math/tex; mode=display">P(C_1)=\frac{N_1+\sum_{x^u} P(C_1|x_u)}{N} \\\mu^1=\frac{1}{N_1}\sum_{x^r \in C_1}x^r+\frac{1}{\sum_{x^u}P(C_1|x^u)}\sum P(C_1|x^u)x^u</script>ç®—æ³•æœ€ç»ˆä¼šæ”¶æ•›ï¼Œä½†åˆå§‹åŒ–çš„å‚æ•°å€¼Î¸ä¼šå½±å“ç»“æœã€‚</li></ol><p><img src="/2020/08/04/semi-supervised-learning/why-does-it-converge.png" alt="7"></p><p>åœ¨ç›‘ç£å­¦ä¹ çš„æ—¶å€™ï¼Œæœ€å¤§ä¼¼ç„¶å‡½æ•°çš„ç»“æœæ˜¯æœ‰é—­å¼è§£çš„ï¼›<br>è€Œåœ¨åŠç›‘ç£å­¦ä¹ æ—¶ï¼Œunlabelled dataå¯¹åº”çš„ä¼¼ç„¶å‡½æ•°ä¾èµ–äºå½“å‰å‚æ•°Î¸ï¼Œæ‰€ä»¥åªèƒ½è¿­ä»£çš„æ¥æ±‚è§£ã€‚</p><hr><p>åŠç›‘ç£å­¦ä¹ é€šå¸¸åŸºäºä¸¤ä¸ªå‡è®¾</p><ul><li>Low-density Separation Assumption</li><li>Smoothness Assumption</li></ul><p>è¿™ä¸¤ä¸ªå‡è®¾æˆç«‹ä¸æˆç«‹ï¼Œå¯¹åŠç›‘ç£å­¦ä¹ çš„æ•ˆæœæ¯æ¯ç›¸å…³ã€‚</p><h2 id="Low-density-Separation-Assumption"><a href="#Low-density-Separation-Assumption" class="headerlink" title="Low-density Separation Assumption"></a>Low-density Separation Assumption</h2><p><img src="/2020/08/04/semi-supervised-learning/what-is-low-density.png" alt="8"></p><p> Low-density Separation Assumptionè®¤ä¸ºï¼Œåœ¨åˆ†ç•Œçº¿ä¸Šï¼Œæ•°æ®çš„å¯†åº¦æ˜¯ä½çš„ã€‚æ¢è¨€ä¹‹ï¼Œè¿™ä¸ªä¸–ç•Œæ˜¯éé»‘å³ç™½çš„ã€‚</p><p> åœ¨æŠ•å½±ä¸­ï¼Œå·¦è¾¹åˆ†ç•Œçº¿ä½ç½®çš„å¯†åº¦æ¯”å³è¾¹åˆ†ç•Œçº¿ä¸Šçš„å¯†åº¦æ›´ä½ï¼Œå› æ­¤è®¤ä¸ºå·¦ä¾§åˆ†ç•Œçº¿æ›´å¥½ã€‚</p><p> åŸºäºè¿™ä¸ªæƒ³æ³•ï¼Œæˆ‘ä»¬å¯ä»¥æå‡ºSelf trainingç®—æ³•ï¼š<br><img src="/2020/08/04/semi-supervised-learning/self-training.png" alt="9"></p><ol><li>æ ¹æ®labelled dataè®­ç»ƒä¸€ä¸ªmodel</li><li>ç”¨1ä¸­å¾—åˆ°çš„modelç»™ä¸€éƒ¨åˆ†çš„unlabelled dataæ‰“ä¸Šæ ‡è®°,ä½œä¸ºlabelled dataã€‚è¿™ä¸ªæ ‡è®°å«åšpseudo label(ä¼ªæ ‡è®°)</li><li>å›åˆ°1</li></ol><p>éœ€è¦æ³¨æ„çš„æ˜¯ä¼ªæ ‡è®°æ˜¯å¦å¯¹æ–°è®­ç»ƒçš„modelæœ‰ç”¨ã€‚</p><p>å¦‚æœåšregressionï¼Œpseudo labelå¹¶ä¸èƒ½è®©modelå­¦åˆ°æ–°ä¸œè¥¿ï¼Œæ‰€ä»¥å°±å®Œå…¨æ²¡æœ‰ç”¨å¤„ã€‚</p><p>å¦‚æœç”¨ç¥ç»ç½‘ç»œåšåˆ†ç±»ï¼Œç”¨soft labelä¹Ÿæ²¡æœ‰ç”¨å¤„<br><img src="/2020/08/04/semi-supervised-learning/self-training-pitfall-2.png" alt="10"></p><p>æ¢ä¸€ç§æ€è·¯ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥æŠŠLow-density Separation Assumptionå½“ä½œä¸€ä¸ªregularization term:<br><img src="/2020/08/04/semi-supervised-learning/entropy-regularizer.png" alt="13"></p><p>å¯¹äºä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒï¼Œæˆ‘ä»¬å¸Œæœ›å¯†åº¦è¶Šé›†ä¸­è¶Šå¥½ã€‚æ¦‚ç‡åˆ†å¸ƒçš„é›†ä¸­ç¨‹åº¦å¯ä»¥ç”¨é¦™å†œç†µæ¥è¡¡é‡ã€‚è¿™æ ·æŠŠé¦™å†œç†µå½“ä½œregularizationçš„ä¸€ä¸ªtermï¼Œå°±èƒ½é¼“åŠ±ç½‘ç»œæ‰¾åˆ°æˆ‘ä»¬æ‰€æœŸå¾…çš„å¯†åº¦åˆ†å¸ƒã€‚</p><h2 id="Smoothness-Assumption"><a href="#Smoothness-Assumption" class="headerlink" title="Smoothness Assumption"></a>Smoothness Assumption</h2><p><img src="/2020/08/04/semi-supervised-learning/smothness.png" alt="11"></p><p>Smoothness Assumptionè®¤ä¸ºå¯¹äºæ¥è¿‘çš„xï¼Œä»–ä»¬çš„æ ‡è®°yåº”è¯¥æ˜¯ç›¸è¿‘çš„ã€‚æ¢è¨€ä¹‹ï¼Œè¿‘æœ±è€…èµ¤è¿‘å¢¨è€…é»‘ï¼›</p><p>ä¸‹é¢æ˜¯Smoothness Assumptionçš„ä¸€ä¸ªä¾‹å­<br><img src="/2020/08/04/semi-supervised-learning/entropy-regularizer.png" alt="13"></p><p>é€šè¿‡â€œç›¸ä¼¼â€çš„ä¼ é€’æ€§ï¼Œmodelå¯ä»¥ç»™å¾ˆå¤šæ•°æ®æ‰“ä¸Šæ­£ç¡®çš„æ ‡ç­¾ã€‚</p><p><img src="/2020/08/04/semi-supervised-learning/what-is-smoothness.png" alt="12"><br>æ¯”è¾ƒæ­£å¼çš„è¯´æ³•æ˜¯ï¼Œè‹¥x1å’Œx2åœ¨ä¸€ä¸ªé«˜å¯†åº¦åŒºåŸŸä¹‹å†…éå¸¸æ¥è¿‘ï¼Œé‚£ä¹ˆy1å’Œy2å°±æ˜¯æ¥è¿‘çš„ï¼›</p><p><img src="/2020/08/04/semi-supervised-learning/clustering.png" alt="17"></p><p>è¿™å’Œclusteringæ¯”è¾ƒç±»ä¼¼ã€‚ä½†clusteringçš„æ•°ç›®å¹¶ä¸å¥½ç¡®å®šã€‚å› æ­¤å¼•å…¥äº†åŸºäºå›¾çš„æ–¹æ³•ã€‚</p><p><img src="/2020/08/04/semi-supervised-learning/graph-distance.png" alt="18"><br>å°†æ•°æ®çœ‹ä½œé«˜ç»´ç©ºé—´ä¸­çš„å›¾ï¼Œåªéœ€æŠŠå›¾å»ºç«‹èµ·æ¥å³å¯ã€‚</p><p>å…³äºå›¾çš„æ„å»ºæ–¹æ³•ï¼Œæœ‰å¦‚ä¸‹å‡ ç§<br><img src="/2020/08/04/semi-supervised-learning/how-to-construct-a-graph.png" alt="19"></p><p>å¯ä»¥çœ‹åˆ°ï¼Œé€šè¿‡æ„å»ºå›¾ï¼Œä¸€ä¸ªç‚¹çš„å½±å“å¯ä»¥æ‰©æ•£çš„å¾ˆå¤§ä¸€ç‰‡åŒºåŸŸï¼š<br><img src="/2020/08/04/semi-supervised-learning/graph-advantage.png" alt="20"></p><p>ä½†å›¾ä¹Ÿæœ‰åŠ£åŠ¿ã€‚å¦‚æœä¸èƒ½ç»™æ¯ä¸€ä¸ªè¿é€šåˆ†é‡æ”¶é›†åˆ°ä¸€ä¸ªæ•°æ®ç‚¹ï¼Œè¿™äº›unlabelled dataå°±æ— æ³•è¢«è®¡ç®—ç›¸ä¼¼åº¦ã€‚</p><p><img src="/2020/08/04/semi-supervised-learning/graph-disadvantage.png" alt="21"><br>æœ‰äº†å›¾ä¹‹åå°±å¯ä»¥è®¡ç®—å›¾çš„smoothnessäº†ã€‚è¿™æ—¶å¯ä»¥æŠŠsmoothnesså½“ä½œç½‘ç»œçš„ä¸€ä¸ªregularizer<br><img src="/2020/08/04/semi-supervised-learning/graph-smoothness.png" alt="22"></p><h2 id="Better-Represerentation"><a href="#Better-Represerentation" class="headerlink" title="Better Represerentation"></a>Better Represerentation</h2><p><img src="/2020/08/04/semi-supervised-learning/better-represerentation.png" alt="15"></p><p>å¯¹ä½¿ç”¨è€…æ¥è¯´ï¼Œæ•°æ®çš„å‘ˆç°æ–¹å¼å¯¹ä½¿ç”¨çš„ä¾¿åˆ©ç¨‹åº¦å½±å“å¾ˆå¤§ã€‚ä¾‹å¦‚ç½—é©¬æ•°å­—çš„é™¤æ³•å°±æ¯”é˜¿æ‹‰ä¼¯æ•°å­—çš„é™¤æ³•å›°éš¾å¾ˆå¤šã€‚</p><p>åœ¨æœºå™¨å­¦ä¹ ä¸­ä¹Ÿæ˜¯å¦‚æ­¤ã€‚ä½¿ç”¨æ›´å¥½çš„å‘ˆç°æ–¹å¼ä¼šè®©å­¦ä¹ äº‹åŠåŠŸå€ã€‚å…³äºå¦‚ä½•æ‰¾åˆ°æ›´å¥½çš„å‘ˆç°æ–¹å¼ï¼Œå¯ä»¥å‚è€ƒAutoencodersã€‚</p><p><img src="/2020/08/04/semi-supervised-learning/why-better-represerentation.png" alt="23"></p>]]></content>
    
    <summary type="html">
    
      &lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML&quot; async&gt;&lt;/script&gt;

&lt;h2 id=&quot;å‰è¨€&quot;&gt;&lt;a href=&quot;#å‰è¨€&quot; class=&quot;headerlink&quot; title=&quot;å‰è¨€&quot;&gt;&lt;/a&gt;å‰è¨€&lt;/h2&gt;&lt;p&gt;æœ¬æ–‡çš„æ˜¯å°æ¹¾å¤§å­¦2020æœºå™¨å­¦ä¹ &lt;a href=&quot;https://youtu.be/fX_guE7JNnY&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Semi-supervised leraning&lt;/a&gt;çš„ç¬”è®°ã€‚å¥‰è¡Œ&lt;a href=&quot;why-semi-supervised.png&quot;&gt;Lazy evaluation&lt;/a&gt;ç­–ç•¥ï¼Œå¯¹è¿™äº›çŸ¥è¯†æ›´æ·±å±‚æ¬¡çš„æ¢ç©¶åªåœ¨&lt;strong&gt;ç»å¯¹å¿…è¦&lt;/strong&gt;æ—¶å®Œæˆã€‚&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Machine learning" scheme="https://verrickt.github.io/categories/Machine-learning/"/>
    
    
      <category term="Machine learning" scheme="https://verrickt.github.io/tags/Machine-learning/"/>
    
      <category term="Semi-supervised learning" scheme="https://verrickt.github.io/tags/Semi-supervised-learning/"/>
    
  </entry>
  
  <entry>
    <title>æ”¯æŒå‘é‡æœº</title>
    <link href="https://verrickt.github.io/2020/07/31/support-vector-machine/"/>
    <id>https://verrickt.github.io/2020/07/31/support-vector-machine/</id>
    <published>2020-07-31T11:49:52.000Z</published>
    <updated>2021-01-11T12:07:36.831Z</updated>
    
    <content type="html"><![CDATA[<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML" async></script><h2 id="å‰è¨€"><a href="#å‰è¨€" class="headerlink" title="å‰è¨€"></a>å‰è¨€</h2><p>æ”¯æŒå‘é‡æœº(Support Vector Machine,SVM)æ˜¯ä¸€ä¸ªèƒ½æ±‚è§£é€šç”¨äºŒå…ƒåˆ†ç±»é—®é¢˜çš„æ¨¡å‹ã€‚SVMä½¿ç”¨Kernel trickï¼Œä½¿å¾—è‡ªå·±åœ¨èƒ½å¤Ÿè¡¨ç¤ºéçº¿æ€§å‡½æ•°çš„åŒæ—¶Lossæ˜¯æœ‰é—­å¼è§£çš„Convex functionã€‚ä¹Ÿæœ‰äººæŠŠSVMå½“ä½œçº¿æ€§æ¨¡å‹å‘ç¥ç»ç½‘ç»œè¿‡æ¸¡çš„ä¸­é—´é˜¶æ®µã€‚</p><a id="more"></a><h2 id="SVM"><a href="#SVM" class="headerlink" title="SVM"></a>SVM</h2><p><img src="/2020/07/31/support-vector-machine/What-is-SVM.png" alt="What is SVM"></p><p>å¯¹äº</p><script type="math/tex; mode=display">D = \{(x_1,y_1),(x_2,y_2)...(x_n,y_n)\} , y_i \in \{-1,+1\}</script><p>çš„åˆ†ç±»é—®é¢˜ï¼ŒSVMè¯•å›¾åœ¨nç»´ç©ºé—´ä¸Šæ‰¾ä¸€ä¸ªè¶…å¹³é¢ï¼Œ<code>wx+b</code>ï¼Œä½¿å¾—</p><ul><li>å½“wx_i+bâ‰¥1æ—¶ï¼Œyi=+1</li><li>å½“wx_i+bâ‰¤1æ—¶ï¼Œyi=-1</li></ul><p>å¹¶ä¸”è¶…å¹³é¢åº”è¯¥ç¦»ä¸¤ä¾§æœ€è¿‘çš„ç‚¹éƒ½å°½å¯èƒ½è¿œã€‚è§£å‡ºè¿™ä¸€ç»„wå’Œbå°±è§£å‡ºäº†SVMã€‚</p><p>å…¶å®å¦‚æœåœ¨å°†wå’Œbå¹¶åœ¨ä¸€èµ·ä½œä¸ºæ–°çš„w,å°†xåæ¥1,åˆ™SVMå¯ä»¥è¿›ä¸€æ­¥ç®€åŒ–ä¸º<code>wx</code>:</p><script type="math/tex; mode=display">f(x)= \sum_i w_ix_i+b = \left[ \begin{array}{c} w \\ b\end{array} \right] \cdot\left[ \begin{array}{c} x \\ 1\end{array} \right]</script><h3 id="Hinge-loss"><a href="#Hinge-loss" class="headerlink" title="Hinge loss"></a>Hinge loss</h3><p><img src="/2020/07/31/support-vector-machine/delta-loss.png" alt="delta-loss"></p><p>å›åˆ°åˆ†ç±»é—®é¢˜æœ¬èº«ï¼Œé€šå¸¸Lossä¼šé€‰æ‹©åœ¨æ•´ä¸ªè®­ç»ƒé›†ä¸Šåˆ†ç±»é”™è¯¯çš„æ¬¡æ•°ï¼š</p><script type="math/tex; mode=display">L(f)=\sum_i \delta(g(x^n) \not ={\hat{y}^n})</script><p>å…¶ä¸­<code>Î´</code>å‡½æ•°çš„ä¸ºçœŸæ—¶å€¼ä¸º1ï¼Œåä¹‹å€¼ä¸º0ã€‚<br>æ­£å¦‚<a href="../../../07/04/from-logistic-regression-to-neural-network/">å¯¹ç‡å›å½’</a>ä¸­æ‰€è¯´ï¼Œ<br>åœ¨åˆ†ç±»é—®é¢˜ä¸Šä½¿ç”¨è¿™ç±»å‡½æ•°æ˜¯ä¸åˆ©äºè®­ç»ƒçš„ï¼Œå› æ­¤å¸Œæœ›æ‰¾åˆ°ä¸€ä¸ªå‡½æ•°</p><script type="math/tex; mode=display">l(f(x^n),\hat y^n)</script><p>æ¥è¿‘ä¼¼ä»£æ›¿<code>Î´</code>å‡½æ•°ã€‚</p><p>hinge losså°±æ˜¯è¿™æ ·çš„ä¸€ä¸ªå‡½æ•°ã€‚ä»–æ˜¯è¿™ä¹ˆå®šä¹‰çš„ï¼š</p><script type="math/tex; mode=display">l(f(x^n),\hat y^n)=\max(0,1-\hat y^nf(x))</script><p><img src="/2020/07/31/support-vector-machine/comparsion-of-loss.png" alt="comparsion of loss"></p><p>å›¾ä¸­ç´«è‰²çš„çº¿å°±æ˜¯hinge lossã€‚æ¨ªåæ ‡1å¤„ï¼Œhinge lossæŠŠä¸¤æ¡ç›´çº¿è¿æ¥èµ·æ¥ï¼Œå°±åƒæ˜¯è¿æ¥éª¨å¤´çš„å…³èŠ‚(hinge)ï¼Œå› æ­¤å¾—åã€‚</p><p>ä»å›¾ä¸Šå¯ä»¥çœ‹å‡º</p><ul><li>MSEéšç€æ¨ªåæ ‡çš„å¢åŠ lossåè€Œå¢åŠ ï¼Œæ ¹æœ¬ä¸é€‚ç”¨</li><li>sigmoid+square losså¯¹æ¨ªåæ ‡çš„å˜åŒ–ä¸æ•æ„Ÿ(saturate)ï¼Œè®­ç»ƒèµ·æ¥ä¹Ÿä¸å¥½åš</li><li>sigmoid+cross entropyä¼šç§¯æçš„å°†åæ ‡ç‚¹å¾€å³æ¨ï¼Œä½†å…¶å®åªè¦æ¨ªåæ ‡è¶…è¿‡äº†1å°±å·²ç»è¶³å¤Ÿäº†</li></ul><p><img src="/2020/07/31/support-vector-machine/entropy-vs-hinge.png" alt="entropy-vs-hinge"></p><p>cross entropyå¥½åƒå®Œç¾ä¸»ä¹‰è€…ï¼Œè€Œhinge lossä¼šæƒ³ç€åŠæ ¼å°±å¥½ã€‚æ—¢ç„¶åœ¨ç°å®ä¸–ç•Œé‡Œä¸¤è€…çš„æ€§èƒ½æ˜¯ä¸€è‡´çš„ï¼Œæˆ‘ä»¬ä¹Ÿå°±ä¹å¾—çœä¸‹è¿™äº›æ— ç•çš„å¼€é”€ã€‚</p><h3 id="Gradient-descent"><a href="#Gradient-descent" class="headerlink" title="Gradient descent"></a>Gradient descent</h3><p><img src="/2020/07/31/support-vector-machine/gradient-descent.png" alt="gradient-descent"></p><p>hinge losså¯¹wçš„åå¾®åˆ†åï¼Œå¾—åˆ°æ¢¯åº¦ä¸‹é™çš„å…¬å¼</p><script type="math/tex; mode=display">w_{i+1}=w_i-\eta\sum_n c^n(w)x_i^n</script><p>å…¶ä¸­</p><script type="math/tex; mode=display">c^n(w) = -\delta(\hat y^nf(x^n)<1)\hat y^nx_i</script><h3 id="soft-margin"><a href="#soft-margin" class="headerlink" title="soft margin"></a>soft margin</h3><p>å°†æ•´ä½“çš„æŸå¤±å‡½æ•°L</p><script type="math/tex; mode=display">L(f)=l(0,1-\hat y^nf(x)) + \lambda|w|_2=\sum_n \max(0,1-\hat y^nf(x)) + \lambda|w|_2</script><p>ä¸­çš„lç”¨Îµä»£æ›¿ï¼Œå¾—åˆ°ä¸‹åˆ—å½¢å¼<br><img src="/2020/07/31/support-vector-machine/slack-variable.png" alt="slack-variable"></p><script type="math/tex; mode=display">L = \sum_n\epsilon^n+\lambda|w|_2</script><script type="math/tex; mode=display">\hat y^nf(x) \ge1-\epsilon^n</script><p>è€Œä¸æ˜¯ç¡¬æ€§è§„å®š</p><script type="math/tex; mode=display">\hat y^nf(x) \ge1</script><p><code>Îµ</code>ä½œä¸ºè¾¹ç•Œæ¡ä»¶çš„æ”¾æ¾ï¼Œè¢«ç§°ä½œæ¾å¼›å˜é‡(slack variable)</p><h3 id="Support-vector"><a href="#Support-vector" class="headerlink" title="Support vector"></a>Support vector</h3><p>SVMçš„wæ˜¯æ‰€æœ‰è®­ç»ƒæ ·æœ¬ç‚¹çš„çº¿æ€§ç»„åˆï¼Œå¹¶ä¸”å¤šæ•°ç‚¹çš„ç³»æ•°éƒ½æ˜¯0(sparsity)ã€‚</p><p>è¿™ä¸ªæ€§è´¨å¯ä»¥ä»KKTç³»æ•°åæ˜ å‡ºæ¥<br><img src="/2020/07/31/support-vector-machine/traditonal-support-vector.png" alt="traditonal-support-vector"></p><p>å½“ç„¶ï¼Œå®ƒä¹Ÿå¯ä»¥ä»å¦ä¸€ä¸ªè§’åº¦è¯´æ˜ã€‚<br><img src="/2020/07/31/support-vector-machine/support-vector.png" alt="support-vector"><br>è€ƒè™‘æ¢¯åº¦ä¸‹é™çš„å¼å­</p><script type="math/tex; mode=display">w_{i+1}=w_i-\eta\sum_n c^n(w)x_i^n</script><p>å…¶ä¸­</p><script type="math/tex; mode=display">c^n(w)  =\frac{\partial l(f(x^n),\hat y^n)}{\partial f(x^n)}=-\delta(\hat y^nf(x^n)<1)\hat y^nx_i</script><p>å½“wè¢«åˆå§‹åŒ–ä¸º0å‘é‡æ—¶ï¼Œæ¯ä¸€æ¬¡å‚æ•°æ›´æ–°åŠ ä¸Šäº†ä¸€ä¸ªæ ·æœ¬ç‚¹çš„çº¿æ€§ç»„åˆï¼›è€Œhinge lossæœ‰å¤§çº¦ä¸€åŠçš„å®šä¹‰åŸŸé‡Œå¾®åˆ†æ˜¯0ï¼Œå› æ­¤å¤§éƒ¨åˆ†ç³»æ•°ä¼šä¸º0</p><h2 id="Kernel-method"><a href="#Kernel-method" class="headerlink" title="Kernel method"></a>Kernel method</h2><p><img src="/2020/07/31/support-vector-machine/kernel-function.png" alt="kernel function"></p><p>æ—¢ç„¶wæ˜¯Xçš„ä¸€ä¸ªçº¿æ€§ç»„åˆï¼Œå°†å…¶å†™ä½œ<code>w=XÎ±</code>ã€‚<br>å¸¦å…¥f(x)</p><script type="math/tex; mode=display">f(x)=w^\top x = \sum_n \alpha_n(x^n \cdot x)</script><p>å°†x^nä¸xçš„å†…ç§¯ç”¨å‡½æ•°K(kernel function)è¡¨ç¤ºï¼Œåˆ™</p><script type="math/tex; mode=display">f(x)=\sum_n \alpha_nK(x^n,x)</script><p><img src="/2020/07/31/support-vector-machine/kernel-trick.png" alt="kernel trick"><br>å°†å¾—åˆ°çš„f(x)å¸¦å…¥lossï¼Œå‘ç°L(f)ä¸xå·²ç»æ— å…³äº†ï¼Œå¯ä»¥ç›´æ¥è¿›è¡Œæ¢¯åº¦ä¸‹é™ã€‚è¿™å°±æ˜¯Kernel trick<br><img src="/2020/07/31/support-vector-machine/useful-kernel-trick.png" alt="useful kenel trick"></p><p>è‹¥æˆ‘ä»¬å…ˆå¯¹xåšfeature transform <code>Ï†(x)</code>ï¼Œ<code>Ï†(x)Â·Ï†(z)</code>æœ‰æ—¶å¯ä»¥è¡¨ç¤ºä¸ºK(x,z)ã€‚è¿™å³æ‰©å±•äº†SVMçš„è¡¨è¿°èƒ½åŠ›ï¼Œä¹Ÿæé«˜äº†æ•ˆç‡ã€‚</p><p>åœ¨ä½¿ç”¨RBF Kernelæ—¶ï¼Œå¯ä»¥åœ¨æ— ç©·ç»´ç©ºé—´åšå†…ç§¯</p><p><img src="/2020/07/31/support-vector-machine/RBF-Kernel.png" alt="RBF-Kernel"></p><h2 id="SVM-and-Neural-network"><a href="#SVM-and-Neural-network" class="headerlink" title="SVM and Neural network"></a>SVM and Neural network</h2><p>SVMä¹Ÿå¯ä»¥è¢«çœ‹ä½œæ˜¯æœ‰ä¸€å±‚hidden unitçš„ç¥ç»ç½‘ç»œ:</p><p><img src="/2020/07/31/support-vector-machine/SVM-as-NN.png" alt="SVM-as-NN"></p><p>æ¯ä¸€ä¸ªneuralçš„weightæ˜¯x^nä¸xçš„å†…ç§¯ï¼Œæ‰€æœ‰neuralçš„è¾“å‡ºç»è¿‡tanhåå†ä»¥Î±ä¸ºæƒå€¼æ±‚å’Œå°±å¾—åˆ°è¾“å‡ºäº†ã€‚</p><p><img src="/2020/07/31/support-vector-machine/SVM-and-NN.png" alt="SVM-and-NN"><br>å®é™…ä¸Šï¼ŒSVMä¹Ÿå¯ä»¥çœ‹ä½œæ˜¯ä¸€ä¸ªç¥ç»ç½‘ç»œã€‚</p><ul><li>Kernel functionç›¸å½“äºæ˜¯ç¥ç»ç½‘ç»œä¸­åšfeature transformçš„hidden layer</li><li>SVMå­¦åˆ°çš„åˆ†ç±»å™¨ä¸ç¥ç»ç½‘ç»œçš„è¾“å‡ºå±‚å­¦åˆ°çš„åˆ†ç±»å™¨æ˜¯ä¸€è‡´çš„ã€‚</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML&quot; async&gt;&lt;/script&gt;

&lt;h2 id=&quot;å‰è¨€&quot;&gt;&lt;a href=&quot;#å‰è¨€&quot; class=&quot;headerlink&quot; title=&quot;å‰è¨€&quot;&gt;&lt;/a&gt;å‰è¨€&lt;/h2&gt;&lt;p&gt;æ”¯æŒå‘é‡æœº(Support Vector Machine,SVM)æ˜¯ä¸€ä¸ªèƒ½æ±‚è§£é€šç”¨äºŒå…ƒåˆ†ç±»é—®é¢˜çš„æ¨¡å‹ã€‚SVMä½¿ç”¨Kernel trickï¼Œä½¿å¾—è‡ªå·±åœ¨èƒ½å¤Ÿè¡¨ç¤ºéçº¿æ€§å‡½æ•°çš„åŒæ—¶Lossæ˜¯æœ‰é—­å¼è§£çš„Convex functionã€‚ä¹Ÿæœ‰äººæŠŠSVMå½“ä½œçº¿æ€§æ¨¡å‹å‘ç¥ç»ç½‘ç»œè¿‡æ¸¡çš„ä¸­é—´é˜¶æ®µã€‚&lt;/p&gt;
    
    </summary>
    
      <category term="Machine learning" scheme="https://verrickt.github.io/categories/Machine-learning/"/>
    
    
      <category term="Machine learning" scheme="https://verrickt.github.io/tags/Machine-learning/"/>
    
      <category term="SVM" scheme="https://verrickt.github.io/tags/SVM/"/>
    
  </entry>
  
  <entry>
    <title>ä¸»æˆåˆ†åˆ†æ</title>
    <link href="https://verrickt.github.io/2020/07/31/principal-component-analysis/"/>
    <id>https://verrickt.github.io/2020/07/31/principal-component-analysis/</id>
    <published>2020-07-31T11:49:41.000Z</published>
    <updated>2021-01-11T12:07:36.808Z</updated>
    
    <content type="html"><![CDATA[<h2 id="å‰è¨€"><a href="#å‰è¨€" class="headerlink" title="å‰è¨€"></a>å‰è¨€</h2><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML" async></script><p>ä¸»æˆåˆ†åˆ†æ(Principal component analysis,PCA)æ˜¯ä¸€ç§å¸¸ç”¨çš„æ— ç›‘ç£å­¦ä¹ ç®—æ³•ã€‚ç»è¿‡PCAåçš„æ•°æ®ä¼šåœ¨å®ƒæ‰€å¤„ç†çš„ç»´åº¦ä¸Šä¼šä¸ç›¸å…³(decorelation)ã€‚ è€ŒPCAæ‰€é€‰çš„ç»´åº¦åˆé€šå¸¸æ¯”æœ¬æ¥çš„ç»´åº¦è¦å°ï¼Œå› æ­¤ä¹Ÿå¯ä»¥ç”¨æ¥åšé™ç»´å¤„ç†ã€‚</p><a id="more"></a><h2 id="PCA"><a href="#PCA" class="headerlink" title="PCA"></a>PCA</h2><p>PCAçš„æƒ³æ³•å¾ˆæœ´ç´ ã€‚å®ƒè¯•å›¾æ‰¾åˆ°ä¸€ä¸ªçº¿æ€§å˜åŒ–Wï¼Œä½¿å¾—ç»è¿‡å˜åŒ–åçš„å‘é‡<code>z=Wx</code>åœ¨å„ä¸ªç»´åº¦<code>z_i</code>ä¸Šéƒ½æ‹¥æœ‰æœ€å¤§çš„æ–¹å·®ã€‚</p><p><img src="/2020/07/31/principal-component-analysis/maximum-variance.png" alt="maximum variance"></p><h2 id="Constraints"><a href="#Constraints" class="headerlink" title="Constraints"></a>Constraints</h2><p>ä»…ä»…è¦æ±‚æ–¹å·®æœ€å¤§è€Œä¸åšä»»ä½•é™åˆ¶æ˜¯ä¸è¡Œçš„ã€‚æˆ‘ä»¬æ€»æ˜¯å¯ä»¥è®©<code>w_i</code>å«æœ‰æ— ç©·ä»è€Œä½¿æ–¹å·®å˜ä¸ºæ— ç©·å¤§ï¼Œä½†è¿™æ²¡æœ‰æ„ä¹‰ã€‚</p><p>å› æ­¤é™åˆ¶<code>w_i</code>çš„æ¨¡æ˜¯1ã€‚<br>åŒæ ·çš„ï¼Œå½“æ‰¾å‡ºä½¿å¾—ç¬¬iç»´çš„æ–¹å·®æœ€å¤§çš„å‚æ•°<code>w_i</code>åï¼Œæœºå™¨å¯ä»¥æŠŠè¿™ç»„å‚æ•°<code>w_i</code>ç”¨åœ¨ç¬¬jç»´ä¸Šæ¥â€œå·æ‡’â€ï¼Œå› æ­¤é™åˆ¶ä»»æ„çš„å‚æ•°æ˜¯å‚ç›´çš„ã€‚</p><p><img src="/2020/07/31/principal-component-analysis/constraint.png" alt="constraint"></p><h2 id="Dimension-reduction"><a href="#Dimension-reduction" class="headerlink" title="Dimension reduction"></a>Dimension reduction</h2><h3 id="to-1-dimension"><a href="#to-1-dimension" class="headerlink" title="to 1 dimension"></a>to 1 dimension</h3><p>å…ˆè€ƒè™‘ä¸€ç»´çš„æƒ…å†µã€‚</p><script type="math/tex; mode=display">z_1=w_1'x,å…¶ä¸­ (w_1')^\top w_1'=1</script><p><img src="/2020/07/31/principal-component-analysis/z1-var.png" alt="z1_var"></p><script type="math/tex; mode=display">\mathrm{Var}(z_1)=\sum_{z_{i}}(z_1-\bar z)^2=\sum_x(w_1'x-w_1'\bar x)^2=\sum_x(w_1'(x- \bar x))^2 \\=\sum_x(w_1')^\top(x- \bar x)(x- \bar x)^\top w_1' \\=(w_1')^\top\left(\sum_x(x-\bar x)(x-\bar x)^\top\right)w_1' \\=(w_1')^\top\mathrm{Cov}(x)w_1'</script><p><img src="/2020/07/31/principal-component-analysis/w1-eigenvector.png" alt="w1-eigenvector"></p><p>æ„é€ æ‹‰æ ¼æœ—æ—¥å‡½æ•°ï¼Œå…¶ä¸­S=Cov(x),</p><script type="math/tex; mode=display">L(w_1')=(w_1')^\top S w_1'-\alpha((w_1')^\top w_1'-1) \\\frac{\partial L}{\partial w}=0 \rArr Sw_1'-\alpha w_1'\rArr Sw_1' = \alpha w_1'\rArr \alphaæ˜¯Sçš„ç‰¹å¾å€¼,w'æ˜¯å¯¹åº”çš„ç‰¹å¾å‘é‡</script><p>å¸¦å›åˆ°ä¸Šå¼</p><script type="math/tex; mode=display">(w_1')^\top\mathrm{Cov}(x)w_1'= (w_1')^\top Sw_1' = (w_1')^\top \alpha w_1'= \alpha (w_1')^\top w_1'=\alpha</script><p><code>Î±</code>çš„æœ€å¤§å€¼ï¼Œå³æ˜¯Cov(x)æœ€å¤§çš„ç‰¹å¾å€¼ã€‚æ­¤æ—¶çš„<code>w1&#39;</code>ä¸º<code>Î±</code>å¯¹åº”çš„ç‰¹å¾å‘é‡</p><p>å¯¹äºäºŒå…ƒçš„æƒ…å†µä¹Ÿç±»ä¼¼</p><h3 id="to-2-dimensions"><a href="#to-2-dimensions" class="headerlink" title="to 2 dimensions"></a>to 2 dimensions</h3><p><img src="/2020/07/31/principal-component-analysis/z2_lagrange_operator.png" alt="z2_lagrange_operator"></p><p>æ‹‰æ ¼æœ—æ—¥å‡½æ•°</p><script type="math/tex; mode=display">g(w_2)=(w_2')^\top S w_2 - \alpha((w_2')^\top w_2' = 1) - \beta((w_2')^\top w_1')</script><p>è€Œ</p><script type="math/tex; mode=display">(w_2')^\top w_2' = 1 \\(w_1')^\top w_2' = 0 \\(w_1')^\top Sw_2'=((w_1')^\top Sw_2')^\top=(w_2')^\top S^\top w_1  \\=(w_2')^\top S w_1 = (w_2')^\top \alpha w_1 \\=\alpha  (w_2')^\top w_1 = 0</script><p>æ‰€ä»¥ç³»æ•°æ‹‰æ ¼æœ—æ—¥å‡½æ•°çš„åªå‰©ä¸‹<code>Î²</code>.</p><p><img src="/2020/07/31/principal-component-analysis/z2_simplified.png" alt="z2_simplified"></p><p><code>Î²</code>çš„æœ€å¤§å€¼ï¼Œå³æ˜¯Cov(x)ç¬¬äºŒå¤§çš„ç‰¹å¾å€¼ã€‚æ­¤æ—¶çš„<code>w2&#39;</code>ä¸º<code>Î²</code>å¯¹åº”çš„ç‰¹å¾å‘é‡</p><h3 id="to-k-dimensions"><a href="#to-k-dimensions" class="headerlink" title="to k dimensions"></a>to k dimensions</h3><p>æŠŠPCAæ¨å¹¿åˆ°kç»´ï¼Œåˆ™<code>W</code>æ˜¯<code>Cov(X)</code>çš„æœ€å¤§çš„kä¸ªç‰¹å¾å€¼æ‰€å¯¹åº”çš„ç‰¹å¾å‘é‡çš„è¡ŒçŸ©é˜µã€‚</p><h2 id="Decorelation"><a href="#Decorelation" class="headerlink" title="Decorelation"></a>Decorelation</h2><p>PCAå¤„ç†åçš„zçŸ©é˜µçš„å„ä¸ªç»´åº¦ä¹‹é—´æ˜¯æ²¡æœ‰ç›¸äº’å…³ç³»çš„ã€‚</p><p><img src="/2020/07/31/principal-component-analysis/decorelation.png" alt="decorelation"></p><script type="math/tex; mode=display">Cov(z)=\sum(z-\bar{z})(z-\bar{z})^\top=\sum_xW(x- \bar x)(x- \bar x)^\top W^\top=WCov(x)W^\top</script><p>å°†Wå±•å¼€ï¼Œ</p><script type="math/tex; mode=display">Cov(z)=WS\left[w^1,w^2...w^k\right]</script><script type="math/tex; mode=display">=W\left[Sw^1,Sw^2...Sw^k\right]</script><script type="math/tex; mode=display">=W\left[\lambda_1w^1,\lambda_2w^2...\lambda_kw^k\right]</script><script type="math/tex; mode=display">=\left[w^1,w^2...w^k\right]^\top \left[\lambda_1w^1,\lambda_2w^2...\lambda_kw^k\right]</script><script type="math/tex; mode=display">\forall i, (w^i)^\top w^i = 1</script><script type="math/tex; mode=display">\forall i\not ={j}, (w^i)^\top w^j = 0</script><p>å› æ­¤</p><script type="math/tex; mode=display">Cov(z)=diag(\lambda_1,\lambda_2,...,\lambda_k)</script><p>åæ–¹å·®çŸ©é˜µä¸ºå¯¹è§’é˜µï¼Œæ‰€ä»¥ä¸åŒåˆ†é‡ä¹‹é—´æ²¡æœ‰çº¿æ€§çš„ç›¸å…³æ€§ã€‚</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;å‰è¨€&quot;&gt;&lt;a href=&quot;#å‰è¨€&quot; class=&quot;headerlink&quot; title=&quot;å‰è¨€&quot;&gt;&lt;/a&gt;å‰è¨€&lt;/h2&gt;&lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML&quot; async&gt;&lt;/script&gt;

&lt;p&gt;ä¸»æˆåˆ†åˆ†æ(Principal component analysis,PCA)æ˜¯ä¸€ç§å¸¸ç”¨çš„æ— ç›‘ç£å­¦ä¹ ç®—æ³•ã€‚ç»è¿‡PCAåçš„æ•°æ®ä¼šåœ¨å®ƒæ‰€å¤„ç†çš„ç»´åº¦ä¸Šä¼šä¸ç›¸å…³(decorelation)ã€‚ è€ŒPCAæ‰€é€‰çš„ç»´åº¦åˆé€šå¸¸æ¯”æœ¬æ¥çš„ç»´åº¦è¦å°ï¼Œå› æ­¤ä¹Ÿå¯ä»¥ç”¨æ¥åšé™ç»´å¤„ç†ã€‚&lt;/p&gt;
    
    </summary>
    
      <category term="Machine learning" scheme="https://verrickt.github.io/categories/Machine-learning/"/>
    
    
      <category term="Machine learning" scheme="https://verrickt.github.io/tags/Machine-learning/"/>
    
      <category term="PCA" scheme="https://verrickt.github.io/tags/PCA/"/>
    
  </entry>
  
  <entry>
    <title>ä½¿ç”¨CUDAä¸ºTensorflowåŠ é€Ÿ</title>
    <link href="https://verrickt.github.io/2020/07/28/gpu-accelerated-tranining-with-cuda/"/>
    <id>https://verrickt.github.io/2020/07/28/gpu-accelerated-tranining-with-cuda/</id>
    <published>2020-07-28T13:39:05.000Z</published>
    <updated>2021-01-11T12:07:36.732Z</updated>
    
    <content type="html"><![CDATA[<p>æ¢¯åº¦ä¸‹é™æ³•å¤§éƒ¨åˆ†æ—¶é—´éƒ½åœ¨è¿›è¡Œå‘é‡å’ŒçŸ©é˜µè¿ç®—ã€‚è¿™äº›è¿ç®—æ˜¯å¤©ç„¶å¯ä»¥å¹¶è¡ŒåŒ–çš„ã€‚å› æ­¤ä½¿ç”¨GPUè¿›è¡Œè¿ç®—ä¼šæ¯”CPUè¿ç®—å¿«å¾—å¤šã€‚è€Œå¸¸ç”¨çš„æ¡†æ¶Tensorflowå°±é€šè¿‡CUDAæä¾›äº†GPUè¿ç®—çš„æ”¯æŒã€‚<br><a id="more"></a><br>æ ¹æ®<a href="https://www.tensorflow.org/install/gpu#windows_setup" target="_blank" rel="external">å®˜æ–¹é¡µé¢</a>ï¼Œå¯¹è½¯ç¡¬ä»¶æœ‰å¦‚ä¸‹è¦æ±‚:</p><blockquote><p>The following GPU-enabled devices are supported:</p><ul><li>NVIDIAÂ® GPU card with CUDAÂ® architectures 3.5 or higher. See the list of CUDAÂ®-enabled GPU cards.</li><li>For GPUs with unsupported CUDAÂ® architectures, or to avoid JIT compilation from PTX, or to use different versions of the NVIDIAÂ® libraries, see the Linux build from source guide.<br>-On systems with NVIDIAÂ® Ampere GPUs (CUDA architecture 8.0) or newer, kernels are JIT-compiled from PTX and TensorFlow can take over 30 minutes to start up. This overhead can be limited to the first start up by increasing the default JIT cache size with: â€˜export CUDA_CACHE_MAXSIZE=2147483648â€™ (see JIT Caching for details).<br>-Packages do not contain PTX code except for the latest supported CUDAÂ® architecture; therefore, TensorFlow fails to load on older GPUs when CUDA_FORCE_PTX_JIT=1 is set. (See Application Compatibility for details.)</li></ul><p>The following NVIDIAÂ® software must be installed on your system:</p><ul><li>NVIDIAÂ® GPU drivers â€”CUDAÂ® 10.1 requires 418.x or higher.</li><li>CUDAÂ® Toolkit â€”TensorFlow supports CUDAÂ® 10.1 (TensorFlow &gt;= 2.1.0)</li><li>CUPTI ships with the CUDAÂ® Toolkit.</li><li>cuDNN SDK 7.6 </li></ul></blockquote><p>æ¢å¥è¯è¯´ï¼Œåªè¦ä¸æ˜¯ä¸Šå¤æ—¶ä»£çš„NVIDIA GPUï¼Œéƒ½å¯ä»¥è¿›è¡Œè¿ç®—ã€‚</p><h2 id="è¸©å‘"><a href="#è¸©å‘" class="headerlink" title="è¸©å‘"></a>è¸©å‘</h2><h3 id="Tensorflow"><a href="#Tensorflow" class="headerlink" title="Tensorflow"></a>Tensorflow</h3><p>å®‰è£…tensorflowåè¦è¿˜è¦å®‰è£…tensorflow-gpuã€‚tensorflow-gpu<strong>ä¸</strong>æ˜¯tensorflowçš„æ›¿ä»£è€…ï¼Œè€Œæ˜¯æ”¯æŒè¿ç®—GPUçš„æ¨¡å—ã€‚ä¸è¦è¢«ç½‘ä¸Šçš„ä¿¡æ¯è¯¯å¯¼ï¼Œä¸¤è€…éƒ½éœ€è¦å®‰è£…<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">pip install tensorflow</div><div class="line">pip install tensorflow-gpu</div></pre></td></tr></table></figure></p><h3 id="CUDA"><a href="#CUDA" class="headerlink" title="CUDA"></a>CUDA</h3><p>CUDAè¯·ä¸€å®šå®‰è£…10.1ç‰ˆæœ¬ï¼Œæ›´æ–°çš„å’Œæ›´æ—§çš„ç‰ˆæœ¬éƒ½ä¸æ”¯æŒã€‚è¦å»Archiveé‡Œæ‰¾ã€‚</p><h3 id="cuDNN"><a href="#cuDNN" class="headerlink" title="cuDNN"></a>cuDNN</h3><p>cuDNNè¯·ä¸€å®šå®‰è£…7.6ç‰ˆæœ¬ï¼Œæ›´æ–°çš„å’Œæ›´æ—§çš„ç‰ˆæœ¬éƒ½ä¸æ”¯æŒã€‚<br>cuDNNè¦å…ˆå»æ³¨å†ŒNVIDIA developerå†å»Archiveé‡Œæ‰¾7.6çš„</p><h3 id="Coding"><a href="#Coding" class="headerlink" title="Coding"></a>Coding</h3><p>å…¨éƒ¨å®‰è£…å¥½åå»è·‘hello worldã€‚tensorflowå¯èƒ½ä¼šå¡åœ¨<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0</div></pre></td></tr></table></figure></p><p>ç°åœ¨å»æ³¡æ¯å’–å•¡ï¼Œåå’Œæ”¾å®½ï¼Œå¤§æ¦‚è¿‡å‡ ä¸ªå°æ—¶å°±å¥½äº†ã€‚è¿™ä¹ˆå¤§çš„å»¶è¿Ÿåªä¼šç¬¬ä¸€æ¬¡å‡ºç°ã€‚åŸå› ä¼¼ä¹æ˜¯å› ä¸ºGPUé‚£è¾¹åœ¨åšJITğŸ™ƒ</p><p>ç­‰å‡ºç°è¿™ä¸€è¡Œ<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1376 MB memory) -&gt; physical GPU (device: 0, name: GeForce 840M, pci bus id: 0000:01:00.0, compute capability: 5.0)</div></pre></td></tr></table></figure></p><p>å°±è¯´æ˜æˆåŠŸäº†ã€‚</p><p>æˆ‘çš„æ¸£æ¸£840Mè·‘è®­ç»ƒæ¯”CPUå¿«äº†ä¸€ä¸ªæ•°é‡çº§;-)</p><h2 id="åæ§½"><a href="#åæ§½" class="headerlink" title="åæ§½"></a>åæ§½</h2><p>å·²ç»2020å¹´äº†ï¼ŒCUDAå·²ç»å‡ºåˆ°<code>11.0</code>äº†çš„RCäº†ï¼Œtensorflowå±…ç„¶è¿˜åªæ”¯æŒ2019å¹´2æœˆå‘å¸ƒçš„<code>10.1</code>ã€‚<br>cuDNNåŒç†ï¼Œä¹Ÿç”¨çš„æ˜¯å¾ˆè€çš„ç‰ˆæœ¬ã€‚</p><p>CUDAä½œä¸ºNVIDIAå®¶ç§æœ‰çš„ä¸€å¥—APIï¼Œå½¢æˆäº†äº‹å®æ ‡å‡†ï¼Œè¿™å¾ˆä¸å¥½ã€‚è€ŒAMDå®¶çš„æå¾—å«åš<code>ROCm</code>çš„ä¸€å¥—ä¸œè¥¿ï¼Œå¾ˆé—æ†¾çš„è¿˜æ²¡æˆä»€ä¹ˆæ°”å€™ã€‚<code>ROCm</code>çš„tensorflowæ˜¯å®˜æ–¹ç‰ˆçš„ä¸€ä»½<a href="https://github.com/ROCmSoftwarePlatform/tensorflow-upstream" target="_blank" rel="external">fork</a>ï¼Œbinaryè¿˜æ˜¯ç¤¾åŒºè‡ªå·±ç¼–è¯‘çš„ï¼Œå¯ä»¥æƒ³è±¡å‘æ˜¯æ— æ¯”çš„å¤šã€‚</p><p>å¸Œæœ›å¼€æºçš„æ ‡å‡†å°½å¿«å–ä»£æ‰ç§æœ‰çš„CUDAï¼Œè®©Aå®¶çš„GPUä¹Ÿèƒ½æ— ç—›çš„è·‘ç§‘å­¦è®¡ç®—ã€‚</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;æ¢¯åº¦ä¸‹é™æ³•å¤§éƒ¨åˆ†æ—¶é—´éƒ½åœ¨è¿›è¡Œå‘é‡å’ŒçŸ©é˜µè¿ç®—ã€‚è¿™äº›è¿ç®—æ˜¯å¤©ç„¶å¯ä»¥å¹¶è¡ŒåŒ–çš„ã€‚å› æ­¤ä½¿ç”¨GPUè¿›è¡Œè¿ç®—ä¼šæ¯”CPUè¿ç®—å¿«å¾—å¤šã€‚è€Œå¸¸ç”¨çš„æ¡†æ¶Tensorflowå°±é€šè¿‡CUDAæä¾›äº†GPUè¿ç®—çš„æ”¯æŒã€‚&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Machine learning" scheme="https://verrickt.github.io/tags/Machine-learning/"/>
    
      <category term="CUDA" scheme="https://verrickt.github.io/tags/CUDA/"/>
    
      <category term="Tensorflow" scheme="https://verrickt.github.io/tags/Tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>å¾ªç¯ç¥ç»ç½‘ç»œ</title>
    <link href="https://verrickt.github.io/2020/07/23/recurrent-neural-network/"/>
    <id>https://verrickt.github.io/2020/07/23/recurrent-neural-network/</id>
    <published>2020-07-23T10:21:45.000Z</published>
    <updated>2021-01-11T12:07:36.814Z</updated>
    
    <content type="html"><![CDATA[<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML" async></script><p>Recurrent Neural network(RNN)æ˜¯ä¸€ç§ä¸“é—¨å¤„ç†åºåˆ—çš„ç¥ç»ç½‘ç»œã€‚æ­£å¦‚CNNå¯ä»¥è½»æ˜“åœ°å¤„ç†å¤§è§„æ¨¡çš„â€œç½‘æ ¼çŠ¶â€æ•°æ®ï¼ŒRNNèƒ½å¤Ÿå¤„ç†å…¶ä»–ç½‘ç»œæ¶æ„éƒ½æ— æ³•å¤„ç†çš„é•¿çš„åºåˆ—ã€‚</p><a id="more"></a><h2 id="Why-RNN"><a href="#Why-RNN" class="headerlink" title="Why RNN"></a>Why RNN</h2><p>è€ƒè™‘è¿™æ ·çš„é—®é¢˜ï¼šèˆªç©ºå…¬å¸è¦ä»æ–‡å­—ä¸­æå–é¡¾å®¢çš„ç›®çš„åœ°å’Œå‡ºå‘åœ°ã€‚</p><p>å¯¹äº</p><blockquote><p>Iâ€™ll arrive from Shanghai to ShenZhen</p></blockquote><p>ç›®çš„åœ°æ˜¯ShenZhenï¼Œå‡ºå‘åœ°æ˜¯Shanghai</p><p>è€Œå¯¹äº</p><blockquote><p>Iâ€™ll arrive to Shanghai from ShenZhen</p></blockquote><p>åˆ™ç›®çš„åœ°æ˜¯Shanghaiï¼Œå‡ºå‘åœ°æ˜¯ShenZhenã€‚</p><p>ä½¿ç”¨åŸºäºåœ°ç‚¹åœ¨å¥ä¸­ä½ç½®çš„æ–¹æ³•æ— æ³•è§£å†³é—®é¢˜ã€‚åˆ°åº•æ˜¯å‡ºå‘åœ°è¿˜æ˜¯ç›®çš„åœ°ï¼Œä¸å‰åçš„è¯è¯­éƒ½æœ‰å…³ã€‚è¿™äº›ä¿¡æ¯è¿™ç§°ä¸ºContext(ä¸Šä¸‹æ–‡)</p><p>è‹¥ä½¿ç”¨MLPï¼Œåˆ™éœ€è¦åœ¨æ¯ä¸ªä½ç½®éƒ½é‡å¤çš„å­¦ä¼šäººç±»è¯­è¨€çš„è§„åˆ™ï¼Œå¤§é‡é‡å¤çš„å‚æ•°ä¸ä½†ä½¿å¾—è®¡ç®—ä»£ä»·æ¿€å¢ï¼Œè€Œä¸”ä¹Ÿæ˜¾è‘—å¢åŠ over-fittingçš„å‡ ç‡ã€‚</p><p>å› æ­¤ï¼Œéœ€è¦ä¸€ç§ä¸“é—¨çš„ç»“æ„æ¥å¤„ç†åºåˆ—æ•°æ®ã€‚RNNåº”è¿è€Œç”Ÿã€‚</p><h2 id="Vanilla-RNN"><a href="#Vanilla-RNN" class="headerlink" title="Vanilla-RNN"></a>Vanilla-RNN</h2><p>RNNä¼å›¾ä½¿ç”¨Memory(è®°å¿†)æ¥è§£å†³é—®é¢˜ï¼š<br>å½“å‰æ•°æ®å¤„ç†è¿‡ç¨‹ä¸­éšè—å•å…ƒçš„å€¼è¢«ä¿ç•™ï¼Œä¸‹ä¸ªæ•°æ®å¤„ç†æ—¶ä½œä¸ºé¢å¤–çš„è¾“å…¥ï¼Œè¿™æ ·RNNå°±æ‹¥æœ‰äº†â€œè®°ä½â€å·²ç»çœ‹è¿‡æ•°æ®çš„èƒ½åŠ›ã€‚</p><p><img src="/2020/07/23/recurrent-neural-network/lemma-RNN.png" alt="Network with memory"></p><p>æ ¹æ®â€œè®°å¿†â€çš„æ¥è‡ªéšè—å•å…ƒè¿˜æ˜¯è¾“å‡ºçš„ä¸åŒï¼Œå¯ä»¥å°†RNNåˆ†ä¸ºElman networkå’ŒJordan networkä¸¤ç§<br><img src="/2020/07/23/recurrent-neural-network/vanilla-RNN.png" alt="Vanilla RNN"></p><p>ç°åœ¨çš„RNNåªèƒ½å¾€ä¸€ä¸ªæ–¹å‘çœ‹ï¼Œæœ‰æ—¶åªâ€œå¾€å‰çœ‹â€çš„ç½‘ç»œå¹¶ä¸èƒ½è§£å†³é—®é¢˜ï¼Œæˆ‘ä»¬è¿˜éœ€è¦â€œå¾€åçœ‹â€ã€‚æ—¢å¾€å‰çœ‹åˆå¾€åçœ‹çš„RNNå«åšbidirectional RNN(åŒå‘RNN)</p><p>è¿™æ—¶åªè¦è®­ç»ƒä¸¤ä¸ªRNNï¼šä¸€ä¸ªä»å‰å¾€åï¼Œä¸€ä¸ªä»åå¾€å‰ï¼Œå†æŠŠä»–ä»¬çš„è¾“å‡ºä¸¢åˆ°å¦ä¸€ä¸ªMLPå»åšå¤„ç†å°±å¯ä»¥äº†<br><img src="/2020/07/23/recurrent-neural-network/bidirectional-RNN.png" alt="bidirectional RNN"></p><h2 id="Exploding-Vanishing-gradients"><a href="#Exploding-Vanishing-gradients" class="headerlink" title="Exploding/Vanishing gradients"></a>Exploding/Vanishing gradients</h2><p>RNNçš„æ€§èƒ½ååˆ†å‡ºè‰²ï¼Œä½†è®­ç»ƒçš„è¿‡ç¨‹å¾€å¾€å›°éš¾é‡é‡ã€‚å…¶ä¸­ä¸€ä¸ªé‡è¦çš„å› ç´ å°±æ˜¯Exploding/Vanishing gradients(æ¢¯åº¦çˆ†ç‚¸/æ¶ˆå¤±)ã€‚</p><p><img src="/2020/07/23/recurrent-neural-network/exploding-vanishing-gradient.png" alt="exploding-vanishing-gradient"><br>åœ¨RNNä¸­ï¼Œt-1æ—¶åˆ»çš„éšè—å•å…ƒçš„è¾“å‡ºä¼šå½±å“åˆ°tæ—¶åˆ»çš„éšè—å•å…ƒã€‚</p><script type="math/tex; mode=display">h^{t}=Wh^{t-1}</script><p>é‡å¤ä½¿ç”¨è¿™ä¸ªå¼å­ï¼Œåˆ™</p><script type="math/tex; mode=display">h^{t}=W^t\prod_{i=1}^{i=t}h_{i}</script><p>è‹¥Wå¯è¿›è¡Œç‰¹å¾å€¼åˆ†è§£ï¼Œåˆ™</p><script type="math/tex; mode=display">W=Q^{\top}A Q\\W^t=Q^{\top}A^{t}Q</script><ul><li>è‹¥Wçš„ç‰¹å¾å€¼Î»&gt;1ï¼Œåˆ™ç»è¿‡tæ¬¡ç›¸ä¹˜åï¼ŒÎ»^tçš„å€¼ä¼šå˜å¾—éå¸¸å¤§(æ¢¯åº¦çˆ†ç‚¸)ã€‚</li><li>è‹¥Î»&lt;1ï¼Œåˆ™ç»è¿‡tæ¬¡ç›¸ä¹˜åï¼ŒÎ»^tçš„å€¼ä¼šå˜å¾—éå¸¸æ¥è¿‘é›¶(æ¢¯åº¦æ¶ˆå¤±)</li></ul><p>æ¢¯åº¦çˆ†ç‚¸/æ¶ˆå¤±å¯¹æ¢¯åº¦ä¸‹é™æ³•çš„å¹²æ‰°éå¸¸å¤§ï¼Œä»¥è‡³äºè®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°åŒªå¤·æ‰€æ€çš„ç»“æœ<br><img src="/2020/07/23/recurrent-neural-network/low-convergence.png" alt="åŒªå¤·æ‰€æ€"></p><p>è§£å†³è¿™ä¸ªé—®é¢˜çš„å…³é”®åœ¨äºç»™â€œè®°å¿†â€å¯å˜çš„æƒé‡ã€‚</p><h2 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h2><p>Long short-term memory(é•¿<strong>çŸ­æœŸè®°å¿†</strong>)æ˜¯ä¸€ä¸ªè§£å†³æ¢¯åº¦çˆ†ç‚¸/æ¶ˆå¤±é—®é¢˜æ•ˆæœè¾ƒå¥½çš„æ–¹æ¡ˆã€‚<br><img src="/2020/07/23/recurrent-neural-network/LSTM.png" alt="LSTM"><br>åœ¨LSTMä¸­ï¼Œåœ¨å­˜å‚¨â€œè®°å¿†â€çš„åœ°æ–¹(cell)åŠ ä¸Šäº†ä¸‰ä¸ªgateï¼š</p><ul><li>input gate</li><li>output gate</li><li>forget gate</li></ul><p>è¿™äº›gateçš„æ§åˆ¶ä¿¡å·ç»è¿‡sigmoidå‡½æ•°åå¾—åˆ°çš„0~1ä¹‹é—´çš„æ•°å€¼è¡¨ç¤ºè¿™äº›é—¨â€œæ‰“å¼€â€çš„ç¨‹åº¦</p><ul><li>input gateå†³å®šâ€œè®°å¿†â€æ˜¯å¦æ¥å—æ–°çš„è¾“å…¥</li><li>output gateå†³å®šâ€œè®°å¿†â€æ˜¯å¦è¢«è¯»å‡º</li><li>forget gateå†³å®šâ€œè®°å¿†æ˜¯å¦è¢«é—å¿˜â€</li></ul><p><img src="/2020/07/23/recurrent-neural-network/LSTM-cell.png" alt="LSTM cell"></p><p>å›¾ä¸­<code>z</code>æ˜¯è¾“å…¥,<code>zi,zo,zf</code>åˆ†åˆ«æ˜¯ä¸Šè¿°ä¸‰ä¸ªgateçš„æ§åˆ¶ä¿¡å·ã€‚<br>è®¾åŸæ¥å­˜å‚¨åœ¨â€œè®°å¿†â€ä¸­çš„å€¼ä¸ºc,åˆ™æ–°çš„å€¼câ€™å¦‚æ­¤äº§ç”Ÿ</p><ol><li>è¾“å…¥zç»è¿‡æ¿€æ´»å‡½æ•°çš„å€¼g(z)ä¸input gateçš„å€¼ç›¸ä¹˜ï¼Œå¾—åˆ°g(z)*f(zi)</li><li>åŸæ¥â€œè®°å¿†â€ä¸­çš„å€¼cä¸forget gateçš„å€¼ç›¸ä¹˜å¾—åˆ°c*f(zf)</li><li>å°†è¾“å…¥ä¸åŸâ€œè®°å¿†â€ç›¸åŠ çš„ç»“æœä½œä¸ºæ–°çš„è®°å¿†câ€™=g(z)<em>f(zi)+c</em>f(zf)</li><li>æ–°çš„å€¼câ€™ä¸output gateçš„å€¼ç›¸ä¹˜å¾—åˆ°câ€™*f(zo)ï¼Œç»“æœä½œä¸ºâ€œè®°å¿†â€çš„è¾“å‡º</li></ol><p>å…¶ä¸­zi,zo,zfæ˜¯ç½‘ç»œçš„å‚æ•°ï¼Œç”±è®­ç»ƒè¿‡ç¨‹ä¸­è‡ªå·±å­¦å¾—ã€‚å°†å¤šä¸ªLSTMçš„cellè¿æ¥èµ·æ¥ï¼Œå°±å¾—åˆ°äº†èƒ½å¤Ÿå¤„ç†å¤æ‚é—®é¢˜çš„RNNã€‚å½“ç„¶åœ¨å®é™…ä½¿ç”¨ä¸­ï¼Œzi,zo,å’Œzfä¹Ÿå¯ä»¥æ¥å—ç½‘ç»œå…¶ä»–éƒ¨åˆ†çš„å‚æ•°<br>ï¼Œè¿™å°±è¦çœ‹å…·ä½“çš„é—®é¢˜äº†ã€‚</p><p><img src="/2020/07/23/recurrent-neural-network/Deep-LSTM.png" alt="Deep-LSTM.png"></p>]]></content>
    
    <summary type="html">
    
      &lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML&quot; async&gt;&lt;/script&gt;

&lt;p&gt;Recurrent Neural network(RNN)æ˜¯ä¸€ç§ä¸“é—¨å¤„ç†åºåˆ—çš„ç¥ç»ç½‘ç»œã€‚æ­£å¦‚CNNå¯ä»¥è½»æ˜“åœ°å¤„ç†å¤§è§„æ¨¡çš„â€œç½‘æ ¼çŠ¶â€æ•°æ®ï¼ŒRNNèƒ½å¤Ÿå¤„ç†å…¶ä»–ç½‘ç»œæ¶æ„éƒ½æ— æ³•å¤„ç†çš„é•¿çš„åºåˆ—ã€‚&lt;/p&gt;
    
    </summary>
    
      <category term="Machine learning" scheme="https://verrickt.github.io/categories/Machine-learning/"/>
    
    
      <category term="Machine learning" scheme="https://verrickt.github.io/tags/Machine-learning/"/>
    
      <category term="RNN" scheme="https://verrickt.github.io/tags/RNN/"/>
    
  </entry>
  
  <entry>
    <title>å·ç§¯ç¥ç»ç½‘ç»œ</title>
    <link href="https://verrickt.github.io/2020/07/18/convolutional-neural-network/"/>
    <id>https://verrickt.github.io/2020/07/18/convolutional-neural-network/</id>
    <published>2020-07-18T11:31:16.000Z</published>
    <updated>2021-01-11T12:07:36.713Z</updated>
    
    <content type="html"><![CDATA[<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML" async></script><p>ä»»ä½•ä¸€ä¸ªæœºå™¨å­¦ä¹ çš„ä»»åŠ¡éƒ½å¯ä»¥è¢«æ‹†è§£ä¸ºä¸‰æ­¥</p><ol><li>æ‰¾åˆ°ä¸€ç»„å‡½æ•°(model)</li><li>æ‰¾åˆ°è¯„ä»·å‡½æ•°å¥½åçš„æ ‡å‡†(loss)</li><li>æ‰¾å‡ºæœ€å¥½çš„å‡½æ•°(optimization)</li></ol><p>è¿™ä¸‰æ­¥åœ¨ç¥ç»ç½‘ç»œä¸­åŒæ ·æˆç«‹ã€‚ä½†åœ¨ç¥ç»ç½‘ç»œä¸­ï¼Œè¦æ‰¾çš„ä¸æ˜¯ä¸€ç»„å‡½æ•°ï¼Œè€Œæ˜¯ä¸€ç§ç½‘ç»œæ¶æ„(architecture)ã€‚æœ¬æ–‡ä¸­çš„å·ç§¯ç¥ç»ç½‘ç»œ(CNN)å°±æ˜¯ä¸€ç§ç½‘ç»œæ¶æ„<br><a id="more"></a><br>æ ¹æ®universal approximation theoremï¼ŒMLPå·²ç»è¶³å¤Ÿè¡¨è¾¾ä»»æ„å‡½æ•°ã€‚è€Œåœ¨å®é™…ä¸­ï¼Œå› ä¸ºç›¸é‚»å±‚çº§çš„ç¥ç»å…ƒä¹‹é—´çš„ä»»æ„äº’è”å¯¼è‡´MLPå®¹æ˜“è¿‡æ‹Ÿåˆã€‚è€Œåœ¨å¤„ç†ä¸€äº›å…·æœ‰ç»“æ„åŒ–çš„è¾“å…¥æ—¶ï¼ŒMLPä¸­åˆä¼šæœ‰å¤§é‡çš„å‚æ•°é‡å¤ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œå¼•å…¥äº†CNNã€‚</p><h2 id="Why-CNN"><a href="#Why-CNN" class="headerlink" title="Why CNN?"></a>Why CNN?</h2><p>è€ƒè™‘è¿™æ ·ä¸€ä¸ªé—®é¢˜ï¼šå¦‚ä½•è®©æœºå™¨è®¤å‡ºå›¾ç‰‡é‡Œçš„ä¸€å¼ é¸Ÿï¼Ÿ</p><p>å›é¡¾äººç±»çš„æ€è€ƒè¿‡ç¨‹ï¼Œè‹¥å›¾ç‰‡ä¸­çš„ç‰©ä½“å…·æœ‰é¸Ÿçš„åŸºæœ¬ç‰¹å¾ï¼Œå¦‚å–™ã€ç¿…è†€å’Œçˆªï¼Œåˆ™è®¤ä¸ºå®ƒæ˜¯é¸Ÿã€‚åœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å¹¶ä¸å…³å¿ƒç¿…è†€åœ¨å›¾ç‰‡ä¸­çš„ä½ç½®ï¼šåªè¦æœ‰å–™å°±å¯ä»¥äº†ã€‚å¯¹ç¿…è†€å’Œçˆªä¹Ÿæ˜¯åŒæ ·æˆç«‹ã€‚</p><p>ç»§ç»­æ€è€ƒï¼Œæ€ä¹ˆè®¤å‡ºå–™ï¼Ÿå–™å¯ä»¥ç”±å®ƒçš„è¾¹ç•Œ(geometry)æ¥å®šä¹‰ï¼šä¸¤æ¡æ–œçº¿å›´æˆçš„å°–å°–çš„å¤šè¥¿ã€‚ç¿…è†€å’Œçˆªä¹Ÿå¯ä»¥æœ‰å¯¹åº”çš„è¾¹ç•Œæ¥å®šä¹‰ã€‚åŒæ ·çš„ï¼Œæˆ‘ä»¬å¹¶ä¸å…³å¿ƒè¾¹ç•Œå‡ºç°çš„ä½ç½®ï¼Œåªè¦å®ƒèƒ½ç¡®å®šè¿™ä¸ªç‰©ä½“æ˜¯ä¸ªå–™å°±å¯ä»¥äº†ã€‚</p><p>å®é™…ä¸Šè¿™å°±æ˜¯CNNå‡ºç°çš„åŠ¨æœºä¹‹ä¸€ï¼šæˆ‘ä»¬åªå…³å¿ƒä¸€äº›æ¨¡å¼æ˜¯å¦å‡ºç°ï¼Œè€Œä¸å…³å¿ƒå®ƒä»¬å‡ºç°çš„ä½ç½®ã€‚æ¢å¥è¯è¯´ï¼Œæˆ‘ä»¬æ‰€æœŸå¾…çš„æ¨¡å¼ä¼šåœ¨å›¾ç‰‡çš„ä¸åŒåŒºåŸŸå‡ºç°ã€‚</p><p>å›åˆ°å›¾åƒçš„ä¾‹å­ä¸Šï¼Œäººä»¬å‘ç°äº†ä¸‰ä¸ªå±æ€§ï¼š</p><ol><li>æ¯”èµ·æ•´å¼ å›¾ç‰‡ï¼Œæ¨¡å¼é€šå¸¸æ˜¯æ¯”è¾ƒå°çš„</li><li>ç›¸åŒçš„æ¨¡å¼ä¼šåœ¨ä¸åŒçš„åŒºåŸŸå‡ºç°</li><li>å¯¹å›¾ç‰‡åšsubsampling(å¦‚åˆ æ‰å¥‡æ•°è¡Œå’Œå¶æ•°åˆ—)å¹¶ä¸ä¼šæ”¹å˜å›¾ä¸­çš„ç‰©ä½“</li></ol><p><img src="/2020/07/18/convolutional-neural-network/properties.png" alt="properties"></p><p>åŸºäºè¿™ä¸‰ç§å±æ€§ï¼Œäººä»¬æå‡ºäº†convolutionå’Œpoolingä¸¤ç§æ“ä½œã€‚convolutionå¯¹åº”å±æ€§1ã€2ï¼Œpoolingå¯¹åº”å±æ€§3ã€‚convolutionå’Œpoolingäº¤æ›¿å°±æ˜¯CNNçš„æ¶æ„ã€‚</p><h2 id="Convolution"><a href="#Convolution" class="headerlink" title="Convolution"></a>Convolution</h2><p>Convolutionçš„åŸºç¡€å•ä½æ˜¯filter(kernel)ã€‚filteræ˜¯ä¸€ä¸ªçŸ©é˜µï¼Œç”¨æ¥æ£€æµ‹æ¨¡å¼<br>è€ƒè™‘å¦‚ä¸‹çš„æƒ…æ™¯<br><img src="/2020/07/18/convolutional-neural-network/filter.png" alt="filter"></p><p>è¾“å…¥æ˜¯<code>6*6</code>çš„å›¾ç‰‡ï¼Œfilteræ˜¯<code>3*3</code>çš„çŸ©é˜µã€‚åœ¨å›¾ç‰‡çš„å·¦ä¸Šè§’æ‰¾åˆ°ä¸€ä¸ªä¸filterä¸€æ ·å¤§çš„çŸ©é˜µï¼Œåšelement-wise productï¼Œä¼šå¾—åˆ°ä¸€ä¸ªæ•°å€¼ã€‚å°†filterå¾€å³ç§»åŠ¨ä¸€æ ¼ï¼Œç»§ç»­è¿™ä¸ªæ­¥éª¤ï¼Œå°±åˆå¾—åˆ°æ–°çš„æ•°å€¼ã€‚ç§»åŠ¨çš„è·ç¦»ç§°ä¸ºstride(æ­¥é•¿)</p><p>filteré€šå¸¸æ¯”è¾“å…¥æ›´å°ï¼Œè€Œfilteråœ¨ç§»åŠ¨çš„è¿‡ç¨‹ä¸­å¯ä»¥åœ¨ä¸åŒçš„ä½ç½®ä¸Šæ£€æµ‹å‡ºå¯¹åº”çš„æ¨¡å¼ã€‚è¿™å°±ä½“ç°äº†å±æ€§1å’Œ2ã€‚</p><p><img src="/2020/07/18/convolutional-neural-network/stride.png" alt="stride"></p><p>filterç§»åŠ¨å®Œæ¯•åä¼šå¾—åˆ°ä¸€ä¸ªæ–°çš„çŸ©é˜µã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨å¤šä¸ªfilter,å°†è¿™äº›filterçš„è¾“å‡ºæ”¾åœ¨ä¸€èµ·ä¼šå¾—åˆ°ä¸€ä¸ªä¸‰ç»´çš„å¼ é‡ï¼Œç§°ä¸ºfeature map.<br><img src="/2020/07/18/convolutional-neural-network/feature-map.png" alt="feature map"></p><p><img src="/2020/07/18/convolutional-neural-network/multi-channel.png" alt="multi-channel"><br>è¾“å…¥ä¹Ÿä¸ä¸€å®šæ˜¯é»‘ç™½çš„ã€‚è‹¥è¾“å…¥å½©è‰²å›¾ç‰‡ï¼Œåˆ™éœ€è¦ä¸€ä¸ªä¸‰ç»´çš„å¼ é‡æ¥è¡¨ç¤º</p><script type="math/tex; mode=display">\mathrm{V}_{c,i,j}</script><p>è¿™æ—¶çš„filterä¹Ÿæ˜¯ä¸€ä¸ªä¸‰ç»´å¼ é‡ã€‚<br>è¿™é‡Œçš„é‡ç‚¹æ˜¯ï¼Œä¸ç®¡è¾“å…¥æ˜¯ä»€ä¹ˆæ ·ï¼Œfilterçš„â€œzè½´â€è¦ä¸è¾“å…¥çš„â€œzè½´â€ä¸€æ ·é«˜ï¼Œç„¶ååœ¨å…¶ä»–è½´ä¸Šä»¥strideç§»åŠ¨ï¼Œå¾—åˆ°çš„æ ‡é‡é›†åˆèµ·æ¥æˆä¸ºæ–°çš„å¼ é‡ä½œä¸ºä¸‹ä¸€å±‚çš„è¾“å…¥ã€‚<br>Convolutionå¾—åˆ°çš„feature mapä¼šç¼©å°ï¼Œè¿™å¯ä»¥è¡¥é›¶(zero padding)æ¥é¿å…</p><p>å®é™…ä¸­é€šè¿‡åœ¨MLPä¸­å…±äº«å‚æ•°æ¥å®ç°ã€‚ä½¿ç”¨äº†æ›´å°‘çš„å‚æ•°æ¥åšåŒæ ·çš„äº‹æƒ…å°±ä¸å®¹æ˜“è¿‡æ‹Ÿåˆã€‚<br><img src="/2020/07/18/convolutional-neural-network/shared-weight.png" alt="shared-weight"></p><h2 id="Pooling"><a href="#Pooling" class="headerlink" title="Pooling"></a>Pooling</h2><p>Poolingçš„æ€è·¯æ›´ç®€å•äº†ã€‚å¯¹ä¸€äº›åŒºåŸŸè¿›è¡Œé‡‡æ ·ï¼Œå¾—åˆ°ä¸€å¼ æ›´å°çš„image,è¿™å°±æ˜¯å±æ€§3ã€‚<br><img src="/2020/07/18/convolutional-neural-network/before-pooling.png" alt="before-pooling"></p><p>poolingæœ‰å¾ˆå¤šé€‰æ‹©ï¼Œå¯ä»¥å–ç®—æœ¯å¹³å‡ï¼Œå¯ä»¥å–L2 normã€‚ä¸€èˆ¬æ¯”è¾ƒå¸¸è§çš„æ˜¯max poolingã€‚<br><img src="/2020/07/18/convolutional-neural-network/after-pooling.png" alt="after-pooling"></p><p>é€šè¿‡poolingï¼Œç½‘ç»œå°±å¯ä»¥å¯¹è¾“å…¥ä¸­æ¯”è¾ƒè§„å¾‹çš„å˜åŒ–ä¸é‚£ä¹ˆæ•æ„Ÿã€‚è¿™ç§°ä¸ºinvarianceã€‚<br><img src="/2020/07/18/convolutional-neural-network/invariance.png" alt="invariance"></p><h2 id="Flatten"><a href="#Flatten" class="headerlink" title="Flatten"></a>Flatten</h2><p>åšå®Œpoolingåå¾—åˆ°çš„æ˜¯ä¸€ä¸ªå¼ é‡ã€‚æŠŠå®ƒâ€œæ‹‰ç›´â€æˆä¸€ä¸ªå‘é‡ï¼Œä¸¢ç»™MLPå°±å¯ä»¥å®ç°æˆ‘ä»¬æ‰€è¦åšçš„äº‹æƒ…äº†ã€‚<br><img src="/2020/07/18/convolutional-neural-network/whole-cnn.png" alt="whole"></p><p>CNNå¹¶ä¸åªèƒ½ç”¨äºå›¾åƒå¤„ç†ï¼Œåªè¦è¦å¤„ç†çš„é—®é¢˜å…·æœ‰ä¸Šè¿°çš„ä¸‰ä¸ªå±æ€§ï¼Œå°±å¯ä»¥ç”¨CNNæ¥è§£å†³ã€‚<br>ä¸‹é¢æ˜¯ä¸€äº›ç”¨CNNæ•ˆæœæ¯”è¾ƒå¥½çš„ä»»åŠ¡:<br><img src="/2020/07/18/convolutional-neural-network/more-cnn.png" alt="more cnn"></p>]]></content>
    
    <summary type="html">
    
      &lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML&quot; async&gt;&lt;/script&gt;

&lt;p&gt;ä»»ä½•ä¸€ä¸ªæœºå™¨å­¦ä¹ çš„ä»»åŠ¡éƒ½å¯ä»¥è¢«æ‹†è§£ä¸ºä¸‰æ­¥&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;æ‰¾åˆ°ä¸€ç»„å‡½æ•°(model)&lt;/li&gt;
&lt;li&gt;æ‰¾åˆ°è¯„ä»·å‡½æ•°å¥½åçš„æ ‡å‡†(loss)&lt;/li&gt;
&lt;li&gt;æ‰¾å‡ºæœ€å¥½çš„å‡½æ•°(optimization)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;è¿™ä¸‰æ­¥åœ¨ç¥ç»ç½‘ç»œä¸­åŒæ ·æˆç«‹ã€‚ä½†åœ¨ç¥ç»ç½‘ç»œä¸­ï¼Œè¦æ‰¾çš„ä¸æ˜¯ä¸€ç»„å‡½æ•°ï¼Œè€Œæ˜¯ä¸€ç§ç½‘ç»œæ¶æ„(architecture)ã€‚æœ¬æ–‡ä¸­çš„å·ç§¯ç¥ç»ç½‘ç»œ(CNN)å°±æ˜¯ä¸€ç§ç½‘ç»œæ¶æ„&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Machine learning" scheme="https://verrickt.github.io/categories/Machine-learning/"/>
    
    
      <category term="Machine learning" scheme="https://verrickt.github.io/tags/Machine-learning/"/>
    
      <category term="CNN" scheme="https://verrickt.github.io/tags/CNN/"/>
    
  </entry>
  
  <entry>
    <title>ç¥ç»ç½‘ç»œä¸­çš„Regularization</title>
    <link href="https://verrickt.github.io/2020/07/15/regularization-of-neural-network/"/>
    <id>https://verrickt.github.io/2020/07/15/regularization-of-neural-network/</id>
    <published>2020-07-15T02:22:44.000Z</published>
    <updated>2021-01-11T12:07:36.819Z</updated>
    
    <content type="html"><![CDATA[<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML" async></script><p>Deep learning algorithms are typically applied to extremely complicated domains where the true generation process essentially involves simulating the entire universeâ€¦ Controlling the complexity of the model is not a simple matter of finding the model of the right size, with the right number of parameters. Instead, we might findâ€”and indeed in practical deep learning scenarios, we almost always do findâ€”that the best fitting model (in the sense of minimizing generalization error) is a large model that has been regularized appropriately</p><a id="more"></a><p>regularizationæ˜¯æˆ‘ä»¬åº”å¯¹overfittingçš„å¸¸ç”¨æ‰‹æ®µã€‚åœ¨æ¯”è¾ƒç®€å•çš„æ¨¡å‹ï¼Œä¾‹å¦‚linear-regresseionä¸­ï¼Œäººå¯¹çš„capacityçš„ä¼°è®¡é€šå¸¸æ¯”è¾ƒå‡†ç¡®ï¼Œä¸€èˆ¬ä¸éœ€è¦åšregularizationã€‚è€Œåœ¨ç¥ç»ç½‘ç»œåŠ¨è¾„æˆåƒä¸Šä¸‡ä¸ªèŠ‚ç‚¹ï¼Œæ•°ä»¥ç™¾ä¸‡çš„å‚æ•°è®©ä¼°æµ‹capacityå˜å¾—æä¸ºå›°éš¾ã€‚æ­£å¦‚èŠ±ä¹¦æ‰€è¯´ï¼Œæ³›åŒ–æ€§èƒ½æœ€å¥½çš„å¾€å¾€æ˜¯å¤æ‚çš„æ¨¡å‹åŠ ä¸Šåˆç†çš„regularizationè€Œå¾—åˆ°çš„</p><h2 id="Parameter-Penalities"><a href="#Parameter-Penalities" class="headerlink" title="Parameter Penalities"></a>Parameter Penalities</h2><p>æ­£å¦‚ä¸‰æ¬¡å¼æœ€é«˜æ¬¡çš„ç³»æ•°ä¸ºé›¶æ—¶é€€åŒ–ä¸ºäºŒæ¬¡å¼ï¼Œæœ‰è¶Šå¤šçš„å‚æ•°æ¥è¿‘0ï¼Œæ¨¡å‹çš„capacityå°±è¶Šä½ï¼Œå³ã€Œç»‘ä½æ‰‹è„šã€ã€‚ä¸€èˆ¬æ¥è¯´regularizationä¸ä¼šè€ƒè™‘<code>bias</code>ï¼Œå› ä¸º<code>bias</code>å¯¹capacityçš„å½±å“å¹¶ä¸å¦‚<code>weight</code>é‚£ä¹ˆå¤§ï¼Œä½†æœ‰éœ€è¦ä»å¯ä»¥ç»™<code>bias</code>åŠ ä¸Šæƒ©ç½šã€‚</p><p>å‚æ•°æƒ©ç½šçš„æ€è·¯æ˜¯ï¼Œç»™<code>Loss</code>åŠ ä¸Šæ‰€æœ‰å‚æ•°<code>Î¸</code>çš„å‡½æ•°<code>Î©(Î¸)</code>è¿™ä¸€é¡¹ï¼Œè®©æ¨¡å‹æ›´åå¥½äºÎ¸æ¥è¿‘0çš„å½’çº³å‡è®¾ã€‚</p><script type="math/tex; mode=display">\hat{L}(\theta,\alpha)=L(\theta)+\alpha \Omega(\theta)</script><h3 id="L2-norm"><a href="#L2-norm" class="headerlink" title="L2 norm"></a>L2 norm</h3><p>L2 normä¸­æ‰€é€‰çš„å‡½æ•°Î©æ˜¯</p><script type="math/tex; mode=display">\Omega(\theta)=\frac{1}{2}\theta^{\top}\theta</script><p><code>Loss</code>çš„æ¢¯åº¦å˜ä¸º</p><script type="math/tex; mode=display">\nabla_{\theta} \hat{L}=\nabla_{\theta}L+\alpha\theta</script><p>å¸¦å…¥æ¢¯åº¦ä¸‹é™å…¬å¼</p><script type="math/tex; mode=display">\theta^{N+1} \leftarrow\theta^N-\epsilon\nabla_\theta \hat{L}=\theta^N-\epsilon\nabla_\theta L - \alpha\epsilon \theta^N = (1-\alpha \epsilon)\theta ^N-\epsilon\nabla_\theta L</script><p>æ¯æ¬¡æ¢¯åº¦ä¸‹é™æ›´æ–°å‚æ•°æ—¶ä¼šæŠŠåŸæ¥çš„å‚æ•°<strong>ä¹˜</strong>ä¸Šä¸€ä¸ªå›ºå®šçš„å€¼<code>1-Î±Îµ</code>ï¼Œä½¿å¾—Î¸æ¥è¿‘äº0</p><h3 id="L1-norm"><a href="#L1-norm" class="headerlink" title="L1 norm"></a>L1 norm</h3><p>L1 normä¸­çš„å‡½æ•°Î©æ˜¯</p><script type="math/tex; mode=display">\Omega(\theta)=\sum_{i=1}^N|\theta_i|</script><p><code>Loss</code>çš„æ¢¯åº¦å˜ä¸º</p><script type="math/tex; mode=display">\nabla_{\theta} \hat{L}=\nabla_{\theta}L+\alpha \mathrm{sgn}(\theta)</script><p>å¸¦å…¥æ¢¯åº¦ä¸‹é™å…¬å¼</p><script type="math/tex; mode=display">\theta^{N+1} \leftarrow\theta^N-\epsilon\nabla_\theta \hat{L}=\theta^N-\epsilon\nabla_{\theta}L-\alpha\epsilon \mathrm{sgn}(\theta^N) = \theta ^N-\epsilon\nabla_\theta L -\alpha\epsilon \mathrm{sgn}(\theta^N)</script><p>å‘ç°å‚æ•°æ›´æ–°æ—¶ä¼šåœ¨æ¯ä¸€ä¸ªåˆ†é‡<code>i</code>ä¸­<strong>å‡</strong>æ‰ä¸€ä¸ªå›ºå®šå€¼<code>Î±Îµ*sgn(i)</code></p><p>L2 normä¸­ï¼Œæ¯æ¬¡éƒ½ä¹˜ä¸Šä¸€ä¸ªå°äº1çš„æ•°ï¼Œå½“å‚æ•°å¤§äº1æ—¶ï¼Œå‡å°çš„é€Ÿåº¦å°±éå¸¸å¿«ï¼›å½“å‚æ•°å°äº1æ—¶ï¼Œå‡å°çš„é€Ÿåº¦å°±æ¯”è¾ƒæ…¢ã€‚å› æ­¤L2ä¸­æœ€ç»ˆå„ä¸ªå‚æ•°æ¯”è¾ƒæ¥è¿‘ï¼Œä½†ä¸ä¼šå‡ºç°å¾ˆå¤š0ã€‚</p><p>è€ŒL1 normä¸­ï¼Œæ¯æ¬¡å‡å°ä¸€ä¸ªå›ºå®šå€¼ã€‚å¯¹å¾ˆå¤§çš„å‚æ•°ï¼Œå‡æ³•çš„æ­¥é•¿å¾ˆå°ï¼Œå‡å°çš„æ•ˆæœå°±ä¸æ˜¯å¾ˆæ˜æ˜¾ã€‚è€Œå‚æ•°å¾ˆå°æ—¶ï¼Œå‡æ³•çš„æ­¥é•¿ç›¸å¯¹å¾ˆå¤§ï¼Œå°±å¾ˆå®¹æ˜“å‡ºç°0ã€‚å› æ­¤L1ä¸­å¾ˆå®¹æ˜“å‡ºç°å¾ˆå¤§çš„å‚æ•°å’Œè¾ƒå¤šçš„0ã€‚</p><p>å«è¾ƒå¤š0çš„çŸ©é˜µå«åšç¨€ç–çŸ©é˜µ(Sparse matrix)ï¼Œå› æ­¤ç”¨L1èƒ½å¤Ÿå¾—åˆ°æ¯”è¾ƒç¨€ç–çš„å‚æ•°ã€‚</p><h2 id="Early-stopping"><a href="#Early-stopping" class="headerlink" title="Early stopping"></a>Early stopping</h2><p><img src="/2020/07/15/regularization-of-neural-network/early-stopping.png" alt="early stopping"></p><p>early stoppingçš„æ€è·¯å°±æ›´ç®€å•äº†ã€‚æ—¢ç„¶å¤æ‚çš„æ¨¡å‹è®­ç»ƒæ—¶ä¼šå…³æ³¨ä¸€äº›æ— å…³çš„ç‰¹å¾ï¼Œé‚£å¹²è„†ä¸è¦è·‘é‚£ä¹ˆå¤šæ¬¡è®­ç»ƒå°±å¥½äº†ã€‚</p><p>early stoppingçš„å½¢å¼åŒ–æè¿°ï¼š<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line">Let n be the number of steps between evaluations.</div><div class="line">Let p be the â€œpatience,â€ the number of times to observe worsening validation set error before giving up. </div><div class="line">Let Î¸^o be the initial parameters. </div><div class="line">Î¸ â† Î¸^o </div><div class="line">i â† 0 </div><div class="line">j â† 0 </div><div class="line">v â†âˆ </div><div class="line">Î¸âˆ— â† Î¸ </div><div class="line">iâˆ— â† i </div><div class="line">while j &lt; p do Î¸</div><div class="line">    Update by running the training algorithm for n steps. </div><div class="line">    i â† i + n</div><div class="line">    v&apos; â† ValidationSetError(Î¸) </div><div class="line">    if v&apos; &lt; v then </div><div class="line">        j  â† 0</div><div class="line">        Î¸âˆ— â† Î¸ </div><div class="line">        iâˆ— â† i</div><div class="line">        v  â† v&apos;</div><div class="line">    else </div><div class="line">        j â† j + 1 </div><div class="line">    end if</div><div class="line">end while</div></pre></td></tr></table></figure></p><hr><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">nodes:</div><div class="line">Best parameters are Î¸âˆ— , best number of training steps is iâˆ—</div></pre></td></tr></table></figure><h2 id="Ensemble"><a href="#Ensemble" class="headerlink" title="Ensemble"></a>Ensemble</h2><p>Ensembleè®¤ä¸ºï¼Œæ¨¡å‹ä¸­å‡ºç°çš„é”™è¯¯æ˜¯éšæœºçš„ï¼Œé‚£ä¹ˆä½¿ç”¨<strong>åŒæ ·</strong>çš„æ•°æ®è®­ç»ƒå‡ºkä¸ªä¸åŒçš„æ¨¡å‹ï¼Œå°†å®ƒä»¬çš„ç»“æœå–å¹³å‡å°±å¯ä»¥å¾—åˆ°æ¯”è¾ƒå¥½çš„ç»“æœã€‚</p><p>å¤æ‚æ¨¡å‹errorä¸»è¦æ¥è‡ªvarianceã€‚ä»ç›´è§‰ä¸Šè¯´ï¼Œå¯¹å¤šä¸ªæ¨¡å‹å–å¹³å‡å°±èƒ½å¾ˆå¥½çš„æŠµæ¶ˆä¸€éƒ¨åˆ†varianceã€‚<br>ensembleçš„åå¤„åœ¨äºï¼Œå®ƒéœ€è¦è®­ç»ƒkä¸ªä¸åŒçš„æ¨¡å‹ï¼Œå¯¼è‡´è®¡ç®—é‡å¤§å¢ã€‚æœ‰æ—¶è¿™æ ·çš„è®¡ç®—é‡æ˜¯æ— æ³•æ‰¿å—çš„ï¼Œå› æ­¤å¸Œæœ›æœ‰ä¸€ä¸ªä»£ä»·ä¸é‚£ä¹ˆå¤§çš„ensembleæ–¹æ³•ï¼Œè¿™å°±æ˜¯dropout</p><h3 id="dropout"><a href="#dropout" class="headerlink" title="dropout"></a>dropout</h3><p>dropoutå’Œä»¥å¾€çš„æ–¹æ³•ä¸åŒï¼Œå®ƒæŠŠå½“å‰è®­ç»ƒçš„æ¨¡å‹å½“ä½œæ˜¯å¾ˆå¤šæ¨¡å‹çš„å åŠ ï¼šæ¯æ¬¡è®­ç»ƒæ—¶ï¼Œæ¯ä¸ªå•å…ƒéƒ½æœ‰pçš„æ¦‚ç‡ä»ç½‘ç»œä¸­è¢«ç§»é™¤ï¼Œæ¯æ¬¡åªé’ˆå¯¹è¿™äº›è¢«ç•™ä¸‹æ¥çš„å•å…ƒåšå‚æ•°çš„æ›´æ–°ã€‚æœ‰nä¸ªå•å…ƒæ—¶ï¼Œå› ä¸ºæ¯ä¸€ä¸ªå•å…ƒéƒ½å¯ä»¥è¢«ç•™ä¸‹/ä¸¢æ‰ï¼Œè¿™å°±å¯èƒ½äº§ç”Ÿ2^Nä¸ªä¸åŒçš„ç½‘ç»œç»“æ„ã€‚dropoutå°±è®¤ä¸ºå½“å‰çš„æ¨¡å‹æ˜¯è¿™2^Nä¸ªç½‘ç»œçš„ensembleã€‚</p><p>ä¸‹é¢æ˜¯ä¸€ä¸ª2å±‚çš„ç¥ç»ç½‘ç»œã€‚å®ƒæœ‰2ä¸ªéšè—å•å…ƒã€‚dropoutå¯ä»¥äº§ç”Ÿçš„ç½‘ç»œç”±16ä¸ªã€‚ä½¿ç”¨dropoutçš„ç½‘ç»œç»è¿‡ä¸€æ¬¡è®­ç»ƒåç›¸å½“äºè®­ç»ƒäº†2^Nä¸ªå­ç½‘ç»œï¼Œè®¡ç®—é‡çš„é—®é¢˜å°±è¿™æ ·è§£å†³äº†ã€‚<br><img src="/2020/07/15/regularization-of-neural-network/dropout.png" alt="droupout"></p><p>ä½¿ç”¨dropoutçš„ç½‘ç»œè®­ç»ƒå®Œæˆåï¼Œåœ¨æµ‹è¯•æ—¶ä¸å†ç§»é™¤ç¥ç»å…ƒï¼Œè€Œæ˜¯ç»™ç½‘ç»œæ•´ä½“çš„å‚æ•°ä¹˜ä¸Š1-pã€‚<br>ä¸€ä¸ªç›´è§‰çš„è§£é‡Šï¼š<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">è®¾wæ˜¯dropoutå¾—åˆ°çš„å‚æ•°ï¼Œw&apos;æ˜¯è®­ç»ƒæ—¶çš„å‚æ•°ã€‚</div><div class="line">åœ¨è®­ç»ƒæ—¶ç¥ç»å…ƒæœ‰pçš„æ¦‚ç‡è¢«ä¸¢å¼ƒï¼Œæ‰€ä»¥ç½‘ç»œä¸­å®é™…ç¥ç»å…ƒä¸ªæ•°çš„æœŸæœ›æ˜¯n*(1-p)ã€‚</div><div class="line">ç°åœ¨è¦è®©ç½‘ç»œçš„è¾“å‡ºå°½å¯èƒ½çš„ä¸€è‡´ï¼Œåˆ™</div><div class="line">n*w*(1-p) = n*w&apos;</div><div class="line">å³w&apos;=w*(1-p)</div></pre></td></tr></table></figure></p><p><img src="/2020/07/15/regularization-of-neural-network/linearity.png" alt="linearity"></p><p>å®é™…çš„åˆ†æå‘ç°ï¼Œåœ¨æ•´ä¸ªç½‘ç»œæ˜¯çº¿æ€§æ—¶ï¼Œä¸Šé¢çš„è®ºè¿°æ˜¯ç²¾ç¡®æˆç«‹çš„ï¼Œè€ŒåŠ ä¸Šæ¿€æ´»å‡½æ•°çš„ç½‘ç»œå¾€å¾€ä¸æ˜¯çº¿æ€§çš„ã€‚ä½†å®é™…ä½¿ç”¨ä¸Šdropoutçš„æ€§èƒ½ä¹Ÿç¡®å®æ¯”è¾ƒæ¥è¿‘ensembleçš„ç»“æœã€‚ä¸ºä»€ä¹ˆä¼šè¿™æ ·æˆ‘ä»¬ä¸æ¸…æ¥šï¼Œæ€»ä¹‹æ‹¿æ¥ä¸»ä¹‰äº†ã€‚ä¸–ç•Œå„åœ°æœ‰å¾ˆå¤šç§‘å­¦å®¶ä¹Ÿåœ¨æ¢ç©¶èƒŒåçš„å¥¥ç§˜ï¼Œå¸Œæœ›æœ‰ä¸€å¤©èƒ½æ‰¾å‡ºä¸€ä¸ªç²¾ç¡®çš„è§£é‡Šå§ã€‚</p>]]></content>
    
    <summary type="html">
    
      &lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML&quot; async&gt;&lt;/script&gt;

&lt;p&gt;Deep learning algorithms are typically applied to extremely complicated domains where the true generation process essentially involves simulating the entire universeâ€¦ Controlling the complexity of the model is not a simple matter of finding the model of the right size, with the right number of parameters. Instead, we might findâ€”and indeed in practical deep learning scenarios, we almost always do findâ€”that the best fitting model (in the sense of minimizing generalization error) is a large model that has been regularized appropriately&lt;/p&gt;
    
    </summary>
    
      <category term="Machine learning" scheme="https://verrickt.github.io/categories/Machine-learning/"/>
    
    
      <category term="Machine learning" scheme="https://verrickt.github.io/tags/Machine-learning/"/>
    
      <category term="Regularization" scheme="https://verrickt.github.io/tags/Regularization/"/>
    
  </entry>
  
  <entry>
    <title>åå‘ä¼ æ’­-è¿›é˜¶ç‰ˆçš„æ¢¯åº¦ä¸‹é™ç®—æ³•</title>
    <link href="https://verrickt.github.io/2020/07/11/back-propagation-gradient-descent-improved/"/>
    <id>https://verrickt.github.io/2020/07/11/back-propagation-gradient-descent-improved/</id>
    <published>2020-07-11T03:21:40.000Z</published>
    <updated>2021-01-11T12:07:36.706Z</updated>
    
    <content type="html"><![CDATA[<p><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML" async></script><br>æœ¬æ–‡æ˜¯<a href="http://speech.ee.ntu.edu.tw/~tlkagk/courses_ML20.html" target="_blank" rel="external">NTU ML 2020</a>ä¸­<a href="https://youtu.be/Dr-WRlEFefw" target="_blank" rel="external">BackPropagation</a>éƒ¨åˆ†çš„ç¬”è®°ã€‚</p><p>æ¢¯åº¦ä¸‹é™æ˜¯éå¸¸å¸¸ç”¨çš„ä¼˜åŒ–ç®—æ³•ã€‚è€Œåœ¨æ·±åº¦å­¦ä¹ ä¸­ï¼Œä¸€ä¸ªç¥ç»ç½‘ç»œçš„å‚æ•°åŠ¨è¾„å‡ ç™¾ä¸‡ï¼Œåƒçº¿æ€§æ¨¡å‹é‚£æ ·äººæ‰‹å·¥ç®—å¥½å†å‘Šè¯‰æœºå™¨çš„æ–¹å¼å·²ç»ä¸åˆé€‚äº†ï¼Œéœ€è¦ä¸€ç§æ›´é«˜æ•ˆçš„ç®—æ³•æ¥è®¡ç®—æ¢¯åº¦ã€‚<br><a id="more"></a><br>åå‘ä¼ æ’­æ˜¯è®¡ç®—æ¢¯åº¦çš„é«˜æ•ˆç®—æ³•ï¼Œå®ƒçš„ç†è®ºåŸºç¡€æ˜¯é“¾å¼æ³•åˆ™ã€‚</p><script type="math/tex; mode=display">p=\phi(x),q=\varphi(x),g=f(p,q) \\\frac{dg}{dx}=\frac{\partial g}{\partial p}\frac{dp}{dx}+\frac{\partial g}{\partial q}\frac{dq}{dx}</script><hr><p>æ¢¯åº¦ä¸‹é™çš„ç›®æ ‡å‡½æ•°<code>total loss</code>æ˜¯å„ä¸ªexampleçš„å’Œ</p><script type="math/tex; mode=display">L(\theta)=\sum_{n=1}^N C^{n}(\theta)</script><p>æ‰€ä»¥åªéœ€èƒ½è®¡ç®—å‡ºå…¶ä¸­ä¸€é¡¹çš„<code>loss</code>å°±å¯æ±‚<code>total loss</code>çš„æ¢¯åº¦ã€‚</p><p>è€ƒè™‘å¦‚ä¸‹çš„ä¸€ä¸ª3x2çš„ç¥ç»ç½‘ç»œ<br><img src="/2020/07/11/back-propagation-gradient-descent-improved/network_structure.png" alt="network structure"></p><p>ç¬¬ä¸€å±‚ç¬¬ä¸€ä¸ªç¥ç»å…ƒçš„å‚æ•°<code>w1</code>ï¼Œè¾“å…¥æ¿€æ´»å‡½æ•°çš„å€¼<code>z=w1*x1+w2*x2+b1</code><br>åˆ™ç”±é“¾å¼æ³•åˆ™</p><script type="math/tex; mode=display">\frac{\partial L}{\partial w_1}=\frac{\partial L}{\partial z}\frac{\partial z}{\partial w_1}=\frac{\partial L}{\partial z}x_1</script><p>è½¬åŒ–ä¸ºæ±‚</p><script type="math/tex; mode=display">\frac{\partial L}{\partial z}</script><p>è€ƒè™‘è¯¥ç¥ç»å…ƒçš„ä¸‹ä¸€å±‚ï¼Œ<code>z</code>ç»è¿‡æ¿€æ´»åçš„è¾“å‡º<code>a</code>è¢«å½“ä½œä¸‹ä¸€å±‚çš„è¾“å…¥ï¼š<br><img src="/2020/07/11/back-propagation-gradient-descent-improved/notions_backward_pass.png" alt="backward-pass"><br>åˆ™ç”±é“¾å¼æ³•åˆ™ï¼Œ</p><script type="math/tex; mode=display">\frac{\partial L}{\partial z} = \frac{\partial L}{\partial a}\frac{\partial a}{\partial z}=\frac{\partial L}{\partial a}\sigma'(z)</script><p>è§‚å¯Ÿåˆ°<code>a</code>å·²ç»å˜æˆäº†ä¸‹ä¸€å±‚ç½‘ç»œçš„è¾“å…¥ï¼Œé—®é¢˜åˆåŒ–ä¸ºäº†æœ€åˆæ±‚<code>loss</code>å¯¹ç½‘ç»œè¾“å…¥çš„å¾®åˆ†ï¼Œä½†é—®é¢˜çš„è§„æ¨¡-æ±‚æ¢¯åº¦çš„â€œå±‚æ•°â€å´å‡å°‘äº†ä¸€å±‚ã€‚æ€ç»´æ•é”çš„åŒå­¦å¯èƒ½å·²ç»å‘ç°äº†ï¼Œè¿™æ˜¯åˆ†æ²»æ³•ã€‚</p><p>è®©æˆ‘ä»¬æ›´è¿›ä¸€æ­¥</p><script type="math/tex; mode=display">\frac{\partial L}{\partial a}=\frac{\partial L}{\partial z'}\frac{\partial z'}{\partial a}+\frac{\partial L}{\partial z''}\frac{\partial z''}{\partial a}= \frac{\partial L}{\partial z'} w_3+ \frac{\partial L}{\partial z''} w_4</script><p>ä»è€Œ</p><script type="math/tex; mode=display">\frac{\partial L}{\partial z}=\sigma'(z) \left[\frac{\partial L}{\partial z'} w_3+ \frac{\partial L}{\partial z''} w_4 \right]</script><p><img src="/2020/07/11/back-propagation-gradient-descent-improved/amplify.png" alt="amplify"></p><p>çœ‹èµ·æ¥å°±åƒæ˜¯ä¸€ä¸ªä»¥åå¾®åˆ†ä¸ºè¾“å…¥,<code>y=Ïƒ&#39;(z)x</code>ä¸ºæ¿€æ´»å‡½æ•°çš„ç¥ç»å…ƒçš„è¾“å‡º</p><p>è€ƒè™‘æ‰€æœ‰æ¿€æ´»å‡½æ•°çš„è¾“å…¥<code>z</code>ï¼Œå½“å‰å±‚çš„åå¾®åˆ†<code>z</code>ä¸åä¸€å±‚ä¹‹é—´çš„åå¾®åˆ†<code>z&#39;</code>å­˜åœ¨è®¡ç®—ä¸Šçš„ä¾èµ–å…³ç³»ã€‚å› æ­¤è®¡ç®—<code>loss</code>å¯¹<code>z</code>çš„åå¾®åˆ†æ—¶ï¼Œè¦ä»åå¾€å‰ç®—ã€‚å› æ­¤å¾—å<code>backward pass</code>ã€‚<br><img src="/2020/07/11/back-propagation-gradient-descent-improved/dependencies.png" alt="dependencies"></p><p>è€Œè®¡ç®—æœ¬å±‚ç¥ç»å…ƒçš„è¾“å…¥<code>a</code>æ—¶æ˜¯ä»å‰å¾€åç®—çš„ï¼Œå› æ­¤å«åš<code>foreward pass</code></p><p><img src="/2020/07/11/back-propagation-gradient-descent-improved/sum.png" alt="sum"></p><p>æ€»ç»“ä¸€ä¸‹ï¼Œè¦è®¡ç®—Lå¯¹å‚æ•°<code>w</code>çš„åå¾®åˆ†éœ€è¦è¿›è¡Œä¸¤ä¸ªæ­¥éª¤ï¼š</p><ol><li>foreward pass. ç®—å‡ºLå¯¹å½“å‰å±‚ç¥ç»å…ƒçš„è¾“å…¥<code>a</code>çš„åå¾®åˆ†ã€‚ä»å‰å¾€åç®—ã€‚</li><li>backward pass. è®¡ç®—å‡ºLå¯¹å½“å‰å±‚æ¿€æ´»å‡½æ•°çš„è¾“å…¥<code>z</code>çš„åå¾®åˆ†ã€‚ä»åå¾€å‰ç®—ã€‚</li></ol><p>ä¸¤è€…ç›¸ä¹˜ï¼Œå³å¾—</p><script type="math/tex; mode=display">\frac{\partial L}{\partial w}</script>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML&quot; async&gt;&lt;/script&gt;&lt;br&gt;æœ¬æ–‡æ˜¯&lt;a href=&quot;http://speech.ee.ntu.edu.tw/~tlkagk/courses_ML20.html&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;NTU ML 2020&lt;/a&gt;ä¸­&lt;a href=&quot;https://youtu.be/Dr-WRlEFefw&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;BackPropagation&lt;/a&gt;éƒ¨åˆ†çš„ç¬”è®°ã€‚&lt;/p&gt;
&lt;p&gt;æ¢¯åº¦ä¸‹é™æ˜¯éå¸¸å¸¸ç”¨çš„ä¼˜åŒ–ç®—æ³•ã€‚è€Œåœ¨æ·±åº¦å­¦ä¹ ä¸­ï¼Œä¸€ä¸ªç¥ç»ç½‘ç»œçš„å‚æ•°åŠ¨è¾„å‡ ç™¾ä¸‡ï¼Œåƒçº¿æ€§æ¨¡å‹é‚£æ ·äººæ‰‹å·¥ç®—å¥½å†å‘Šè¯‰æœºå™¨çš„æ–¹å¼å·²ç»ä¸åˆé€‚äº†ï¼Œéœ€è¦ä¸€ç§æ›´é«˜æ•ˆçš„ç®—æ³•æ¥è®¡ç®—æ¢¯åº¦ã€‚&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Machine learning" scheme="https://verrickt.github.io/categories/Machine-learning/"/>
    
    
      <category term="Machine learning" scheme="https://verrickt.github.io/tags/Machine-learning/"/>
    
      <category term="Backpropagation" scheme="https://verrickt.github.io/tags/Backpropagation/"/>
    
  </entry>
  
  <entry>
    <title>ä»åˆ†ç±»åˆ°å¯¹ç‡å›å½’å†åˆ°åˆ°ç¥ç»ç½‘ç»œ</title>
    <link href="https://verrickt.github.io/2020/07/04/from-logistic-regression-to-neural-network/"/>
    <id>https://verrickt.github.io/2020/07/04/from-logistic-regression-to-neural-network/</id>
    <published>2020-07-04T14:01:07.000Z</published>
    <updated>2021-01-11T12:07:36.726Z</updated>
    
    <content type="html"><![CDATA[<p><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML" async></script></p><h2 id="å‰è¨€"><a href="#å‰è¨€" class="headerlink" title="å‰è¨€"></a>å‰è¨€</h2><p>æœ¬æ–‡çš„æ˜¯å°æ¹¾å¤§å­¦2020æœºå™¨å­¦ä¹ çš„Classification<a href="https://youtu.be/fZAZUYEeIMg" target="_blank" rel="external">1</a>å’Œ<a href="https://youtu.be/hSXFuypLukA" target="_blank" rel="external">2</a>çš„ç¬”è®°ï¼Œå¥‰è¡Œ<a href="https://en.wikipedia.org/wiki/Lazy_evaluation" target="_blank" rel="external">Lazy evaluation</a>ç­–ç•¥ï¼Œå¯¹è¿™äº›çŸ¥è¯†æ›´æ·±å±‚æ¬¡çš„æ¢ç©¶åªåœ¨<strong>ç»å¯¹å¿…è¦</strong>æ—¶å®Œæˆã€‚<br><a id="more"></a></p><h2 id="åˆ†ç±»"><a href="#åˆ†ç±»" class="headerlink" title="åˆ†ç±»"></a>åˆ†ç±»</h2><p>åœ¨åˆ†ç±»é—®é¢˜ï¼Œæˆ‘ä»¬è¦æ‰¾ä¸€ä¸ªå‡½æ•°fï¼Œfçš„è¾“å…¥æ˜¯ä»£è¡¨æ ·æœ¬çš„å‘é‡ï¼Œè¾“å‡ºæ˜¯ä¸€ä¸ªä»£è¡¨ç±»åˆ«çš„æ ‡é‡ã€‚å°†æ ·æœ¬åˆ†ä¸ºä¸¤ç±»çš„å«åšäºŒå…ƒåˆ†ç±»ï¼Œå¤šäºä¸¤ç±»çš„å«åšå¤šå…ƒåˆ†ç±»ã€‚æˆ‘ä»¬å…ˆè€ƒè™‘äºŒå…ƒåˆ†ç±»é—®é¢˜ã€‚</p><h3 id="å¼•ä¾‹"><a href="#å¼•ä¾‹" class="headerlink" title="å¼•ä¾‹"></a>å¼•ä¾‹</h3><p>XCOMä¸­é¢å¯¹çš„æ•Œäººæœ‰å˜ç§äºº(ADVENT)å’Œå¤–æ˜Ÿäºº(Alien)ä¸¤ç§ï¼Œé•¿è€(The elder)é€æ¥äº†æ–°çš„ç‰©ç§ï¼Œç°æœ‰è®­ç»ƒèµ„æ–™å˜ç§äººæ•°æ®15ç»„ï¼Œå¤–æ˜Ÿäºº10ç»„ï¼Œä½ èƒ½æ®æ­¤å¸®åŠ©è¢«å…³åœ¨å¤–æ˜Ÿäººç½‘ç»œä¸­å……å½“é¦–è„‘çš„äººç±»æŒ‡æŒ¥å®˜åˆ†è¾¨æ–°ç‰©ç§çš„ç§ç±»å—ï¼Ÿ</p><p>å…ˆè€ƒè™‘è¿™ä¸ªé—®é¢˜ï¼Œè“ç»¿ä¸¤ä¸ªç›’å­ï¼Œå…¶ä¸­å„æœ‰è“ç»¿çƒè‹¥å¹²ã€‚ä»è“ç›’å­æŠ½çƒçš„æ¦‚ç‡æ˜¯1/3ï¼Œä»ç»¿ç›’å­æŠ½çƒçš„æ¦‚ç‡æ˜¯2/3ã€‚å·²çŸ¥æŠ½åˆ°äº†è“çƒï¼Œé—®ä»è“è‰²ç›’å­é‡ŒæŠ½åˆ°çš„æ¦‚ç‡æ˜¯å¤šå°‘?<br><img src="/2020/07/04/from-logistic-regression-to-neural-network/two_boxes.png" alt="ä¸¤ä¸ªç›’å­"></p><script type="math/tex; mode=display">P(è“ç›’å­)=\frac{1}{3} \\P(ç»¿ç›’å­)=\frac{2}{3} \\P(æŠ½åˆ°è“çƒ|è“ç›’å­)=\frac{4}{5} \\P(ç»¿ç›’å­|æŠ½åˆ°è“çƒ)=\frac{2}{5} \\P(æŠ½åˆ°è“çƒ)=P(è“ç›’å­)P(æŠ½åˆ°è“çƒ|è“ç›’å­)+P(ç»¿ç›’å­)P(ç»¿ç›’å­|æŠ½åˆ°è“çƒ)=\frac{1*4}{3*5}*\frac{2*2}{3*5}=\frac{8}{15} \\P(è“ç›’å­|æŠ½åˆ°è“çƒ)=\frac{P(æŠ½åˆ°è“çƒ|è“ç›’å­)P(è“ç›’å­)}{P(æŠ½åˆ°è“çƒ)}=\frac{\frac{1*4}{3*5}}{\frac{8}{15}}=\frac{1}{2}</script><h3 id="è´å¶æ–¯å…¬å¼"><a href="#è´å¶æ–¯å…¬å¼" class="headerlink" title="è´å¶æ–¯å…¬å¼"></a>è´å¶æ–¯å…¬å¼</h3><p>æˆ‘ä»¬é¢„æµ‹çš„ä¾æ®æ˜¯è´å¶æ–¯å…¬å¼ï¼š</p><script type="math/tex; mode=display">P(A|B)=\frac{P(B|A)P(A)}{P(B)}</script><p>ç”¨C1è¡¨ç¤ºæ•Œäººå±äºå˜ç§äººï¼ŒC2è¡¨ç¤ºæ•Œäººå±äºå¤–æ˜Ÿäººï¼Œé‚£ä¹ˆ</p><script type="math/tex; mode=display">P(C_1|x)=\frac{P(x|C_1)P(C_1)}{P(x|C_1)P(C_1)+P({x|C_2})P(C_2))}</script><p>training setä¸­ä¸€å…±æœ‰15ç»„å˜ç§äººï¼Œ10ç»„å¤–æ˜Ÿäººã€‚åˆ™</p><script type="math/tex; mode=display">P(C_1)=\frac{15}{15+10}=\frac{3}{5} \\P(C_2)=\frac{10}{15+10}=\frac{2}{5}</script><p>æ¥ä¸‹æ¥æ˜¯ç¡®å®šP(x|C1)ã€‚æ€è€ƒä¸€ä¸‹ï¼Œtesting-setä¸Šçš„æ•°æ®æˆ‘ä»¬çš„ç®—æ³•ä»æ¥æ²¡æœ‰è§è¿‡ï¼Œé‚£ä¹ˆP(x|C1)åº”è¯¥æ˜¯0å’¯ã€‚å¯è¿™æ ·è´å¶æ–¯å…¬å¼å°±å˜æˆ0/0äº†ï¼Œè¿˜æ€ä¹ˆé¢„æµ‹?</p><h3 id="P-C1-x"><a href="#P-C1-x" class="headerlink" title="P(C1|x)"></a>P(C1|x)</h3><p>åœ¨æ­¤ï¼Œæˆ‘ä»¬å¤§èƒ†å‡è®¾ï¼Œç›®å‰æ‰€è§è¿‡çš„C1çš„exampleæ˜¯ç”±ä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒäº§ç”Ÿçš„ï¼Œè¿™æ ·å°±å¯ä»¥å¯¹testing-setä¸Šæ²¡è§è¿‡çš„æ•°æ®äº†æ±‚æ¦‚ç‡äº†ã€‚</p><p><img src="/2020/07/04/from-logistic-regression-to-neural-network/prior.png" alt="å…ˆéªŒ"></p><p>è¿™æ˜¯è¯¾ä»¶çš„ä¸€å¼ å›¾ï¼Œå¯¹äºå›¾ä¸­79ä¸ªexampleï¼Œè®¤ä¸ºæ˜¯ä¸€ä¸ªGaussianäº§ç”Ÿçš„ï¼Œä½†å¥½å¤šä¸ªGaussianéƒ½å¯ä»¥äº§ç”Ÿè¿™æ ·çš„ç‚¹ï¼Œå…·ä½“æ˜¯å“ªä¸€ä¸ªå‘¢ï¼Ÿè¿™é‡Œä½¿ç”¨æå¤§ä¼¼ç„¶çš„æ€æƒ³ï¼Œå°±è®¤ä¸ºä½¿å¾—äº§ç”Ÿè®­ç»ƒæ•°æ®çš„æ¦‚ç‡æœ€å¤§çš„Gaussianå¥½äº†</p><p>å›åˆ°æˆ‘ä»¬çš„ä¾‹å­ï¼Œä¼¼ç„¶å‡½æ•°æ˜¯</p><script type="math/tex; mode=display">L(\mu,\Sigma)=\prod_i^{15}f_{\mu,\Sigma}(x^i) \\f_{\mu,\Sigma}=\frac{1}{2\pi^{D/2}}\frac{1}{|\Sigma|^{1/2}}\exp{\{-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)\}}\\\mu^*,\Sigma^*=\argmax_{\mu,\Sigma} L(\mu,\Sigma)</script><p>ç»è¿‡ä¸€ç•ªè¿ç®—åï¼Œå¾—åˆ°Î¼å’ŒÎ£çš„é—­å¼è§£:</p><script type="math/tex; mode=display">\mu^*=\frac{1}{15}\sum_{n=1}^{15}x^n\Sigma^*=\frac{1}{15}\sum_{n=1}^{15}(x^n-\mu^*)(x^n-\mu^*)^T</script><p>è¿™æ ·å°±å¾—åˆ°äº†C1çš„åˆ†å¸ƒ<code>G1(Î¼,Î£)</code>ï¼ŒåŒç†å¯ä»¥å¾—åˆ°C2çš„åˆ†å¸ƒ<code>G2(Î¼,Î£)</code>ï¼Œæœ‰äº†è¿™ä¸¤é¡¹å°±å¯ä»¥è®¡ç®—æ¡ä»¶æ¦‚ç‡ä»è€Œå®Œæˆé¢„æµ‹äº†ã€‚<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">if(x|C1)&gt;0.5 output ADVENT</div><div class="line">else output ALIEN</div></pre></td></tr></table></figure></p><p><img src="/2020/07/04/from-logistic-regression-to-neural-network/res_1.png" alt="ç»“æœ"><br>å›åˆ°ä¸Šè¯¾çš„ä¾‹å­ï¼Œè¿™ä¸ªæ¨¡å‹çš„æ•ˆæœå¹¶ä¸å¥½:(ã€‚</p><p>æƒ³åˆ°<a href="../../../06/26/machine-learning-basics-probability-and-information-theory/">Probability</a>ä¸­Gaussionçš„PS</p><blockquote><p>å¦‚æœç”¨é«˜æ–¯åˆ†å¸ƒæ¥åšåˆ†ç±»é—®é¢˜ï¼Œè®©åˆ†å¸ƒå…±ç”¨åæ–¹å·®çŸ©é˜µÎ£é€šå¸¸æ•ˆæœä¼šæ¯”ä½¿ç”¨å„è‡ªçš„åæ–¹å·®çŸ©é˜µæ•ˆæœå¥½ã€‚</p></blockquote><p>é‚£å°±è¯•è¯•å…±ç”¨åæ–¹å·®çŸ©é˜µå§</p><h3 id="å…±ç”¨åæ–¹å·®çŸ©é˜µ"><a href="#å…±ç”¨åæ–¹å·®çŸ©é˜µ" class="headerlink" title="å…±ç”¨åæ–¹å·®çŸ©é˜µ"></a>å…±ç”¨åæ–¹å·®çŸ©é˜µ</h3><p>ä¼¼ç„¶å‡½æ•°å˜ä¸º</p><script type="math/tex; mode=display">L(\mu_1,\mu_2,\Sigma)=\prod_i^{15}f_{\mu1,\Sigma}(x^i)\prod_{j=16}^{25}f_{\mu2,\Sigma}(x^i) \\</script><p><img src="/2020/07/04/from-logistic-regression-to-neural-network/res_2.png" alt="å…±ç”¨covariance-matrix"><br>æ±‚è§£è¿‡ç¨‹çœç•¥ï¼Œæ¥çœ‹ä¸€ä¸‹ç»“æœï¼Œå‡†ç¡®åº¦ä¸€ä¸‹å­å°±ä¸Šæ¥äº†ã€‚è¿™æ—¶å‘ç°åˆ†ç•Œçº¿å˜æˆäº†ä¸€æ¡ç›´çº¿ã€‚è¿™ä¸çº¿æ€§æ¨¡å‹æœ‰å…³ç³»å—ï¼Ÿæ¨ä¸€ä¸‹çœ‹çœ‹</p><h3 id="Why-linear-ä¸€äº›æ¨å¯¼"><a href="#Why-linear-ä¸€äº›æ¨å¯¼" class="headerlink" title="Why linear?ä¸€äº›æ¨å¯¼"></a>Why linear?ä¸€äº›æ¨å¯¼</h3><script type="math/tex; mode=display">P(C_1|x)=\frac{P(x|C_1)P(C_1)}{P(x|C_1)P(C_1)+P({x|C_2})P(C_2))} \\</script><p>ä¸Šä¸‹åŒé™¤ä»¥åˆ†å­</p><script type="math/tex; mode=display">P(C_1|x)=\frac{1}{1+\frac{P({x|C_2})P(C_2)}{P(x|C_1)P(C_1)}}</script><script type="math/tex; mode=display">ä»¤z=\ln \frac{P({x|C_1})P(C_1)}{P(x|C_2)P(C_2)}ï¼Œåˆ™P(C_1|x)=\frac{1}{1+\exp(-z)}=\sigma(z)</script><p>sigmoidå‡½æ•°å°±è¿™æ ·å‡ºç°äº†!<br>ç»§ç»­åˆ†è§£z </p><script type="math/tex; mode=display">z=\ln \frac{P({x|C_1})P(C_1)}{P(x|C_2)P(C_2)}=\ln \frac{P({x|C_1})}{P(x|C_2)}+\ln\frac{P(C_1)}{P(C_2)}</script><script type="math/tex; mode=display">\lg \frac{P(C_1)}{P(C_2)}=\lg \frac{\frac{N_1}{N_1+N_2}}{\frac{N_2}{N_1+N_2}}=\lg \frac{N_1}{N2}</script><p>æ˜¯ä¸ªå¸¸æ•° </p><script type="math/tex; mode=display">P(x|C_1)=\frac{1}{2\pi^{D/2}}\frac{1}{|\Sigma|^{1/2}}\exp{\{-\frac{1}{2}(x-\mu^1)^T(\Sigma^1)^{-1}(x-\mu^1)\}}</script><script type="math/tex; mode=display">P(x|C_2)=\frac{1}{2\pi^{D/2}}\frac{1}{|\Sigma|^{1/2}}\exp{\{-\frac{1}{2}(x-\mu^2)^T(\Sigma^2)^{-1}(x-\mu^2)\}}</script><p>å¸¦å…¥ï¼Œåˆ™</p><script type="math/tex; mode=display">\ln \frac{P({x|C_1})}{P(x|C_2)} = \ln \frac{\frac{1}{2\pi^{D/2}}\frac{1}{|\Sigma|^{1/2}}\exp{\{-\frac{1}{2}(x-\mu^1)^T(\Sigma^1)^{-1}(x-\mu^1)\}}}{\frac{1}{2\pi^{D/2}}\frac{1}{|\Sigma|^{1/2}}\exp{\{-\frac{1}{2}(x-\mu^2)^T(\Sigma^2)^{-1}(x-\mu^2)\}}}</script><script type="math/tex; mode=display">=\ln \frac{|\Sigma^2|^{1/2}}{|\Sigma^1|^{1/2}} +\ln \exp\lgroup -\frac{1}{2}[(x-\mu^1)^T(\Sigma^1)^{-1}(x-\mu^1)-(x-\mu^2)^T(\Sigma^2)^{-1}(x-\mu^2)]\rgroup</script><script type="math/tex; mode=display">= \ln \frac{|\Sigma^2|^{1/2}}{|\Sigma^1|^{1/2}} +\lgroup -\frac{1}{2}[(x-\mu^1)^T(\Sigma^1)^{-1}(x-\mu^1)-(x-\mu^2)^T(\Sigma^2)^{-1}(x-\mu^2)]\rgroup</script><script type="math/tex; mode=display">= \ln \frac{|\Sigma^2|^{1/2}}{|\Sigma^1|^{1/2}} +\lgroup -\frac{1}{2}[x^T(\Sigma^1)^{-1}x-2(\mu^1)^T(\Sigma^1)^{-1}x+(\mu^1)^T(\Sigma^1)^{-1}\mu^1-x^T(\Sigma^2)^{-1}x+2(\mu^1)^T(\Sigma^2)^{-1}x-(\mu^2)^T(\Sigma^2)^{-1}\mu^2]\rgroup</script><script type="math/tex; mode=display">z= \ln \frac{|\Sigma^2|^{1/2}}{|\Sigma^1|^{1/2}} +\lgroup -\frac{1}{2}[x^T(\Sigma^1)^{-1}x-2(\mu^1)^T(\Sigma^1)^{-1}x+(\mu^1)^T(\Sigma^1)^{-1}\mu^1-x^T(\Sigma^2)^{-1}x+2(\mu^1)^T(\Sigma^2)^{-1}x-(\mu^2)^T(\Sigma^2)^{-1}\mu^2]\rgroup + \lg \frac{N_1}{N2}</script><p>å…±ç”¨åæ–¹å·®çŸ©é˜µæ—¶ï¼Œ</p><script type="math/tex; mode=display">\Sigma^1=\Sigma^2=\Sigma</script><script type="math/tex; mode=display"> \ln \frac{|\Sigma^2|^{1/2}}{|\Sigma^1|^{1/2}} =0 x^T(\Sigma^1)^{-1}x - x^T(\Sigma^2)^{-1}x=0</script><p> æ­¤æ—¶ </p><script type="math/tex; mode=display">z = (\mu^1-\mu^2)^T\Sigma^{-1}x-\frac{1}{2}(\mu^1)^T(\Sigma)^{-1}\mu^1+\frac{1}{2}(\mu^2)^T\Sigma^{-1}\mu^2+\ln \frac{N_1}{N2}</script><script type="math/tex; mode=display"> ä»¤w^T=(\mu^1-\mu^2)^T\Sigma^{-1},b=-\frac{1}{2}(\mu^1)^T(\Sigma)^{-1}\mu^1+\frac{1}{2}(\mu^2)^T\Sigma^{-1}\mu^2+\ln \frac{N_1}{N2}</script><script type="math/tex; mode=display">åˆ™z=w^T+b</script><script type="math/tex; mode=display">P(C_1|x)= \sigma(z)</script><p> æ˜¯zçš„å¹¿ä¹‰çº¿æ€§æ¨¡å‹ï¼Œç»™å®ƒä¸€ä¸ªåå­—ï¼Œå³logistic regressionã€‚</p><p> ç”±</p><script type="math/tex; mode=display">\mu^2,\mu^2,\Sigma^1,\Sigma^2</script><p> å¾—åˆ°wå’Œbçš„æ¨¡å‹ç§°ä¸ºç”Ÿæˆæ¨¡å‹(generative model)ï¼Œå› ä¸ºå„ä¸ªxå¯ä»¥ç”±ä¸€ç»„åˆ†å¸ƒç”Ÿæˆå‡ºæ¥ã€‚ ç›´æ¥æ‰¾å‡ºwå’Œbçš„æ¨¡å‹ç§°ä¸ºåˆ¤åˆ«æ¨¡å‹(discriminative model)</p><h2 id="logistic-regression"><a href="#logistic-regression" class="headerlink" title="logistic regression"></a>logistic regression</h2><h3 id="Step1-find-a-function-set"><a href="#Step1-find-a-function-set" class="headerlink" title="Step1, find a function set"></a>Step1, find a function set</h3><p>logistic regressioné—®é¢˜ä¸­ï¼Œæˆ‘ä»¬è¦æ‰¾ä¸€ç»„å‚æ•°wå’Œbï¼ŒÏƒ(wTx+b)ç»™å‡ºçš„å€¼æ˜¯xå±äºæ­£ä¾‹çš„æ¦‚ç‡ã€‚</p><h3 id="Step2-determine-goodness-of-function"><a href="#Step2-determine-goodness-of-function" class="headerlink" title="Step2, determine goodness of function"></a>Step2, determine goodness of function</h3><p>è¿™é‡Œä½¿ç”¨ä¼¼ç„¶å‡½æ•°<br><img src="/2020/07/04/from-logistic-regression-to-neural-network/goodness.png" alt="loss"></p><p>æœ€å°åŒ–å¯¹æ•°ä¼¼ç„¶å‡½æ•°(NLL)å®é™…ä¸Šå°±æ˜¯æœ€å°åŒ–äº¤å‰ç†µ</p><p><img src="/2020/07/04/from-logistic-regression-to-neural-network/cross_entropy.png" alt="cross_entropy"></p><h3 id="Step3-find-the-best-function"><a href="#Step3-find-the-best-function" class="headerlink" title="Step3, find the best function"></a>Step3, find the best function</h3><p>æ¢¯åº¦ä¸‹é™èµ°ä½ </p><h3 id="Why-likelihood-instead-of-MSE"><a href="#Why-likelihood-instead-of-MSE" class="headerlink" title="Why likelihood instead of MSE?"></a>Why likelihood instead of MSE?</h3><p><img src="/2020/07/04/from-logistic-regression-to-neural-network/MSE.png" alt="MSE"></p><p>è‹¥ä½¿ç”¨MSEï¼Œåˆ™åå¾®åˆ†çš„å€¼ä¸€ç›´æ˜¯0ï¼Œæ— æ³•è¿›è¡Œæ¢¯åº¦ä¸‹é™ã€‚<br><img src="/2020/07/04/from-logistic-regression-to-neural-network/cross_entropy_vs_MSE.png" alt="MSE&amp;cross entropy"><br>ç”±å›¾å¯è§ï¼Œåœ¨logistic regressionè¿™ä¸ªé—®é¢˜ä¸Šï¼Œäº¤å‰ç†µ(æå¤§ä¼¼ç„¶)ç¡®å®æ¯”MSEçš„æ¢¯åº¦æ›´å¤§ï¼Œæ›´é€‚åˆå½“ä½œè¯„ä»·å‡½æ•°å¥½åçš„æ ‡å‡†ã€‚</p><h3 id="å¤šå…ƒåˆ†ç±»"><a href="#å¤šå…ƒåˆ†ç±»" class="headerlink" title="å¤šå…ƒåˆ†ç±»"></a>å¤šå…ƒåˆ†ç±»</h3><p><img src="/2020/07/04/from-logistic-regression-to-neural-network/multi-class_classification.png" alt="Multiclass-classification"></p><h4 id="Step-1"><a href="#Step-1" class="headerlink" title="Step 1"></a>Step 1</h4><p>ä»¥ä¸‰å…ƒåˆ†ç±»ä¸ºä¾‹ï¼Œè¦åšä¸‰å…ƒåˆ†ç±»ï¼Œè¦æ‰¾ä¸‰ç»„å‚æ•°</p><script type="math/tex; mode=display">C_1:w^1,b^1 \text{  } z_1=w^1x+b_1 \\C_2:w^2,b^2 \text{  } z_2=w^2x+b_2 \\C_3:w^3,b^3 \text{  } z_3=w^3x+b_3 \\\mathrm{softmax}: \vec{z} \in \mathbb{R}^n\to \vec{y} \in\mathbb{R}^n \\\text{}\\\\y_i = \frac{\exp{z_i}}{\sum_{i=1}^3}</script><h4 id="Step-2"><a href="#Step-2" class="headerlink" title="Step 2"></a>Step 2</h4><p><img src="/2020/07/04/from-logistic-regression-to-neural-network/multi_class_loss.png" alt="Multiclass-Loss"></p><p>æŠŠ<code>z</code>ç»è¿‡<code>softmax</code>å‡½æ•°å˜æ¢åå¾—åˆ°çš„æ¦‚ç‡åˆ†å¸ƒ<code>y</code>ä¸çœŸå®å€¼è®¡ç®—äº¤å‰ç†µï¼Œå³ä¸ºéœ€è¦æœ€å°åŒ–çš„å‡½æ•°</p><h4 id="Step-3"><a href="#Step-3" class="headerlink" title="Step 3"></a>Step 3</h4><p>æ¢¯åº¦ä¸‹é™</p><h2 id="çº¿æ€§æ¨¡å‹çš„é™åˆ¶"><a href="#çº¿æ€§æ¨¡å‹çš„é™åˆ¶" class="headerlink" title="çº¿æ€§æ¨¡å‹çš„é™åˆ¶"></a>çº¿æ€§æ¨¡å‹çš„é™åˆ¶</h2><p>çº¿æ€§æ¨¡å‹çš„å‡½æ•°æ˜¯ç›´çº¿ï¼Œè€Œç¨å¾®å¤æ‚ç‚¹çš„é—®é¢˜æ˜¯æ— æ³•ç”¨ç›´çº¿è§£å†³çš„ã€‚ä¾‹å¦‚XORé—®é¢˜</p><p><img src="/2020/07/04/from-logistic-regression-to-neural-network/XOR.png" alt="XOR"></p><h3 id="feature-transformation"><a href="#feature-transformation" class="headerlink" title="feature transformation"></a>feature transformation</h3><p><img src="/2020/07/04/from-logistic-regression-to-neural-network/feature_transformation.png" alt="feature transformation"></p><p>é€šè¿‡å¯¹featureåšä¸€äº›å¤„ç†ï¼Œæ˜¯æœ‰å¯èƒ½çªç ´çº¿æ€§æ¨¡å‹çš„é™åˆ¶çš„ã€‚<br>ä½†éœ€è¦äººå·¥å‚ä¸ï¼Œè€Œä¸”æ²¡æœ‰ä¸€èˆ¬è§„å¾‹ã€‚è¿™å°±å˜æˆ<em>äººå·¥</em>å­¦ä¹ è€Œä¸æ˜¯<em>æœºå™¨</em>å­¦ä¹ äº†:(</p><h3 id="feature-transformation-with-linear-unit"><a href="#feature-transformation-with-linear-unit" class="headerlink" title="feature transformation with linear unit"></a>feature transformation with linear unit</h3><p><img src="/2020/07/04/from-logistic-regression-to-neural-network/cascading_logistic_regression.png" alt="cascading_logistic_regression"><br>ä½†æˆ‘ä»¬å¯ä»¥æŠŠfeature transformationçœ‹ä½œæ˜¯ä¸€ä¸ªçº¿æ€§çš„å˜åŒ–ï¼Œè¿™æ ·å°±å¯ä»¥ç”±logistic regressionæ¥è§£å†³ã€‚è¿™æ ·æŠŠå¤šä¸ªlogistic regressionä¸²èµ·æ¥ï¼Œå°±å¯ä»¥å¯¹featureåšå¤æ‚çš„å˜åŒ–ã€‚</p><p><img src="/2020/07/04/from-logistic-regression-to-neural-network/feature_transformation_with_linear_unit.png" alt="feature_transformation_with_linear_unit"><br>ä¸Šå›¾æ˜¯ä½¿ç”¨ä¸¤ä¸ªlogistic regressionè¿›è¡Œfeature transformationçš„ç»“æœï¼Œåªè¦å†æ¥ä¸Šä¸€ä¸ªlogistic regressionæ¥æ”¶ä»–ä¿©çš„è¾“å‡ºå°±å¯ä»¥è¿›è¡Œåˆ†ç±»äº†ã€‚</p><h3 id="Neural-network"><a href="#Neural-network" class="headerlink" title="Neural network"></a>Neural network</h3><p>ç”±æ­¤æ¨å¹¿ï¼ŒæŠŠè‹¥å¹²ä¸ªçº¿æ€§æ¨¡å‹ä¸²èµ·æ¥å°±å¯ä»¥å®ç°å¾ˆå¤šå¤æ‚çš„åŠŸèƒ½ã€‚é‚£ä¹ˆç»™logistic regressionçš„å•å…ƒèµ·ä¸ªæ–°åå­—å«åšç¥ç»å…ƒ(neuron)ï¼ŒæŠŠå®ƒä»¬äº’ç›¸è¿æ¥å½¢æˆçš„ç»“æ„å«åšç¥ç»ç½‘ç»œ(neural network)ï¼Œç¬é—´å°±é«˜å¤§ä¸Šèµ·æ¥äº†ã€‚</p><p>ç°åœ¨ä½ å¯ä»¥éª—éº»ç“œè¯´ã€Œæˆ‘ä»¬åœ¨æ¨¡æ‹Ÿäººç±»å¤§è„‘çš„è¿ä½œå®ç°äººå·¥æ™ºæ…§ã€äº†;-)</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML&quot; async&gt;&lt;/script&gt;&lt;/p&gt;
&lt;h2 id=&quot;å‰è¨€&quot;&gt;&lt;a href=&quot;#å‰è¨€&quot; class=&quot;headerlink&quot; title=&quot;å‰è¨€&quot;&gt;&lt;/a&gt;å‰è¨€&lt;/h2&gt;&lt;p&gt;æœ¬æ–‡çš„æ˜¯å°æ¹¾å¤§å­¦2020æœºå™¨å­¦ä¹ çš„Classification&lt;a href=&quot;https://youtu.be/fZAZUYEeIMg&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;1&lt;/a&gt;å’Œ&lt;a href=&quot;https://youtu.be/hSXFuypLukA&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;2&lt;/a&gt;çš„ç¬”è®°ï¼Œå¥‰è¡Œ&lt;a href=&quot;https://en.wikipedia.org/wiki/Lazy_evaluation&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Lazy evaluation&lt;/a&gt;ç­–ç•¥ï¼Œå¯¹è¿™äº›çŸ¥è¯†æ›´æ·±å±‚æ¬¡çš„æ¢ç©¶åªåœ¨&lt;strong&gt;ç»å¯¹å¿…è¦&lt;/strong&gt;æ—¶å®Œæˆã€‚&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Machine learning" scheme="https://verrickt.github.io/categories/Machine-learning/"/>
    
    
      <category term="Machine learning" scheme="https://verrickt.github.io/tags/Machine-learning/"/>
    
      <category term="classification" scheme="https://verrickt.github.io/tags/classification/"/>
    
      <category term="logistic regression" scheme="https://verrickt.github.io/tags/logistic-regression/"/>
    
      <category term="neural network" scheme="https://verrickt.github.io/tags/neural-network/"/>
    
  </entry>
  
  <entry>
    <title>æœºå™¨å­¦ä¹ åŸºç¡€â€”â€”çº¸ä¸Šè°ˆå…µ</title>
    <link href="https://verrickt.github.io/2020/07/03/machine-learning-in-pure-paperworks/"/>
    <id>https://verrickt.github.io/2020/07/03/machine-learning-in-pure-paperworks/</id>
    <published>2020-07-03T13:29:45.000Z</published>
    <updated>2021-01-11T12:07:36.755Z</updated>
    
    <content type="html"><![CDATA[<p><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML" async></script></p><h2 id="å‰è¨€"><a href="#å‰è¨€" class="headerlink" title="å‰è¨€"></a>å‰è¨€</h2><p>æœ¬æ–‡çš„ä¸»ä½“æ˜¯Ian Goodfellow ã€ŒDeep Learningã€ç¬¬äº”ç« ã€ŒMACHINE LEARNING BASICSã€ï¼Œæœªç»åŠ¨æ‰‹å®åšï¼Œæ•…ç§°çº¸ä¸Šè°ˆå…µã€‚ç›®å‰å¥‰è¡Œ<a href="https://en.wikipedia.org/wiki/Lazy_evaluation" target="_blank" rel="external">Lazy evaluation</a>ï¼Œå¯¹ç›¸å…³çŸ¥è¯†çš„è¡¥å……ä¼šåœ¨å®è·µåè¿›è¡Œã€‚<br><a id="more"></a></p><h2 id="what-is-machine-learning"><a href="#what-is-machine-learning" class="headerlink" title="what is machine learning"></a>what is machine learning</h2><blockquote><p>A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E</p></blockquote><h3 id="The-Task-T"><a href="#The-Task-T" class="headerlink" title="The Task T"></a>The Task T</h3><ul><li>Classification</li><li>Regression</li><li>Structured output<h3 id="The-performance-measure-P"><a href="#The-performance-measure-P" class="headerlink" title="The performance measure P"></a>The performance measure P</h3></li></ul><p>Usually we are interested in how well the machine learning algorithm performs<br>on data that it has <strong>not</strong> seen before, since this determines how well it will work when deployed in the real world. We therefore evaluate these performance measures using a <strong>test set</strong> of data that is separate from the data used for training the machine learning system.</p><p>Itâ€™s often difficult to choose a performance measure that corresponds well to the desired behavior of the system for two reasons</p><ul><li>The performance measure can be difficult to decide. When performing a regression task, should we penalize the system more if it frequently makes medium-sized mistakes or if it rarely makes very large mistakes? These kinds of design choices depend on the application</li><li>we know what quantity we would ideally like to measure, but measuring it is impractical. Computing the actual probability value assigned to a specific point in space in many density estimation models is intractable</li></ul><h3 id="The-Experience-E"><a href="#The-Experience-E" class="headerlink" title="The Experience E"></a>The Experience E</h3><p>Machine learning algorithms can be broadly categorized as unsupervised or supervised by what kind of experience they are allowed to have during the learning process.</p><ul><li>Unsupervised learning algorithms experience a dataset containing many features, then learn useful properties of the structure of this dataset. In the context of deep learning, we usually want to learn the entire probability distribution that generated a dataset, whether explicitly as in density estimation or implicitly for tasks like synthesis or denoising. Some other unsupervised learning algorithms perform other roles, like clustering, which consists of dividing the dataset into clusters of similar examples.</li><li>Supervised learning algorithms experience a dataset containing features,but each example is also associated with a label or target. </li></ul><p>Roughly speaking, unsupervised learning involves observing several examples<br>of a random vector x, and attempting to implicitly or explicitly learn the probability distribution p(x), or some interesting properties of that distribution, while supervised learning involves observing several examples of a random vector x and an associated value or vector y, and learning to predict y from x, usually by estimating <code>p(y|x)</code> The term supervised learning originates from the view of the target y being provided by an instructor or teacher who shows the machine learning system what to do. In unsupervised learning, there is no instructor or teacher, and the algorithm must learn to make sense of the data without this guide</p><p>Some machine learning algorithms do not just experience a fixed dataset. For<br>example, <strong>reinforcement learning</strong> algorithms interact with an environment, so there is a feedback loop between the learning system and its experiences</p><h3 id="Example-Linear-regression"><a href="#Example-Linear-regression" class="headerlink" title="Example: Linear regression"></a>Example: Linear regression</h3><ul><li>The task T: linear regression solves a regression problem. The goal is to build a system that can take a vector x âˆˆ R^n as input and predict the value of a scalar y âˆˆ R as its output.</li></ul><script type="math/tex; mode=display">\hat{y}=w^{\top}x+b</script><p><strong>w</strong> is a vector of the weights over different features.</p><p><strong>b</strong> is the bias(not the bias in statistic)</p><p>Together ,<strong>w</strong> and <strong>b</strong> are called the <strong>parameters</strong></p><p><strong>y</strong> is the <strong>label</strong> of the data.</p><p><strong>y-hat</strong> is the prediction value</p><ul><li>The performance measurement P:</li></ul><p>mean squared error(MSE)</p><script type="math/tex; mode=display">\mathrm{MSE_{test}}=\frac{1}{m}\sum_i(\hat{y}^{(test)}-y^{(test)})</script><h2 id="Capacity-Overfitting-and-Underfitting"><a href="#Capacity-Overfitting-and-Underfitting" class="headerlink" title="Capacity, Overfitting and Underfitting"></a>Capacity, Overfitting and Underfitting</h2><p>The central challenge in machine learning is that we must perform well on <em>new, previously unseen inputs</em>â€”not just those on which our model was trained. The ability to perform well on previously unobserved inputs is called generalization.</p><p>The factors determining how well a machine learning algorithm will perform are its ability to:</p><ol><li>Make the training error small. </li><li>Make the gap between training and test error small.</li></ol><p>These two factors are underfitting and overfitting. Underfitting occurs when the model is not able to obtain a sufficiently low error value on the training set. Overfitting occurs when the gap between the training error and test error is too large.</p><p>Informally, a modelâ€™s capacity is its ability to fit a wide variety of functions. Models with low capacity may struggle to fit the training set. Models with high capacity can overfit by memorizing properties of the training set that do not serve them well on the test set. Usually,the error mainly comes from bias in the case of underfitting and variance in the case of overfitting.<br><img src="/2020/07/03/machine-learning-in-pure-paperworks/capacity.png" alt="Capacity"><br><img src="/2020/07/03/machine-learning-in-pure-paperworks/optimal_capacity.png" alt="Capacity,overfitting and underfitting"></p><h3 id="Bayes-error"><a href="#Bayes-error" class="headerlink" title="Bayes error"></a>Bayes error</h3><p>The error incurred by an oracle making predictions from the true distribution<br><code>p(x,y)</code> is called the Bayes error.</p><blockquote><p>Why are there errors if we know the true probability distribution ?</p></blockquote><p>because there may still be some noise in the distribution</p><h3 id="How-about-training-size"><a href="#How-about-training-size" class="headerlink" title="How about training size"></a>How about training size</h3><p>Training and generalization error vary as the size of the training set varies.<br>Expected generalization error can never increase as the number of training examples increases. For non-parametric models, more data yields better generalization until the best possible error is achieved. Any fixed parametric model with less than optimal capacity will asymptote to an error value that exceeds the Bayes error. See figure<br>5.4 for an illustration. Note that it is possible for the model to have optimal<br>capacity and yet still have a large gap between training and generalization error. In this situation, we may be able to reduce this gap by gathering more training examples.<br><img src="/2020/07/03/machine-learning-in-pure-paperworks/training_set_size_and_errors.png" alt="The effect of the training dataset size on the train and test error"></p><h3 id="Hyperparameters"><a href="#Hyperparameters" class="headerlink" title="Hyperparameters"></a>Hyperparameters</h3><p>hyperparameters are use to control the behavior of the learning algorithm.</p><p>Eg. In linear regression, we could use a model</p><script type="math/tex; mode=display">\hat{y}=b+\sum_i^kw_ix^i</script><p><strong>k</strong> controls degree of the polynomial,which acts as a <em>capacity hyperparameter</em></p><p>Why hyperparameters?</p><p>Sometimes a setting is chosen to be a hyperparameter that the learning algorithm does not learn because it is difficult to optimize. The setting must be a hyperparameter because it is not appropriate to learn that hyperparameter on the training set. This applies to all hyperparameters that control model capacity(e.g:<strong>k</strong>). If learned on the training set, such hyperparameters would always choose the maximum possible model capacity, resulting in overfitting</p><p>How to adjust hyperparameters?</p><p>Validation set!</p><h3 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a>Regularization</h3><p>We can give a learning algorithm a preference for one solution in its<br>hypothesis space to another. This means that both functions are eligible, but one is preferred.(å³è¥¿ç“œä¹¦æåˆ°çš„å½’çº³å‡è®¾)</p><p>For example, we can modify the training criterion for linear regression to include<br><strong>weight decay</strong></p><script type="math/tex; mode=display">J(w)=\mathrm{MSE_{train}}+\lambda w^\top w</script><p>where Î» is a value chosen ahead of time that controls the strength of our preference for smaller weights. in this sense, Î» is also a hyperparameter<br><img src="/2020/07/03/machine-learning-in-pure-paperworks/weight_decay.png" alt="weight_decay"></p><h2 id="Frequentist-vs-baysian"><a href="#Frequentist-vs-baysian" class="headerlink" title="Frequentist vs baysian"></a>Frequentist vs baysian</h2><ul><li>Frequentist  statistics:<br>we assume that the true parameter value Î¸ is fixed but unknown, while the point estimate Î¸_hat is a function of the data. Since the data is drawn from a random process, any function of the data is random. Therefore Î¸_hat is a random variable</li><li>Bayesian Statistics:<br>the dataset is directly observed and so is not random. On the other hand, the true parameter Î¸ is unknown or uncertain and thus is represented as a random variable. Before observing the data, we represent our knowledge of Î¸ using <em>the prior probability distribution</em>, <strong>p(Î¸)</strong>. Generally, the machine learning practitioner selects a prior distribution that is quite broad (i.e. with high entropy) to reflect a high degree of uncertainty in the value of Î¸ before observing any data</li></ul><h3 id="Maximum-likelihood"><a href="#Maximum-likelihood" class="headerlink" title="Maximum likelihood"></a>Maximum likelihood</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">æœ€å¤§ä¼¼ç„¶ä¼°è®¡ï¼šä½¿å¾—ç°æœ‰è§‚æµ‹å€¼å‡ºç°æ¦‚ç‡æœ€å¤§çš„å‚æ•°Î¸</div></pre></td></tr></table></figure><p>The maximum likelihood estimator for is defined as</p><script type="math/tex; mode=display">\theta_{ML}=\argmax _\theta\prod_{i=1}^mp_{model}(x^i;\theta)</script><p>log-likelihood estimator</p><script type="math/tex; mode=display">\theta_{ML}=\argmax _\theta\sum_{i=1}^m\log p_{model}(x^i;\theta)</script><p>è§‚å¯ŸKL-Divergence:</p><script type="math/tex; mode=display">D_{KL}(\hat{p}_{data}\|p_{model})=\mathbb{E}_{x\sim \hat{p}_{data}}[\log \hat{p}_{data}(x) - \log \hat{p}_{model}(x)]</script><p>ä¸ºäº†æœ€å°åŒ–KL-Divergenceï¼Œåªéœ€æœ€å°åŒ–</p><script type="math/tex; mode=display">- \mathbb{E}_{x\sim \hat{p}_{data}} \log \hat{p}_{model}(x)]</script><p>è¿™ä¸æœ€å¤§åŒ–log-likelihoodæ˜¯<strong>ç­‰ä»·</strong>çš„</p><h2 id="Simple-machine-learning-algorithm"><a href="#Simple-machine-learning-algorithm" class="headerlink" title="Simple machine learning algorithm"></a>Simple machine learning algorithm</h2><h3 id="SVM"><a href="#SVM" class="headerlink" title="SVM"></a>SVM</h3><p>One of the most influential approaches to supervised learning is the support vector machine  This model is similar to logistic regression in that it is driven by a linear function </p><script type="math/tex; mode=display">y=w^\top x+b</script><p>. The SVM predicts that the positive class is present when y is positive. Likewise, it predicts that the negative class is present when y is negative.</p><p>One key innovation associated with support vector machines is the kernel trick. The kernel trick consists of observing that many machine learning algorithms can be written exclusively in terms of dot products between examples. For example, it can be shown that the linear function used by the support vector machine can be re-written as</p><script type="math/tex; mode=display">w^\top x+b=b+\sum_i^m \alpha_i x^\top x^{(i)}</script><p>where x^i is a training example and Î± is a vector of coefficients.Rewriting the learning algorithm this way allows us to replace x by the output of a given feature function Ï†(x) and the dot product with a function k(x,x^i) = Ï†(x)Â· Ï†(x^i) called a kernel.</p><p>The most commonly used kernel is the <strong>Gaussian kernel</strong></p><script type="math/tex; mode=display">k(u,v)=\mathbin{N}(u-v;0;\sigma^2I)</script><p>this kernel is also known as the radial basis function (RBF) kernel. The Gaussian kernel is performing a kind of template matching. A training example x associated with training label y becomes a template for class y. When a test point xâ€™ is near x according to Euclidean distance, the Gaussian kernel has a large response, indicating that xâ€™ is very similar to the x template. The model then puts a large weight on the associated training label y. Overall, the prediction will combine many such training labels weighted by the similarity of the corresponding training examples</p><h3 id="Gradient-descent-improved-stochastic-gradient-descent"><a href="#Gradient-descent-improved-stochastic-gradient-descent" class="headerlink" title="Gradient descent improved: stochastic gradient descent"></a>Gradient descent improved: stochastic gradient descent</h3><p>Idea:</p><ul><li>take a step over a â€˜minibatchâ€™ instead of observing the whole training-set</li><li>step cost does not depend on size of training set, thus achieving convergence much faster.</li></ul><p>A recurring problem in machine learning is that large training sets are necessary<br>for good generalization, but large training sets are also more computationally expensive.</p><p>the insight of stochastic gradient descent is that the gradient is an expectation.<br>The expectation may be approximately estimated using a small set of samples. Specifically, on each step of the algorithm, we can sample a <strong>minibatch</strong> of examples</p><p>For a fixed model size, the cost per SGD update does not depend on the training set size m. In practice, we often use a larger model as the training set size increases, but we are not forced to do so. The number of updates required to reach convergence usually increases with training set size. However, as m approaches infinity, the model will eventually converge to its best possible test error before SGD has sampled every example in the training set. Increasing m further will not extend the amount of training time needed to reach the modelâ€™s best possible test error. From this point of view, one can argue that the asymptotic cost of training a model with SGD is O(1) as a function of m</p><h2 id="Why-deep-learning"><a href="#Why-deep-learning" class="headerlink" title="Why deep learning?"></a>Why deep learning?</h2><h3 id="curse-of-dimensionality"><a href="#curse-of-dimensionality" class="headerlink" title="curse of dimensionality"></a>curse of dimensionality</h3><p>Many machine learning problems become exceedingly difficult when the number of dimensions in the data is high.</p><div class="table-container"><table><thead><tr><th style="text-align:center">dimension</th><th style="text-align:center">number of states</th></tr></thead><tbody><tr><td style="text-align:center">1</td><td style="text-align:center">n</td></tr><tr><td style="text-align:center">2</td><td style="text-align:center">n^2</td></tr><tr><td style="text-align:center">3</td><td style="text-align:center">n^3</td></tr></tbody></table></div><p>Traditional machine learning algorithm canâ€™t distinguish a state thatâ€™s not seen in the training set.</p><h3 id="Local-Constancy-and-Smoothness-Regularization"><a href="#Local-Constancy-and-Smoothness-Regularization" class="headerlink" title="Local Constancy and Smoothness Regularization"></a>Local Constancy and Smoothness Regularization</h3><p>In order to generalize well, machine learning algorithms need to be guided by prior beliefs about what kind of function they should learn.</p><p>Among the most widely used of these implicit â€œpriorsâ€ is the smoothness<br>prior or local constancy prior. This prior states that the function we learn should not change very much within a small region.<br>Many simpler algorithms rely exclusively on this prior to generalize well, and<br>as a result they <em>fail</em> to scale to the statistical challenges involved in solving AIlevel tasks.</p><h3 id="How-do-we-deal-with-curse-of-dimensionality"><a href="#How-do-we-deal-with-curse-of-dimensionality" class="headerlink" title="How do we deal with curse of dimensionality?"></a>How do we deal with curse of dimensionality?</h3><p>The key insight is that a very large number of regions, e.g., O(2^k), can be defined with O(k) examples, so long as we introduce some <strong>dependencies</strong> between the regions via additional assumptions about the underlying data generating distribution</p><p>The core idea in deep learning is that we assume that the data was generated by the composition of factors or features, potentially at multiple levels in a hierarchy.</p><h3 id="Manifold-Learning-amp-manifold-hypothesis"><a href="#Manifold-Learning-amp-manifold-hypothesis" class="headerlink" title="Manifold Learning &amp; manifold hypothesis"></a>Manifold Learning &amp; manifold hypothesis</h3><p>Manifold learning algorithms  assuming that most of R^n<br>consists of <strong>invalid inputs</strong>, and that interesting inputs occur only along<br>a collection of manifolds containing a small subset of points.</p><p>manifold hypothesis:</p><ul><li><p>probability distribution over images, text strings, and sounds that occur in real life is highly concentrated. </p></li><li><p>we can also imagine such neighborhoods and transformations, at least informally. In the case of images, we can certainly think of many possible transformations that allow us to trace out a manifold in image space: we can gradually dim or brighten the lights, gradually move or rotate objects in the image, gradually alter the colors on the surfaces of objects</p></li></ul><p>æµå½¢å‡è¯´:</p><ol><li>å¤§éƒ¨åˆ†æœ‰ç»“æ„çš„æ•°æ®çš„åˆ†å¸ƒå‡½æ•°å¹¶ä¸æ˜¯åˆ†æ•£çš„ï¼Œè€Œæ˜¯é›†ä¸­èšé›†åœ¨æŸäº›èŒƒå›´ã€‚<br>æ”¯æŒè¯æ®ï¼šéšæœºå–åƒç´ è¯•å›¾äº§ç”Ÿè€ŒæœŸå¾…å…¶äº§ç”Ÿæ—¥å¸¸ç”Ÿæ´»ä¸­çš„ç…§ç‰‡çš„æ¦‚ç‡æ˜¯å¾®ä¹å…¶å¾®çš„ã€‚éšæœºå–å­—æ¯æŒ‡æœ›å®ƒç”Ÿæˆä¸€ç¯‡æ–‡ç« çš„æ¦‚ç‡ä¹Ÿæ˜¯å¾®ä¹å…¶å¾®çš„ã€‚æ‰€ä»¥æœ‰ç»“æ„çš„æ•°æ®çš„åˆ†å¸ƒå‡½æ•°å¿…ç„¶æ˜¯åœ¨æŸä¸ªèŒƒå›´å†…èšé›†ï¼Œè€Œåœ¨å¤§éƒ¨åˆ†åŒºåŸŸåˆ†æ•£<br><img src="/2020/07/03/machine-learning-in-pure-paperworks/noise.png" alt="noise"></li><li>åœ¨æµå½¢çš„è¡¨é¢ç§»åŠ¨ï¼Œå°†å¯ä»¥å¾—åˆ°æµå½¢æ‰€ä»£è¡¨çš„å…¨éƒ¨æ•°æ®ã€‚å¦‚åœ¨ä»£è¡¨äººè„¸çš„æµå½¢ä¸Šç§»åŠ¨ï¼ŒAç‚¹ä»£è¡¨å¾®ç¬‘çš„äººï¼ŒBç‚¹ä»£è¡¨æµæ³ªçš„äººã€‚è‹¥ä»ç›´çº¿ç›´æ¥è¿‡å»ï¼Œåˆ™åœ¨Aï¼ŒBä¸­é—´ç‚¹æ‰€å¯¹åº”çš„æ•°æ®å¯èƒ½ä¸æ˜¯äººè„¸ï¼›è‹¥ä»æµå½¢çš„è¡¨é¢ç§»åŠ¨åˆ°Bï¼Œåˆ™æ•°æ®ä¸€ç›´éƒ½æ˜¯äººè„¸ã€‚ï¼ˆå¾®ç¬‘-&gt;çš±çœ‰-&gt;å“­æ³£ï¼‰</li></ol><p>è¯·å‚è€ƒ<a href="https://www.youtube.com/watch?v=BePQBWPnYuE" target="_blank" rel="external">Youtubeè§†é¢‘ï¼šMy understanding of the Manifold Hypothesis | Machine learning</a></p><iframe maxwidth="100%" width="560" height="315" src="https://www.youtube.com/embed/BePQBWPnYuE" style="max-width:100%" frameborder="0" allow="accelerometer; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML&quot; async&gt;&lt;/script&gt;&lt;/p&gt;
&lt;h2 id=&quot;å‰è¨€&quot;&gt;&lt;a href=&quot;#å‰è¨€&quot; class=&quot;headerlink&quot; title=&quot;å‰è¨€&quot;&gt;&lt;/a&gt;å‰è¨€&lt;/h2&gt;&lt;p&gt;æœ¬æ–‡çš„ä¸»ä½“æ˜¯Ian Goodfellow ã€ŒDeep Learningã€ç¬¬äº”ç« ã€ŒMACHINE LEARNING BASICSã€ï¼Œæœªç»åŠ¨æ‰‹å®åšï¼Œæ•…ç§°çº¸ä¸Šè°ˆå…µã€‚ç›®å‰å¥‰è¡Œ&lt;a href=&quot;https://en.wikipedia.org/wiki/Lazy_evaluation&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Lazy evaluation&lt;/a&gt;ï¼Œå¯¹ç›¸å…³çŸ¥è¯†çš„è¡¥å……ä¼šåœ¨å®è·µåè¿›è¡Œã€‚&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Machine learning" scheme="https://verrickt.github.io/categories/Machine-learning/"/>
    
    
      <category term="Machine learning" scheme="https://verrickt.github.io/tags/Machine-learning/"/>
    
      <category term="The flower book" scheme="https://verrickt.github.io/tags/The-flower-book/"/>
    
  </entry>
  
  <entry>
    <title>æœºå™¨å­¦ä¹ åŸºç¡€â€”â€”æ•°å€¼è®¡ç®—</title>
    <link href="https://verrickt.github.io/2020/06/28/machine-learning-basics-numerical-computation/"/>
    <id>https://verrickt.github.io/2020/06/28/machine-learning-basics-numerical-computation/</id>
    <published>2020-06-28T03:00:27.000Z</published>
    <updated>2021-01-11T12:07:36.742Z</updated>
    
    <content type="html"><![CDATA[<p><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML" async></script></p><h2 id="å‰è¨€"><a href="#å‰è¨€" class="headerlink" title="å‰è¨€"></a>å‰è¨€</h2><p>æœ¬æ–‡çš„ä¸»ä½“æ˜¯æœºå™¨å­¦ä¹ ä¸­æ‰€ç”¨åˆ°çš„æ•°å€¼è®¡ç®—çŸ¥è¯†ï¼Œå› æ­¤å¥‰è¡Œ<a href="https://en.wikipedia.org/wiki/Lazy_evaluation" target="_blank" rel="external">Lazy evaluation</a>ï¼Œå¯¹è¿™äº›çŸ¥è¯†æ›´æ·±å±‚æ¬¡çš„æ¢ç©¶åªåœ¨<strong>ç»å¯¹å¿…è¦</strong>æ—¶å®Œæˆã€‚<br><a id="more"></a></p><h2 id="Source-of-errors"><a href="#Source-of-errors" class="headerlink" title="Source of errors"></a>Source of errors</h2><h3 id="Overflow-and-Underflow"><a href="#Overflow-and-Underflow" class="headerlink" title="Overflow and Underflow"></a>Overflow and Underflow</h3><p>è®¡ç®—æœºçš„å†…å­˜æ˜¯æœ‰é™çš„ï¼Œè€Œæœ‰é™çš„å†…å­˜ä¸Šæ— æ³•å®ç°æ— é™ç²¾åº¦çš„æ•°å€¼è®¡ç®—ï¼Œå› æ­¤æ•°å€¼è®¡ç®—æ˜¯æœ‰è¯¯å·®çš„ã€‚</p><p>æ¥è¿‘é›¶çš„éé›¶æ•°å€¼å› ä¸ºèˆå…¥å˜ä¸º0ç§°ä¸ºUnderflowã€‚</p><p>è¶…å‡ºè¡¨ç¤ºèŒƒå›´çš„wrap-aroundç§°ä¸ºOverflow</p><ul><li>positive-overflow æŒ‡è¶…è¿‡èŒƒå›´çš„æ­£æ•°è¢«æˆªæ–­ä¸ºè´Ÿæ•°</li><li>negative-overflow æŒ‡è¶…è¿‡èŒƒå›´çš„è´Ÿæ•°è¢«æˆªæ–­ä¸ºæ­£æ•°</li></ul><h3 id="Poor-Conditioning"><a href="#Poor-Conditioning" class="headerlink" title="Poor Conditioning"></a>Poor Conditioning</h3><p>ConditionæŒ‡è¾“å…¥çš„å¾®å°å˜åŒ–å¼•èµ·å‡½æ•°å€¼å˜åŒ–çš„å‰§çƒˆç¨‹åº¦ã€‚è®¡ç®—æœºä¸­çš„æ•°å­—æ˜¯æœ‰è¡¨ç¤ºèŒƒå›´çš„ï¼Œå› æ­¤Conditionå¤§çš„å‡½æ•°ä¼šé‡åˆ°å¾ˆå¤šé—®é¢˜ã€‚</p><p>å¯¹äº</p><script type="math/tex; mode=display">f(\mathbf{x})=A_{-1}\mathbf{x},A \in \mathbb{R}^{nxn}</script><p>å¦‚æœAå¯ä»¥åšç‰¹å¾å€¼åˆ†è§£ï¼Œé‚£ä¹ˆfçš„condition numberå®šä¹‰ä¸º</p><script type="math/tex; mode=display">\max{i,j}\left|\frac{\lambda_i}{\lambda_j}\right|</script><h2 id="Optimization"><a href="#Optimization" class="headerlink" title="Optimization"></a>Optimization</h2><p>è§£å‡ºä½¿å‡½æ•°f(x)å–å¾—æœ€å¤§å€¼/æœ€å°å€¼çš„xçš„è¿‡ç¨‹ç§°ä¸ºä¼˜åŒ–(optimization)ã€‚æ±‚f(x)æœ€å¤§å€¼å¯ä»¥ç”¨æ±‚-f(x)çš„æœ€å°å€¼å®ç°ï¼Œå› æ­¤ä¸»è¦è®¨è®ºæ±‚æœ€å°å€¼ã€‚</p><h3 id="naive-gradient-descent"><a href="#naive-gradient-descent" class="headerlink" title="naive gradient descent"></a>naive gradient descent</h3><p>æ¢¯åº¦(gradient)ç»™å‡ºäº†å‡½æ•°å¢é•¿æœ€å¿«çš„æ–¹å‘ï¼Œé‚£ä¹ˆæ²¿ç€æ¢¯åº¦çš„åæ–¹å‘å°±å¯ä»¥å°†å‡½æ•°å€¼å‡å°‘ã€‚æ¢¯åº¦ä¸‹é™æ³•(gradient descent)å°±æ˜¯é‡‡ç”¨è¿™ä¸ªæ€è·¯ä¸€ç§æ–¹æ³•:</p><script type="math/tex; mode=display">x^{i+1} = x^{i} - \epsilon \nabla_xf(x)</script><p>å…¶ä¸­Îµæ˜¯å­¦ä¹ ç‡(learning rate)ã€‚ä¸€èˆ¬æ¥è¯´Îµæœ‰ä¸¤ç§å–æ³•</p><ul><li>è®¾ä¸ºä¸€ä¸ªå›ºå®šçš„å¾ˆå°çš„å¸¸é‡</li><li>å–å¤šä¸ªÎµï¼Œé€‰å…¶ä¸­è®©f(x-Îµâˆ‡f(x))æœ€å°çš„é‚£ä¸ªÎµã€‚è¿™ç§æ–¹æ³•åˆç§°ä¸ºline search</li></ul><h3 id="Beyond-the-Gradient-Jacobian-and-Hessian-Matrices"><a href="#Beyond-the-Gradient-Jacobian-and-Hessian-Matrices" class="headerlink" title="Beyond the Gradient: Jacobian and Hessian Matrices"></a>Beyond the Gradient: Jacobian and Hessian Matrices</h3><p>å¯¹äºä¸€ä¸ªè¾“å…¥å’Œè¾“å‡ºéƒ½æ˜¯å‘é‡çš„å‡½æ•°fï¼Œå«æœ‰å®ƒçš„æ‰€æœ‰åå¾®åˆ†çš„çŸ©é˜µç§°ä¸ºé›…å¯æ¯”çŸ©é˜µ(Jacobian matrix)</p><script type="math/tex; mode=display">f:\mathbb{R^m} \to \mathbb{R^n}</script><script type="math/tex; mode=display">J_{i,j}=\frac{\partial}{\partial x_j}f(x)_i å…¶ä¸­J \in \mathbb{R^{n\times m}}</script><p>åŒæ ·çš„ï¼Œå«æœ‰fçš„æ‰€æœ‰äºŒé˜¶åå¾®åˆ†çš„çŸ©é˜µç§°ä¸ºæµ·å¸ŒçŸ©é˜µ(Hessian matrix)</p><script type="math/tex; mode=display">\mathbf{}{H}(f)(x)_{i,j}=\frac{\partial ^2}{\partial x_i \partial x_j}f(x)</script><p>æµ·å¸ŒçŸ©é˜µå°±æ˜¯æ¢¯åº¦çš„é›…å¯æ¯”çŸ©é˜µã€‚</p><p>å½“äºŒé˜¶åå¾®åˆ†è¿ç»­æ—¶ï¼Œå¾®åˆ†è¿ç®—ç¬¦å…·æœ‰äº¤æ¢å¾‹ï¼Œå³</p><script type="math/tex; mode=display">H_{i,j}=H_{j,i}</script><p>æœºå™¨å­¦ä¹ ä¸­å¤§éƒ¨åˆ†å‡½æ•°féƒ½å…·æœ‰è¿ç»­çš„äºŒé˜¶åå¾®åˆ†ï¼Œå› æ­¤æµ·å¸ŒçŸ©é˜µæ˜¯å®å¯¹ç§°çŸ©é˜µï¼Œå› è€Œå¯ä»¥åšç‰¹å¾å€¼åˆ†è§£ï¼Œå¹¶ä¸”Uæ˜¯æ­£äº¤çŸ©é˜µã€‚</p><p>få¯¹æŸä¸ªå•ä½å‘é‡uæ–¹å‘uçš„äºŒé˜¶åå¾®åˆ†ç”±</p><script type="math/tex; mode=display">u^{\top} Hu</script><p>ç»™å‡ºã€‚</p><p>åœ¨x0å¤„å¯¹fåšäºŒé˜¶æ³°å‹’å±•å¼€</p><script type="math/tex; mode=display">f(x)\approx f(x^0)+(x-x^0)^{\top}g+\frac{1}{2}(x-x^0)^\top H(x-x^0)</script><p>Hä¸ºæµ·å¸ŒçŸ©é˜µï¼Œgä¸ºæ¢¯åº¦ã€‚å½“é‡‡ç”¨å­¦ä¹ ç‡Îµæ—¶ï¼Œæ¢¯åº¦ä¸‹é™çš„æ–°åæ ‡æ˜¯x0-Îµgï¼Œå¸¦å…¥æ³°å‹’å±•å¼€ï¼Œæœ‰</p><script type="math/tex; mode=display">f(x^0-\epsilon g) \approx f(x^0) - \epsilon g^{\top}g+\frac{1}{2}\epsilon^2g^{\top}Hg</script><p>å½“æœ€åä¸€é¡¹å°äºç­‰äºé›¶æ—¶ï¼ŒÎµå¯ä»¥ä»»é€‰ã€‚å½“æœ€åä¸€é¡¹å¤§äºé›¶æ—¶ï¼Œæœ‰</p><script type="math/tex; mode=display">\epsilon^*=\frac{g^{\top}g}{g^{\top}Hg}</script><p>æ—¶ä¸‹é™æœ€å¿«ã€‚æœ€åæƒ…å†µä¸‹ï¼Œgä¸Hé‡åˆï¼Œå› æ­¤å­¦ä¹ ç‡çš„æ•°é‡çº§ç”±</p><script type="math/tex; mode=display">\frac{1}{\lambda_{\max}}</script><p>å†³å®šã€‚</p><p>å­¦ä¹ ç‡è®¾çš„è¿‡å¤§æˆ–è¿‡å°éƒ½ä¼šé€ æˆé—®é¢˜ï¼Œè¿™å¯ä»¥ç”±æµ·å¸ŒçŸ©é˜µè§£å†³ã€‚å…¶ä¸­æœ€ç®€å•çš„å°±æ˜¯<strong>ç‰›é¡¿æ³•</strong></p><p>ç‰›é¡¿æ³•çš„æ­¥éª¤åŸºæœ¬æ€è·¯æ˜¯ï¼Œåœ¨xç‚¹åšäºŒé˜¶æ³°å‹’å±•å¼€ã€‚</p><script type="math/tex; mode=display">f(x)\approx f(x^0)+(x-x^0)^{\top}g+\frac{1}{2}(x-x^0)^\top H(x-x^0)</script><p>è§£å‡ºå‡½æ•°çš„critical point</p><script type="math/tex; mode=display">x^*=x^0-H(f)(x^0)^{-1}\nabla_xf(x^0)</script><p>fæ˜¯æ­£å®šäºŒæ¬¡å‡½æ•°æ—¶ï¼Œä¸€æ¬¡å°±æ‰¾åˆ°äº†æœ€ä½ç‚¹ã€‚å½“fä¸æ˜¯äºŒæ¬¡å‡½æ•°ä½†æ˜¯å¯ä»¥ç”¨äºŒæ¬¡å‡½æ•°å±€éƒ¨è¿‘ä¼¼æ—¶ï¼Œå¤šæ¬¡è¿­ä»£x*å³å¯ã€‚</p><p>ç‰›é¡¿æ³•åªé€‚ç”¨äºè§£æœ€å°å€¼ï¼Œè€Œæ¢¯åº¦ä¸‹é™æ²¡æœ‰è¿™ä¸ªé™åˆ¶ã€‚</p><p>åªç”¨åˆ°æ¢¯åº¦çš„ä¼˜åŒ–ç®—æ³•ï¼Œå¦‚æ¢¯åº¦ä¸‹é™ï¼Œå«åšä¸€é˜¶ä¼˜åŒ–ç®—æ³•ã€‚ä½¿ç”¨æµ·å¸ŒçŸ©é˜µçš„ç®—æ³•ï¼Œå¦‚ç‰›é¡¿æ³•ï¼Œå«åšäºŒé˜¶ä¼˜åŒ–ç®—æ³•ã€‚</p><h3 id="Constrained-optimization"><a href="#Constrained-optimization" class="headerlink" title="Constrained optimization"></a>Constrained optimization</h3><p>æœ‰æ—¶æˆ‘ä»¬å¸Œæœ›åœ¨æŸé›†åˆSä¸Šè€Œä¸æ˜¯R^nä¸Šæ‰¾å‡ºå‡½æ•°çš„æœ€å¤§/æœ€å°å€¼ï¼Œè¿™å«åšconstrainted optimization(å—é™ä¼˜åŒ–)ã€‚è½åœ¨sä¸­çš„ç‚¹ç§°ä¸ºfeasible point(å¯è¡Œç‚¹)</p><p>KKTæ–¹æ³•æ˜¯ä¸€ä¸ªå—é™ä¼˜åŒ–é—®é¢˜çš„é€šç”¨æ–¹æ³•ã€‚ä½¿ç”¨KKTæ–¹æ³•ï¼Œé¦–å…ˆå®šä¹‰å¹¿ä¹‰æ‹‰æ ¼æœ—æ—¥å‡½æ•°ã€‚</p><p>ä¸ºäº†å®šä¹‰å¹¿ä¹‰æ‹‰æ ¼æœ—æ—¥å‡½æ•°ï¼Œé¦–å…ˆéœ€è¦å°†Så®šä¹‰ä¸ºå‡½æ•°çš„é›†åˆ</p><script type="math/tex; mode=display">\mathbb{S}=\{x| \forall i,g^{(i)}(x)=0 \text{ and } \forall j,h^{(j)}(x) \le0 \}</script><p>å«æœ‰g(i)çš„æ–¹ç¨‹ç§°ä¸ºequality constraintsï¼Œå«æœ‰h(j)çš„æ–¹ç¨‹ç§°ä¸ºinequality constraintsã€‚</p><p>å¯¹äºæ¯ä¸ªå‡½æ•°ï¼Œå®šä¹‰Î»iå’ŒÎ±jã€‚ä»–ä»¬å«åšKKTä¹˜æ•°ã€‚</p><p>ç°åœ¨å®šä¹‰å¹¿ä¹‰æ‹‰æ ¼æœ—æ—¥å‡½æ•°</p><script type="math/tex; mode=display">L(x,\lambda,\alpha)=f(x)+\sum_i \lambda_i g^{(i)}(x)+\sum_j \alpha_jh^{(j)}(x)</script><p>æ¥ä¸‹æ¥åœ¨å¹¿ä¹‰æ‹‰æ ¼æœ—æ—¥å‡½æ•°ä¸Šä½¿ç”¨éå—é™ä¼˜åŒ–ç®—æ³•ï¼Œå°±å¯ä»¥è§£å†³å—é™ä¼˜åŒ–ç®—æ³•äº†ã€‚</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML&quot; async&gt;&lt;/script&gt;&lt;/p&gt;
&lt;h2 id=&quot;å‰è¨€&quot;&gt;&lt;a href=&quot;#å‰è¨€&quot; class=&quot;headerlink&quot; title=&quot;å‰è¨€&quot;&gt;&lt;/a&gt;å‰è¨€&lt;/h2&gt;&lt;p&gt;æœ¬æ–‡çš„ä¸»ä½“æ˜¯æœºå™¨å­¦ä¹ ä¸­æ‰€ç”¨åˆ°çš„æ•°å€¼è®¡ç®—çŸ¥è¯†ï¼Œå› æ­¤å¥‰è¡Œ&lt;a href=&quot;https://en.wikipedia.org/wiki/Lazy_evaluation&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Lazy evaluation&lt;/a&gt;ï¼Œå¯¹è¿™äº›çŸ¥è¯†æ›´æ·±å±‚æ¬¡çš„æ¢ç©¶åªåœ¨&lt;strong&gt;ç»å¯¹å¿…è¦&lt;/strong&gt;æ—¶å®Œæˆã€‚&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Machine learning" scheme="https://verrickt.github.io/categories/Machine-learning/"/>
    
    
      <category term="Machine learning" scheme="https://verrickt.github.io/tags/Machine-learning/"/>
    
      <category term="The flower book" scheme="https://verrickt.github.io/tags/The-flower-book/"/>
    
      <category term="Numberical computation" scheme="https://verrickt.github.io/tags/Numberical-computation/"/>
    
  </entry>
  
  <entry>
    <title>æœºå™¨å­¦ä¹ åŸºç¡€â€”â€”æ¦‚ç‡è®ºä¸ä¿¡æ¯è®º</title>
    <link href="https://verrickt.github.io/2020/06/26/machine-learning-basics-probability-and-information-theory/"/>
    <id>https://verrickt.github.io/2020/06/26/machine-learning-basics-probability-and-information-theory/</id>
    <published>2020-06-26T14:25:05.000Z</published>
    <updated>2021-01-11T12:07:36.749Z</updated>
    
    <content type="html"><![CDATA[<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML" async></script><h2 id="å‰è¨€"><a href="#å‰è¨€" class="headerlink" title="å‰è¨€"></a>å‰è¨€</h2><p>æœ¬æ–‡çš„ä¸»ä½“æ˜¯æœºå™¨å­¦ä¹ ä¸­æ‰€ç”¨åˆ°çš„æ¦‚ç‡è®ºçŸ¥è¯†ï¼Œå› æ­¤å¥‰è¡Œ<a href="https://en.wikipedia.org/wiki/Lazy_evaluation" target="_blank" rel="external">Lazy evaluation</a>ï¼Œå¯¹è¿™äº›çŸ¥è¯†æ›´æ·±å±‚æ¬¡çš„æ¢ç©¶åªåœ¨<strong>ç»å¯¹å¿…è¦</strong>æ—¶å®Œæˆã€‚æœ¬æ–‡å‡è®¾ä½ å­¦è¿‡ã€Šæ¦‚ç‡è®ºä¸æ•°ç†ç»Ÿè®¡ã€‹ï¼Œä»…æŒ‡å‡ºèŠ±ä¹¦çš„ç°æœ‰å®šä¹‰çš„ä¸åŒï¼ŒåŒæ—¶è¡¥å……èŠ±ä¹¦ä¸­ç‰¹æœ‰çš„çŸ¥è¯†<br><a id="more"></a></p><h2 id="Probability"><a href="#Probability" class="headerlink" title="Probability"></a>Probability</h2><h3 id="Conditional-probability"><a href="#Conditional-probability" class="headerlink" title="Conditional probability"></a>Conditional probability</h3><script type="math/tex; mode=display">P(A|B)=\frac{P(AB)}{P(B)}</script><p>Chain rule/product rule of conditiona probability</p><script type="math/tex; mode=display">P(x^{(1)},...,x^{(n)})=P(x^{(1)})\prod_{i=2}^{n}P(x^{1}|x^{(1)},...,x^{(n)})</script><p>eg</p><script type="math/tex; mode=display">P(a,b,c)=P(a|b,c)P(b,c)</script><script type="math/tex; mode=display">P(b,c)=P(b|c)P(c)</script><script type="math/tex; mode=display">P(a,b,c)=P(a|b,c)P(b|c)P(c)</script><h3 id="Independence"><a href="#Independence" class="headerlink" title="Independence"></a>Independence</h3><p>Indenpendence:</p><script type="math/tex; mode=display">\forall x \in x,y \in y,p(x=x,y=y)=p(x=x)p(y=y)</script><p>åˆ™x,yäº’ç›¸ç‹¬ç«‹ã€‚è®°ä½œ</p><script type="math/tex; mode=display">x \perp y</script><p>Conditional independent:</p><script type="math/tex; mode=display">\forall x \in x,y \in y ,z\in z,p(x=x,y=y|z=z)=p(x=x|z=z)p(y=y|z=z)</script><p>ç§°x,yåœ¨zä¸‹ç›¸äº’ç‹¬ç«‹ï¼Œè®°ä½œ</p><script type="math/tex; mode=display">x \perp y \mid z</script><h3 id="Covariance"><a href="#Covariance" class="headerlink" title="Covariance"></a>Covariance</h3><script type="math/tex; mode=display">Cov(f(x),g(y))=\mathbb{E}[(f(x))-\mathbb{E}[f(x)])(g(x))-\mathbb{E}[g(x)]]</script><p>åæ–¹å·®å€¼è¶Šé«˜ï¼Œæ„å‘³ç€få’Œgçš„å˜åŒ–éå¸¸å¤§ï¼Œå¹¶ä¸”åŒæ—¶è·ç¦»å„è‡ªçš„å‡å€¼å¾ˆè¿œã€‚å¦‚æœåæ–¹å·®æ˜¯æ­£å€¼ï¼Œé‚£ä¹ˆfå’Œgå€¾å‘äºåŒæ—¶ç›¸å¯¹å¤§çš„å€¼ã€‚å¦‚æœæ˜¯è´Ÿå€¼ï¼Œåˆ™ä¸€ä¸ªå–é«˜å€¼çš„åŒæ—¶å¦ä¸€ä¸ªå–ä½å€¼ã€‚</p><p>å‘é‡</p><script type="math/tex; mode=display">x \in \mathbb{R}^n</script><p>çš„covariance matrix(åæ–¹å·®çŸ©é˜µ)æ˜¯ä¸€ä¸ª<code>nxn</code>çš„æ–¹é˜µï¼Œå…¶ä¸­</p><script type="math/tex; mode=display">Cov(x)_{i,j}=Cov(x_i,x_j)</script><p>å¯¹äºå¯¹è§’çº¿çš„å…ƒç´ ï¼Œ</p><script type="math/tex; mode=display">Cov(x_i,x_i)=Var(x_i)</script><h3 id="Gaussian-Distribution"><a href="#Gaussian-Distribution" class="headerlink" title="Gaussian Distribution"></a>Gaussian Distribution</h3><p>é«˜æ–¯åˆ†å¸ƒï¼Œåˆç§°æ­£æ€åˆ†å¸ƒã€‚<br>æ¦‚ç‡å¯†åº¦å‡½æ•°(PDF):</p><script type="math/tex; mode=display">\mathcal{N}(x;\mu,\sigma)=\sqrt{\frac{1}{2 \pi \sigma^2}}exp(-\frac{1}{2\sigma^2}(x-\mu)^2)</script><p>è®¡ç®—æ¦‚ç‡å¯†åº¦æ—¶ï¼Œç»å¸¸è¦å–Ïƒçš„å¹³æ–¹å€’æ•°ï¼Œå·¥ç¨‹ä¸­å¸¸ä½¿ç”¨å¦ä¸€ä¸ªå‚æ•°Î²âˆˆ(0,âˆ)è¡¨ç¤ºé«˜æ–¯åˆ†å¸ƒçš„<strong>ç²¾å‡†åº¦</strong></p><script type="math/tex; mode=display">\mathcal{N}(x;\mu,\beta^{-1})=\sqrt{\frac{\beta}{2 \pi \sigma^2}}exp(-\frac{1}{2}\beta(x-\mu)^2)</script><p>é«˜æ–¯åˆ†å¸ƒçš„ç‰¹ç‚¹</p><ul><li>ç°å®ä¸­å¾ˆå¤šå¤æ‚çš„ç³»ç»Ÿå¯ä»¥ç”±é«˜æ–¯åˆ†å¸ƒå»ºæ¨¡ï¼ˆ<strong>ä¸­å¿ƒæé™å®šç†</strong>ï¼‰</li><li>åœ¨æ–¹å·®ç›¸åŒçš„æ‰€æœ‰åˆ†å¸ƒä¸­ï¼Œé«˜æ–¯åˆ†å¸ƒåœ¨å®æ•°èŒƒå›´ä¸Šçš„â€œä¸ç¡®å®šåº¦â€æœ€é«˜ã€‚æ¢å¥è¯è¯´ï¼Œé«˜æ–¯åˆ†å¸ƒæ˜¯æ‰€æœ‰åˆ†å¸ƒä¸­å¯¹æ ·æœ¬åšå‡ºæœ€å°‘å…ˆéªŒå‡è®¾çš„</li></ul><p>Nç»´é«˜æ–¯åˆ†å¸ƒï¼š</p><script type="math/tex; mode=display">\mathcal{N}(x;\mu,\Sigma)=\sqrt{\frac{1}{(2\pi)^n det(\Sigma)}}exp(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu))</script><p>å…¶ä¸­Î£æ˜¯æ­£å®šå¯¹ç§°çŸ©é˜µã€‚Î¼æ˜¯çŸ¢é‡å½¢å¼çš„åˆ†å¸ƒå‡å€¼ï¼ŒÎ£ç»™å‡ºåˆ†å¸ƒçš„åæ–¹å·®çŸ©é˜µã€‚ä¸ºäº†ä¾¿äºè®¡ç®—ï¼Œå¯¹äºNç»´é«˜æ–¯åˆ†å¸ƒï¼Œå¸¸ç”¨<strong>å‡†ç¡®åº¦çŸ©é˜µÎ²</strong>ä½œä¸ºå‚æ•°ï¼š</p><script type="math/tex; mode=display">\mathcal{N}(x;\mu,\beta^{-1})=\sqrt{\frac{det(\beta)}{(2\pi)^n }}exp(-\frac{1}{2}(x-\mu)^T\beta(x-\mu))</script><p>å®è·µä¸Šé€šå¸¸å°†åæ–¹å·®çŸ©é˜µå›ºå®šä¸ºå¯¹è§’é˜µã€‚æ›´ç®€å•çš„æ–¹å¼æ˜¯å°†<strong>isotropic matrix</strong>ä½œä¸ºåæ–¹å·®çŸ©é˜µï¼Œå…¶ä¸­<strong>isotropic matrix</strong>æŒ‡æ ‡é‡æ•°ä¹˜å•ä½çŸ©é˜µçš„ç»“æœã€‚</p><h3 id="Dirac-delta-distribution-amp-empirical-distribution"><a href="#Dirac-delta-distribution-amp-empirical-distribution" class="headerlink" title="Dirac delta distribution &amp; empirical distribution"></a>Dirac delta distribution &amp; empirical distribution</h3><p>æœ‰æ—¶æˆ‘ä»¬å¸Œæœ›æ‰€æœ‰çš„æ¦‚ç‡å¯†åº¦éƒ½èšé›†åœ¨ä¸€ä¸ªç‚¹é™„è¿‘ã€‚è¿™å¯ä»¥é€šè¿‡Dirac deltaå‡½æ•°</p><script type="math/tex; mode=display">$\delta(x)$$$å®ç°ï¼š$$p(x)=\delta(x-\mu)</script><p>Dirac deltaåˆ†å¸ƒå¸¸å¸¸è¢«ç”¨ä½œempirical distributionï¼ˆç»éªŒåˆ†å¸ƒï¼‰çš„ä¸€ä¸ªç»„ä»¶ï¼š</p><script type="math/tex; mode=display">\hat{p}(x)=\frac{1}{m}\sum_{i=1}^m\delta(x-x^{(i)})</script><p>empirical distributionåœ¨å…¨éƒ¨mä¸ªç‚¹</p><script type="math/tex; mode=display">x^{(1)},...x^{(m)}</script><p>ä¸Šæ”¾ç½®</p><script type="math/tex; mode=display">\frac{1}{m}</script><p>æ¦‚ç‡å¯†åº¦</p><h3 id="Mixture-distribution"><a href="#Mixture-distribution" class="headerlink" title="Mixture distribution"></a>Mixture distribution</h3><p>ç”¨å…¶ä»–ç®€å•çš„æ¦‚ç‡åˆ†å¸ƒæ¥å®šä¹‰æ¦‚ç‡åˆ†å¸ƒæ˜¯ååˆ†æ™®éçš„ï¼Œæ··åˆåˆ†å¸ƒï¼ˆmixture distribution)å°±æ˜¯è¿™æ ·ä¸€ç§æ–¹å¼ã€‚æ··åˆåˆ†å¸ƒç”±å¥½å‡ ä¸ªç»„ä»¶(component)ç»„æˆã€‚æ¯æ¬¡é‡‡æ ·æ—¶ï¼Œç”±ä¸€ä¸ªå¤šé‡åˆ†å¸ƒçš„ç»“æœé€‰æ‹©ç»„ä»¶æ ‡è¯†(component identity)ï¼Œç”±æ­¤æœ€ç»ˆç»“æœæ˜¯ç”±å“ªä¸€ä¸ªåˆ†å¸ƒç»™å‡ºçš„ã€‚</p><script type="math/tex; mode=display">P(x)=\sum_iP(c=i)P(x\mid c=i)</script><p>å…¶ä¸­P(c)æ˜¯æ‰€æœ‰ç»„ä»¶ä¸Šçš„å¤šé‡åˆ†å¸ƒã€‚</p><p>ä¸€ç§å¸¸è§ä¸”å¼ºå¤§çš„æ··åˆæ¨¡å‹æ˜¯é«˜æ–¯æ··åˆæ¨¡å‹ã€‚é«˜æ–¯æ··åˆæ¨¡å‹æ‰€æœ‰ç»„ä»¶éƒ½æ˜¯é«˜æ–¯åˆ†å¸ƒï¼Œä»–ä»¬å…·æœ‰ä¸åŒçš„å‚æ•°<code>Î¼</code>å’Œ<code>Î£</code>ã€‚æœ‰äº›åˆ†å¸ƒå¯ä»¥å¢åŠ é™åˆ¶ï¼Œå¦‚æ‰€æœ‰ç»„ä»¶å…±ç”¨åæ–¹å·®çŸ©é˜µç­‰ã€‚</p><p>é«˜æ–¯æ··åˆåˆ†å¸ƒæ˜¯æ¦‚ç‡å¯†åº¦çš„é€šç”¨è¿‘ä¼¼æ–¹å¼ã€‚å…·æœ‰è¶³å¤Ÿåˆ†é‡çš„é«˜æ–¯æ··åˆæ¨¡å‹å¯ä»¥ç”¨ä»»ä½•ç‰¹å®šçš„éé›¶è¯¯å·®é‡æ¥è¿‘ä¼¼ä»»ä½•å¹³æ»‘å¯†åº¦ã€‚</p><p>PSï¼Œå¦‚æœç”¨é«˜æ–¯åˆ†å¸ƒæ¥åšåˆ†ç±»é—®é¢˜ï¼Œè®©ä¸¤ä¸ªåˆ†å¸ƒå…±ç”¨åæ–¹å·®çŸ©é˜µÎ£é€šå¸¸æ•ˆæœä¼šæ¯”ä½¿ç”¨å„è‡ªçš„åæ–¹å·®çŸ©é˜µæ•ˆæœå¥½ã€‚</p><h3 id="Latent-variable"><a href="#Latent-variable" class="headerlink" title="Latent variable"></a>Latent variable</h3><p>éšå«å˜é‡ï¼ˆLatent variable)æŒ‡æ— æ³•ç›´æ¥è§‚æµ‹çš„éšæœºå˜é‡ã€‚æ··åˆåˆ†å¸ƒä¸­çš„ç»„ä»¶æ ‡è¯†å˜é‡cå°±æ˜¯éšå«å˜é‡ã€‚</p><script type="math/tex; mode=display">P(x,c)=P(x\mid c)P(c)</script><p>éšå«å˜é‡çš„åˆ†å¸ƒ<code>P(c)</code>å’Œæ¡ä»¶åˆ†å¸ƒ<code>P(x|c)</code>å…±åŒå†³å®šäº†<code>P(x)</code>çš„åˆ†å¸ƒã€‚å°½ç®¡<code>P(x)</code>å¯ä»¥åœ¨æ²¡æœ‰éšå«å˜é‡çš„æ¡ä»¶ä¸‹è¢«è®¡ç®—å‡ºæ¥ã€‚</p><h3 id="Useful-properties-of-Common-Functions"><a href="#Useful-properties-of-Common-Functions" class="headerlink" title="Useful properties of Common Functions"></a>Useful properties of Common Functions</h3><p>logistic sigmoid:</p><script type="math/tex; mode=display">\sigma(x)=\frac{1}{1+exp(-x)}</script><p>softplus:</p><script type="math/tex; mode=display">\zeta(x)=log(1+exp(x))</script><blockquote><p>why the name softplus?<br>Itâ€™s a â€œsoftededâ€ version of x^=max(0,x)</p></blockquote><h3 id="Bayesâ€™-Rule"><a href="#Bayesâ€™-Rule" class="headerlink" title="Bayesâ€™ Rule"></a>Bayesâ€™ Rule</h3><p>å·²çŸ¥<code>P(y|x)</code>å’Œ<code>P(x)</code>æ±‚<code>P(x|y)</code>æ—¶å¯ä»¥ä½¿ç”¨è´å¶æ–¯å…¬å¼ï¼š</p><script type="math/tex; mode=display">P(x|y)=\frac{P(x)P(y \mid x)}{P(y)}</script><p>å…¶ä¸­</p><script type="math/tex; mode=display">P(y)=\sum_xP(y \mid x)P(x)</script><h3 id="prior-probability-and-posterior-probability"><a href="#prior-probability-and-posterior-probability" class="headerlink" title="prior-probability and posterior-probability"></a>prior-probability and posterior-probability</h3><ul><li>prior-probability<br>å³å…ˆéªŒæ¦‚ç‡ã€‚æŒ‡æ ¹æ®ä»¥å¾€ç»éªŒå’Œåˆ†æå¾—åˆ°çš„æ¦‚ç‡</li><li>posterior-probability<br>åéªŒæ¦‚ç‡æ˜¯åœ¨è€ƒè™‘å’Œç»™å‡ºç›¸å…³è¯æ®æˆ–æ•°æ®åæ‰€å¾—åˆ°çš„æ¡ä»¶æ¦‚ç‡</li></ul><p>è€ƒè™‘bayesâ€™ Rule</p><script type="math/tex; mode=display">P(\theta \mid x)=\frac{P(x\mid\theta)P(\theta)}{P(x)}</script><ul><li>Î¸ï¼šparameter</li><li>xï¼šobserved value</li><li>P(x)ï¼ševidence</li><li>P(Î¸)ï¼šprior</li><li>P(x|Î¸)ï¼šlikelihood</li><li>P(Î¸|x)ï¼šposterior</li></ul><h3 id="PDF-of-y-where-y-g-x"><a href="#PDF-of-y-where-y-g-x" class="headerlink" title="PDF of y where y=g(x)"></a>PDF of y where y=g(x)</h3><p>å‡è®¾æœ‰ç°éšæœºå˜é‡xï¼Œyï¼Œå…¶ä¸­y=g(x)ï¼Œæ±‚yçš„PDF</p><p>PDFæ ¹æ®pdfçš„å®šä¹‰,</p><script type="math/tex; mode=display">P(x)_{x \in \delta}=\int_{x}p(x)dx</script><p>p(x)dxä¸ºxè½åœ¨æŸä¸€é‚»åŸŸÎ´å†…çš„æ¦‚ç‡ã€‚ç°ä¿ç•™è¯¥å±æ€§ï¼Œåˆ™æœ‰</p><script type="math/tex; mode=display">|p_y(g(x))dy|=|p_x(x)dx|</script><script type="math/tex; mode=display">p_y(y)=p_x(g^{-1}(y))\mid \frac{\partial x}{\partial y} \mid</script><script type="math/tex; mode=display">p_x(x)=p_y(g(x))\mid \frac{\partial g(x)}{\partial y} \mid</script><p>è€ƒè™‘é«˜ç»´æƒ…å†µï¼Œxä¸yä¸ºå‘é‡ï¼Œå®šä¹‰é›…å¯æ¯”çŸ©é˜µJï¼Œå…¶ä¸­</p><script type="math/tex; mode=display">J_{i,j}=\frac{\partial x_i}{\partial y_j}</script><p>åˆ™</p><script type="math/tex; mode=display">p_x(x)=p_y(g(x))\mid \det \left\{ \frac{\partial g(x)}{\partial x}\right\} \mid</script><h2 id="Information-theory"><a href="#Information-theory" class="headerlink" title="Information theory"></a>Information theory</h2><p>basic assemption:</p><ul><li>Likely events should have low information content, and in the extreme case, events that are guaranteed to happen should have no information content whatsoever.<ul><li>Less likely events should have higher information content.</li><li>Independent events should have additive information. For example, finding out that a tossed coin has come up as heads twice should convey twice as much information as finding out that a tossed coin has come up as heads once.</li></ul></li></ul><h3 id="Self-information-amp-Shanon-entropy"><a href="#Self-information-amp-Shanon-entropy" class="headerlink" title="Self-information &amp; Shanon entropy"></a>Self-information &amp; Shanon entropy</h3><p>å•ä¸€äº‹ä»¶æ‰€å«çš„ä¿¡æ¯ï¼Œå•ä½ä¸ºnat</p><script type="math/tex; mode=display">I(x)=-\log P(x)</script><p>æ•´ä¸ªæ¦‚ç‡åˆ†å¸ƒä¸Šçš„ä¸ç¡®å®šæ€§ï¼Œå³é¦™å†œç†µ</p><script type="math/tex; mode=display">H(x)=E_{x\sim P}[I(x)]=-\mathbb{E}_{x\sim P}[\log P(x)]</script><p>ä¹Ÿè®°ä½œH(P).</p><p>å½“P(x)å’ŒQ(x)ä¸ºç›¸åŒéšæœºå˜é‡xçš„åˆ†éƒ¨æ—¶ï¼Œä¸¤ç§åˆ†å¸ƒé—´çš„â€œè·ç¦»â€ç”¨<br>Kullback-Leibler (KL) divergenceå®šä¹‰ï¼š</p><script type="math/tex; mode=display">D_{KL}(P\|Q)=\mathbb{E}_{x\sim P}\left[\log\frac{P(x)}{Q(x)}\right]=\mathbb{E}_{x\sim p}[\log P(x) - \log Q(x)]</script><p>the value means the extra amount of information<br>needed to send a message containing symbols drawn from probability distribution P, when we use a code that was designed to minimize the length of messages drawn from probability distribution .</p><p>KL divergence</p><ul><li>non-negarive</li><li>not symmetric<script type="math/tex; mode=display">D_{KL}(P\|Q)\not ={D_{KL}(Q\|P)}</script></li></ul><p>ä¸KL divergenceå¯†åˆ‡ç›¸å…³çš„ä¸€ç§åº¦é‡æ˜¯cross-entropy<br>å®šä¹‰ä¸º</p><script type="math/tex; mode=display">H(P,Q) = H(P)+D_{KL}(P\|Q)=-\mathbb{E}_{x\sim P}\log Q(x)</script><p>Minimizing the cross-entropy with respect to Q is equivalent to minimizing the KL divergence, because Q does not participate in the omitted term.</p><h3 id="Structed-Probabilistic-Models"><a href="#Structed-Probabilistic-Models" class="headerlink" title="Structed Probabilistic Models"></a>Structed Probabilistic Models</h3><p>æœºå™¨å­¦ä¹ ä¸­çš„å‚æ•°æˆåƒä¸Šä¸‡ï¼Œä½¿ç”¨å«æœ‰è¿™ä¹ˆå¤šå‚æ•°çš„åˆ†å¸ƒä¸åˆ‡å®é™…ã€‚æ ¹æ®æ¡ä»¶æ¦‚ç‡çš„ä¹˜æ³•å…¬å¼ï¼Œå¯ä»¥æŠŠå¤§åˆ†å¸ƒæ‹†æˆå°åˆ†å¸ƒçš„ä¹˜ç§¯ï¼ˆè¿™ä¸€è¿‡ç¨‹å«factorization)ã€‚å½“ä½¿ç”¨CSä¸­çš„å›¾æ¥è¡¨ç¤ºè¿™ç§<code>factorization</code>æ—¶ï¼Œå°±æŠŠæ¨¡å‹ç§°ä¸ºstructured probabilistic modelæˆ–graphical modelã€‚structed probabilistic modelåˆ†ä¸ºä¸¤ç±»ï¼Œåˆ†åˆ«ä½¿ç”¨DAGå’ŒUAGã€‚</p><h4 id="DAG"><a href="#DAG" class="headerlink" title="DAG"></a>DAG</h4><p>Directed models use graphs with directed edges, and they represent factorizations into conditional probability distributions,  Specifically, a directed model contains one factor for every random variable xi in the distribution</p><script type="math/tex; mode=display">p(\mathbf{x})=\prod_ip(x_i|Pa\mathcal{G}(x_i))</script><p>where</p><script type="math/tex; mode=display">Pa\mathcal{G}(x_i)</script><p>is the parents of xi.</p><h4 id="UAG"><a href="#UAG" class="headerlink" title="UAG"></a>UAG</h4><p>Undirected models use graphs with undirected edges, and they represent<br>factorizations into a set of functions; unlike in the directed case, these functions are usually not probability distributions of any kind. <strong>Any</strong> set of nodes that are all <strong>connected to each other</strong> in G is called a <strong><a href="https://en.wikipedia.org/wiki/Clique_(graph_theory)" target="_blank" rel="external">clique</a></strong>. Each clique Ci in an undirected model is associated with a factor Ï†i . These factors are just functions, not probability distributions. The output of each factor must be non-negative, but there is no constraint that the factor must sum or integrate to 1 like a probability distribution.</p><p>The probability of a configuration of random variables is proportional to the<br>product of all of these factorsâ€”assignments that result in larger factor values are more likely. Of course, there is no guarantee that this product will sum to 1. We therefore divide by a normalizing constant Z, defined to be the sum or integral over all states of the product of the Ï† functions, in order to obtain a normalized probability distribution:</p><script type="math/tex; mode=display">p(x)=\frac{1}{Z}\prod_i\phi^{(i)}(\mathcal{C}^{(i)})</script><p>DAGå’ŒUAGéƒ½æ˜¯æè¿°æ¦‚ç‡åˆ†å¸ƒçš„æ–¹æ³•ï¼Œä»–ä»¬å¹¶ä¸æ˜¯äº’æ–¥çš„æ¦‚ç‡åˆ†å¸ƒã€‚ä½¿ç”¨DAGè¿˜æ˜¯UAGå¹¶ä¸æ˜¯æ¦‚ç‡åˆ†å¸ƒçš„å±æ€§ï¼Œè€Œæ˜¯æŸç§ç‰¹å®š<strong>æè¿°æ–¹å¼</strong>çš„å±æ€§</p>]]></content>
    
    <summary type="html">
    
      &lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML&quot; async&gt;&lt;/script&gt;

&lt;h2 id=&quot;å‰è¨€&quot;&gt;&lt;a href=&quot;#å‰è¨€&quot; class=&quot;headerlink&quot; title=&quot;å‰è¨€&quot;&gt;&lt;/a&gt;å‰è¨€&lt;/h2&gt;&lt;p&gt;æœ¬æ–‡çš„ä¸»ä½“æ˜¯æœºå™¨å­¦ä¹ ä¸­æ‰€ç”¨åˆ°çš„æ¦‚ç‡è®ºçŸ¥è¯†ï¼Œå› æ­¤å¥‰è¡Œ&lt;a href=&quot;https://en.wikipedia.org/wiki/Lazy_evaluation&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Lazy evaluation&lt;/a&gt;ï¼Œå¯¹è¿™äº›çŸ¥è¯†æ›´æ·±å±‚æ¬¡çš„æ¢ç©¶åªåœ¨&lt;strong&gt;ç»å¯¹å¿…è¦&lt;/strong&gt;æ—¶å®Œæˆã€‚æœ¬æ–‡å‡è®¾ä½ å­¦è¿‡ã€Šæ¦‚ç‡è®ºä¸æ•°ç†ç»Ÿè®¡ã€‹ï¼Œä»…æŒ‡å‡ºèŠ±ä¹¦çš„ç°æœ‰å®šä¹‰çš„ä¸åŒï¼ŒåŒæ—¶è¡¥å……èŠ±ä¹¦ä¸­ç‰¹æœ‰çš„çŸ¥è¯†&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Machine learning" scheme="https://verrickt.github.io/categories/Machine-learning/"/>
    
    
      <category term="Machine learning" scheme="https://verrickt.github.io/tags/Machine-learning/"/>
    
      <category term="Probability" scheme="https://verrickt.github.io/tags/Probability/"/>
    
      <category term="Information theory" scheme="https://verrickt.github.io/tags/Information-theory/"/>
    
      <category term="The flower book" scheme="https://verrickt.github.io/tags/The-flower-book/"/>
    
  </entry>
  
  <entry>
    <title>æœºå™¨å­¦ä¹ åŸºç¡€â€”â€”çº¿æ€§ä»£æ•°</title>
    <link href="https://verrickt.github.io/2020/06/24/machine-learning-basics-linear-algebra/"/>
    <id>https://verrickt.github.io/2020/06/24/machine-learning-basics-linear-algebra/</id>
    <published>2020-06-24T01:38:56.000Z</published>
    <updated>2021-01-11T12:07:36.737Z</updated>
    
    <content type="html"><![CDATA[<p><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML" async></script></p><h2 id="å‰è¨€"><a href="#å‰è¨€" class="headerlink" title="å‰è¨€"></a>å‰è¨€</h2><p>æœ¬æ–‡çš„ä¸»ä½“æ˜¯æœºå™¨å­¦ä¹ ä¸­æ‰€ç”¨åˆ°çš„çº¿æ€§ä»£æ•°çŸ¥è¯†ï¼Œå› æ­¤å¥‰è¡Œ<a href="https://en.wikipedia.org/wiki/Lazy_evaluation" target="_blank" rel="external">Lazy evaluation</a>ï¼Œå¯¹è¿™äº›çŸ¥è¯†æ›´æ·±å±‚æ¬¡çš„æ¢ç©¶åªåœ¨<strong>ç»å¯¹å¿…è¦</strong>æ—¶å®Œæˆã€‚<br><a id="more"></a></p><h2 id="Concepts"><a href="#Concepts" class="headerlink" title="Concepts"></a>Concepts</h2><h3 id="Tensor"><a href="#Tensor" class="headerlink" title="Tensor"></a>Tensor</h3><p>çŸ©é˜µæ˜¯å‘é‡çš„å»¶ç”³ï¼Œè€ŒTensor(å¼ é‡)åˆ™æ˜¯çŸ©é˜µçš„å»¶ç”³ã€‚<br>å‘é‡å¯ç”¨ä¸€ç»´æ•°ç»„è¡¨ç¤ºï¼ŒçŸ©é˜µå¯ç”¨äºŒç»´æ•°ç»„è¡¨ç¤ºï¼Œè€Œå¼ é‡å¯ç”¨ä¸‰ç»´æ•°ç»„è¡¨ç¤ºã€‚<br>è¡¨ç¤ºå¼ é‡Açš„ä¸€ä¸ªåˆ†é‡</p><script type="math/tex; mode=display">A_{i,j,k}</script><h3 id="Identify-matrix"><a href="#Identify-matrix" class="headerlink" title="Identify matrix"></a>Identify matrix</h3><p>Identity matrixä¸ºå•ä½çŸ©é˜µçš„å¦ä¸€åç§°ã€‚ä¸€èˆ¬ç”¨Iè¡¨ç¤ºï¼Œè€Œå•ä½çŸ©é˜µç”¨Eè¡¨ç¤ºã€‚</p><h3 id="matrix-inversion"><a href="#matrix-inversion" class="headerlink" title="matrix inversion"></a>matrix inversion</h3><p>èŠ±ä¹¦å®šä¹‰ï¼š</p><script type="math/tex; mode=display">A^{-1}A=I</script><p>å¹¶<strong>æœªé™å®š</strong>Aä¸ºæ–¹é˜µï¼Œå¹¶ä¸”æ˜¯å·¦é€†</p><p>èŠ±ä¹¦ä¹Ÿå®šä¹‰äº†å³é€†</p><script type="math/tex; mode=display">AA^{-1}=I</script><p>åŒæ ·<strong>æœªé™å®š</strong>Aä¸ºæ–¹é˜µ</p><p>è€Œåè¯´æ˜ï¼Œ<strong>æ–¹é˜µ</strong>çš„å·¦é€†å’Œå³é€†æ˜¯ç›¸åŒçš„</p><blockquote><p>For square matrices, the left inverse and right inverse are equal</p></blockquote><p>è€Œè¯¾æœ¬è®¤ä¸º<strong>éæ–¹é˜µ</strong>æ²¡æœ‰é€†çŸ©é˜µã€‚å¯¹å·¦é€†å’Œå³é€†ä¹Ÿä¸åšæ˜¾å¼åŒºåˆ†ã€‚</p><p>åœ¨èŠ±ä¹¦æ¥ä¸‹æ¥æåˆ°é€†çŸ©é˜µæ—¶ï¼Œè¿™å¯èƒ½æ˜¯ä¸€ä¸ªå‘ç‚¹</p><h3 id="Singular-matrix"><a href="#Singular-matrix" class="headerlink" title="Singular matrix"></a>Singular matrix</h3><p>é<strong>æ»¡ç§©</strong>çš„<strong>æ–¹é˜µ</strong>ç§°ä¸ºå¥‡å¼‚çŸ©é˜µã€‚</p><h3 id="Norms"><a href="#Norms" class="headerlink" title="Norms"></a>Norms</h3><p>åˆç§°èŒƒå¼ã€‚èŒƒå¼æ˜¯ä¸€ä¸ªå°†å‘é‡æ˜ å°„ä¸ºéè´Ÿæ•°é‡çš„å‡½æ•°ï¼Œç”¨ä»¥æè¿°å‘é‡çš„â€œå¤§å°â€ã€‚</p><p>æ­£å¼å®šä¹‰ï¼š</p><ul><li>f(x) = 0 â‡’ x = 0</li><li>f(x+y) â‰¤ f(x) + f(y)(ä¸‰è§’ä¸ç­‰å¼)</li><li>âˆ€Î±âˆˆR,f(Î±x) = |Î±|f(x)</li></ul><p>å¸¸ç”¨çš„èŒƒå¼:</p><ul><li>L2 norm<br>å‘é‡ä¸åŸç‚¹çš„è·ç¦»</li></ul><script type="math/tex; mode=display">||x||_2=(\sum_{i} |x_i|^2)^{\frac{1}{2}}</script><ul><li><p>Squared L2 norm<br>L2 normçš„å¹³æ–¹å½¢å¼ï¼ŒæŸäº›æƒ…å†µä¸‹æ˜“äºåˆ†æå’Œè®¡ç®—</p></li><li><p>L1 norm</p><script type="math/tex; mode=display">||x||_1=\sum_{i} |x_i|</script></li><li><p>max norm</p><script type="math/tex; mode=display">||x||_{\infty }={\max} |x_i|</script></li><li><p>Frobenius norm<br>è®¡ç®—çŸ©é˜µçš„â€œå¤§å°â€</p></li></ul><script type="math/tex; mode=display">||A||_F=\sqrt{(\sum_{i,j} A_{i,j}^2)}</script><h2 id="Operations"><a href="#Operations" class="headerlink" title="Operations"></a>Operations</h2><h3 id="Decomposition"><a href="#Decomposition" class="headerlink" title="Decomposition"></a>Decomposition</h3><p>å°†çŸ©é˜µåˆ†è§£ä¸ºæŸäº›æ›´åŸºç¡€çš„æˆåˆ†å¯ä»¥æ›´å¥½çš„å¸®åŠ©æˆ‘ä»¬åˆ†æå…¶ä¸­çš„ä¸€èˆ¬è§„å¾‹ã€‚</p><h4 id="Eigendecomposition"><a href="#Eigendecomposition" class="headerlink" title="Eigendecomposition"></a>Eigendecomposition</h4><p>åŸºäºç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡åˆ†è§£ã€‚ä¸è¯¾æœ¬ç›¸åŒï¼Œä¸å†èµ˜è¿°ã€‚</p><h4 id="Singular-value-decomposition-SVD"><a href="#Singular-value-decomposition-SVD" class="headerlink" title="Singular value decomposition(SVD)"></a>Singular value decomposition(SVD)</h4><p>åˆç§°å¥‡å¼‚å€¼åˆ†è§£ã€‚å¯¹çŸ©é˜µçš„å½¢çŠ¶æ²¡æœ‰è¦æ±‚ï¼Œä¸”ä»»æ„å®çŸ©é˜µéƒ½å¯ä»¥åšå¥‡å¼‚å€¼åˆ†è§£ï¼Œå› è€Œæ¯”ç‰¹å¾å€¼åˆ†è§£æ›´å…·é€šç”¨æ€§ã€‚</p><p>å¯¹äºä¸€ä¸ª<code>m*n</code>çš„çŸ©é˜µAï¼Œå¥‡å¼‚å€¼åˆ†è§£å¸Œæœ›å°†å…¶åˆ†è§£ä¸ºä¸‰ä¸ªçŸ©é˜µçš„ä¹˜ç§¯ï¼š</p><script type="math/tex; mode=display">A=UDV^T</script><ul><li>Uæ˜¯å¤§å°<code>m*m</code>çš„æ­£äº¤çŸ©é˜µ</li><li>Dæ˜¯å¤§å°<code>m*n</code>çš„å¯¹è§’çŸ©é˜µ</li><li>Væ˜¯å¤§å°<code>n*n</code>çš„æ­£äº¤çŸ©é˜µ</li></ul><p>Dä¸­å¯¹è§’çº¿çš„å…ƒç´ ç§°ä¸ºAçš„å¥‡å¼‚å€¼ã€‚Uçš„åˆ—å‘é‡ç§°ä¸ºAçš„<strong>å·¦</strong>å¥‡å¼‚å‘é‡ï¼ŒVçš„åˆ—å‘é‡ç§°ä¸ºAçš„<strong>å³</strong>å¥‡å¼‚å‘é‡</p><p>Açš„å¥‡å¼‚å€¼åˆ†è§£å¯ä»¥ç†è§£ä¸ºã€ŒAçš„å‡½æ•°ã€çš„ç‰¹å¾å€¼åˆ†è§£ï¼š</p><ul><li>Açš„å·¦å¥‡å¼‚å€¼å‘é‡æ˜¯AA^Tçš„ç‰¹å¾å‘é‡ã€‚</li><li>Açš„å³å¥‡å¼‚å€¼å‘é‡æ˜¯A^TAçš„ç‰¹å¾å‘é‡</li><li>Açš„éé›¶å¥‡å¼‚å€¼æ˜¯A^TAçš„ç‰¹å¾å€¼ï¼Œä¹Ÿæ˜¯AA^Tçš„ç‰¹å¾å€¼</li></ul><p>SVDçš„ä¸€ä¸ªåº”ç”¨ï¼šéƒ¨åˆ†åœºæ™¯ä¸‹å°†çŸ©é˜µæ±‚é€†æ¨å¹¿åˆ°éæ–¹é˜µï¼Œå¦‚ä¸‹</p><h3 id="The-Moore-Penrose-Pseudoinverse"><a href="#The-Moore-Penrose-Pseudoinverse" class="headerlink" title="The Moore-Penrose Pseudoinverse"></a>The Moore-Penrose Pseudoinverse</h3><p>åˆç§°æ‘©å°”ï¼å½­è‹¥æ–¯å¹¿ä¹‰é€†ï¼ˆå¥½é•¿çš„åå­—â€¦ï¼‰</p><script type="math/tex; mode=display">A^+=\lim_{\alpha \searrow 0}(A^TA+\alpha I)^{-1}A^{T}</script><p>å®é™…ä¸Šï¼Œä¸€èˆ¬ç”¨</p><script type="math/tex; mode=display">A^+=VD^+U^T</script><p>æ¥è®¡ç®—å¹¿ä¹‰é€†</p><p>Uï¼ŒDï¼ŒVæ˜¯Açš„å¥‡å¼‚å€¼åˆ†è§£ã€‚Dçš„å¹¿ä¹‰é€†D+ç”±éé›¶å…ƒç´ å–å€’æ•°ï¼Œç„¶åè½¬ç½®å¾—åˆ°ã€‚</p><p>å½“Aæ˜¯Ax=yçš„ç³»æ•°çŸ©é˜µæ—¶ï¼Œè‹¥xæœ‰è§£ï¼Œåˆ™x=A+yæ˜¯æ‰€æœ‰è§£ä¸­å…·æœ‰æœ€å°L2èŒƒå¼çš„é‚£ä¸€ä¸ªã€‚</p><p>è‹¥xæ— è§£ï¼ŒAxç»™å‡ºäº†L2èŒƒå¼ä¸­y-Axçš„æœ€å°å€¼</p><h3 id="Trace"><a href="#Trace" class="headerlink" title="Trace"></a>Trace</h3><p>åˆç§°è¿¹ï¼Œæ˜¯çŸ©é˜µä¸»å¯¹è§’çº¿å…ƒç´ çš„å’Œã€‚</p><script type="math/tex; mode=display">\mathrm{Tr}(A)=\sum_i{A_{i,i}}</script><p>è¿¹å’ŒçŸ©é˜µä¹˜æ³•å¯ä»¥ä»£æ›¿ä¸€äº›éœ€è¦æ±‚å’Œç¬¦å·çš„æ“ä½œï¼š</p><script type="math/tex; mode=display">||A||_F=\sqrt{\mathrm{Tr(AA^T)}}</script><p>çŸ©é˜µçš„è¿¹å…·æœ‰å¾ªç¯ä¸å˜æ€§ï¼š</p><script type="math/tex; mode=display">\mathrm{Tr(ABC)=Tr(CAB)=Tr(BCA)}</script><p>æ›´é€šç”¨æ¥è¯´ï¼Œ</p><script type="math/tex; mode=display">\mathrm{Tr}(\prod_{i=1}^nF^{(i)})=\mathrm{Tr}(F^{(n)}\prod_{i=1}^{n-1}F^{(i)})</script>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML&quot; async&gt;&lt;/script&gt;&lt;/p&gt;
&lt;h2 id=&quot;å‰è¨€&quot;&gt;&lt;a href=&quot;#å‰è¨€&quot; class=&quot;headerlink&quot; title=&quot;å‰è¨€&quot;&gt;&lt;/a&gt;å‰è¨€&lt;/h2&gt;&lt;p&gt;æœ¬æ–‡çš„ä¸»ä½“æ˜¯æœºå™¨å­¦ä¹ ä¸­æ‰€ç”¨åˆ°çš„çº¿æ€§ä»£æ•°çŸ¥è¯†ï¼Œå› æ­¤å¥‰è¡Œ&lt;a href=&quot;https://en.wikipedia.org/wiki/Lazy_evaluation&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Lazy evaluation&lt;/a&gt;ï¼Œå¯¹è¿™äº›çŸ¥è¯†æ›´æ·±å±‚æ¬¡çš„æ¢ç©¶åªåœ¨&lt;strong&gt;ç»å¯¹å¿…è¦&lt;/strong&gt;æ—¶å®Œæˆã€‚&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Machine learning" scheme="https://verrickt.github.io/categories/Machine-learning/"/>
    
    
      <category term="Machine learning" scheme="https://verrickt.github.io/tags/Machine-learning/"/>
    
      <category term="The flower book" scheme="https://verrickt.github.io/tags/The-flower-book/"/>
    
      <category term="Linear algebra" scheme="https://verrickt.github.io/tags/Linear-algebra/"/>
    
  </entry>
  
  <entry>
    <title>HTMLæ¸²æŸ“ä¸ºUWPçš„åŸç”Ÿæ§ä»¶</title>
    <link href="https://verrickt.github.io/2020/04/21/render-html-natively-in-uwp/"/>
    <id>https://verrickt.github.io/2020/04/21/render-html-natively-in-uwp/</id>
    <published>2020-04-21T13:24:32.000Z</published>
    <updated>2021-01-11T12:07:36.824Z</updated>
    
    <content type="html"><![CDATA[<h3 id="æ²¡å•¥ç”¨çš„å‰è¨€"><a href="#æ²¡å•¥ç”¨çš„å‰è¨€" class="headerlink" title="æ²¡å•¥ç”¨çš„å‰è¨€"></a>æ²¡å•¥ç”¨çš„å‰è¨€</h3><p>è¯´ç€å†åšUWPå°±å‰æ‰‹ï¼Œæˆ‘è¿˜æ˜¯å¼€äº†ä¸€ä¸ªæ–°å‘ğŸ¤£<br>è¿™æ¬¡æ˜¯<a href="http://bog.ac" target="_blank" rel="external">Bå²›</a>çš„<a href="https://github.com/Verrickt/BogNMB.UWP" target="_blank" rel="external">UWPç«¯</a><br>è®ºå›å®¢æˆ·ç«¯çš„ä¸€ä¸ªè€å¤§éš¾é—®é¢˜æ˜¯å†…å®¹çš„å‘ˆç°ã€‚è®ºå›ä¸€èˆ¬ä»¥ç½‘é¡µç«¯ä¸ºä¸»ï¼Œç½‘é¡µåšå¥½ï¼Œè®ºå›æ´»è·ƒèµ·æ¥åä¹‹åæ‰ä¼šå¼€å‘å®¢æˆ·ç«¯/æœ‰å¼€å‘è€…æ„¿æ„åšç¬¬ä¸‰æ–¹çš„å®¢æˆ·ç«¯ã€‚å› æ­¤ï¼ŒAPIç»å¤§å¤šæ•°æƒ…å†µæ˜¯ä¸ºç½‘é¡µç«¯ä¸ºä¸€ç­‰å…¬æ°‘çš„ã€‚æ­¤å¤–ï¼Œå„ä¸ªUIæ¡†æ¶å±•ç¤ºå†…å®¹çš„æ ¼å¼ä¹Ÿå„æœ‰ä¸åŒã€‚ä»¥ä¸Šä¸¤ä¸ªåŸå› å¯¼è‡´HTMLè¢«é€‰åšå¯Œæ–‡æœ¬å±•ç¤ºçš„é€šç”¨è¯­è¨€ã€‚</p><a id="more"></a><p>å¯¹äºå®¢æˆ·ç«¯æ¥è¯´ï¼ŒHTMLçš„å‘ˆç°å°±æˆäº†é—®é¢˜ã€‚å¯ä»¥åµŒå…¥æµè§ˆå™¨æ¥æ¸²æŸ“HTMLï¼Œä½†å­˜åœ¨ä¸¤ä¸ªéš¾ä»¥è§£å†³çš„é—®é¢˜</p><ul><li>ä¸åº”ç”¨åŸç”Ÿéƒ¨åˆ†äº¤äº’å›°éš¾ï¼Œ</li><li>å¯èƒ½æœ‰æ€§èƒ½é—®é¢˜ã€‚</li></ul><p>å› æ­¤ï¼Œå®¢æˆ·ç«¯çš„åšæ³•ä¸€èˆ¬æ˜¯ç»•è¿‡WebViewï¼Œå°†HTMLç›´æ¥æ¸²æŸ“ä¸ºåŸç”Ÿæ§ä»¶ã€‚é‚£ä¹ˆé—®é¢˜æ¥äº†ï¼Œæ€ä¹ˆåšå‘¢ï¼ŸAndroidçš„TextViewå¯ä»¥æ¸²æŸ“éƒ¨åˆ†HTMLï¼Œä½†UWPé‡Œå°±æ²¡æœ‰ç›¸åº”çš„APIäº†ã€‚//@å¾®è½¯ï¼Œå‡ºæ¥æŒ¨æ‰“</p><p>å…ˆæ¥çœ‹çœ‹æ‰‹ä¸Šæœ‰ä»€ä¹ˆå·¥å…·ã€‚é¦–å…ˆæ˜¯åŸç”Ÿçš„XAMLæ§ä»¶ã€‚Windows SDK 1903ä¸.Net starndard 2.0å…¼å®¹ã€‚åˆæœ‰ä¸€å¤§å †.NET Standard 2.0çš„ç±»åº“å¯ä»¥ç”¨äº†ã€‚  </p><p>APIè¿”å›çš„ç»“æœæ˜¯<br><figure class="highlight html"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">æŠ¥ä¸ªBUGã€‚ä¸»å²›çš„API pæ¨¡å¼çš„data2ï¼Œæ–‡æ¡£è¯´å°äº1æŒ‰1å¤„ç†ï¼Œå®é™…ä¸Šå–0æ—¶è¿”å›çš„æ˜¯'['ï¼Œ<span class="tag">&lt;<span class="name">br</span> /&gt;</span><span class="tag">&lt;<span class="name">br</span> /&gt;</span>curl  http://bog.ac/api/p/0/0<span class="tag">&lt;<span class="name">br</span> /&gt;</span>]</div></pre></td></tr></table></figure></p><h3 id="å°è¯•æœç´¢"><a href="#å°è¯•æœç´¢" class="headerlink" title="å°è¯•æœç´¢"></a>å°è¯•æœç´¢</h3><p>æœç´¢ä¸€ä¸‹ï¼Œæ‰¾åˆ°äº†<a href="https://github.com/VincentH-Net/WinRT-RichTextBlock.Html2Xaml" target="_blank" rel="external">WinRT-RichTextBlock.Html2Xaml</a>ï¼Œå®ƒèƒ½æŠŠHTMLæ¸²æŸ“åˆ°<code>RichTextBlock</code>ä¸Šï¼Œä½†æ˜¯å¾ˆé—æ†¾ï¼Œå®ƒä¸æ”¯æŒUWPã€‚ç»§ç»­æœç´¢ï¼Œæ‰¾åˆ°HTML2XAMLçš„ä¸€ä¸ªæ”¯æŒUWPçš„<a href="https://github.com/XeonKHJ/RichTextBlock.Html2Xaml" target="_blank" rel="external">fork</a>ï¼Œä½¿ç”¨åå‘ç°æœ‰ä¸æ”¯æŒçš„æ ‡ç­¾ã€‚æŸ¥çœ‹ä»–çš„ä»£ç ï¼Œä¼¼ä¹ç”¨åˆ°äº†<code>xslt</code>ã€‚é¢å¯¹HTMLå·²ç»å¤Ÿå¤´ç–¼äº†ï¼Œè¿˜æ˜¯åˆ«å¼•å…¥å¦ä¸€ä¸ªæ ‡è®°è¯­è¨€äº†ã€‚å‡ºå¸ˆä¸åˆ©ã€‚</p><p>ç»§ç»­æœç´¢ï¼Œæ‰¾åˆ°äº†ä¸€ä¸ª<a href="https://docs.microsoft.com/en-us/windows/communitytoolkit/controls/markdowntextblock" target="_blank" rel="external">MarkdownTextBlock</a>çš„åº“ã€‚æˆ‘ä»¥å‰åšå¦ä¸€ä¸ªè®ºå›çš„<a href="https://github.com/Verrickt/COLG-UWP" target="_blank" rel="external">å®¢æˆ·ç«¯</a>æ—¶ç”¨è¿‡å®ƒã€‚å½“æ—¶æ˜¯å…ˆæƒ³åŠæ³•æŠŠ<a href="https://github.com/Verrickt/COLG-UWP/blob/84a008d11518c1d7074be3c942e26e686e8cef5f/Colg%20UWP/Service/Html2Markdown.cs" target="_blank" rel="external">HTMLè½¬æˆMarkdown</a>ï¼Œå†ç”¨å®ƒæ¥æ¸²æŸ“ã€‚ä½†æ˜¯åœ¨å¤„ç†å¤šçº§åµŒå¥—å¼•ç”¨(<code>&lt;quote&gt;</code>)çš„æ—¶å€™ä¼šå‡ºé”™ã€‚<del>å†µä¸”æ—¶éš”è¿™ä¹ˆä¹…æˆ‘å·²ç»çœ‹ä¸æ‡‚å½“å¹´å†™çš„ä»£ç äº†ğŸ¤£</del></p><p>é€šè¿‡è¿™ä¸¤æ¬¡æœç´¢æˆ‘ä»¬å¾—åˆ°äº†ä»¥ä¸‹ä¿¡æ¯ï¼š</p><ol><li><code>RichTextBlock</code>å¾ˆå¯èƒ½èƒ½å¤Ÿä½œä¸ºæˆ‘ä»¬æ¸²æŸ“çš„å®¹å™¨</li><li>HTMLæ ‡ç­¾å’Œä½¿ç”¨çš„åŸç”Ÿæ§ä»¶æœ‰å…³</li><li>ç»“æ„åŒ–çš„è¾“å‡ºå¤„ç†èµ·æ¥æ›´æ–¹ä¾¿ï¼Œå¦‚æœèƒ½æŠŠHTMLè½¬åŒ–ä¸ºDOMæ ‘ï¼Œé dfså°±å¯ä»¥å®ç°è½¬æ¢ã€‚</li></ol><p>ç¬¬ä¸€æ­¥ï¼Œå…ˆè¦æŠŠHTMLç»“æ„åŒ–ã€‚</p><h3 id="ç»“æ„åŒ–HTML"><a href="#ç»“æ„åŒ–HTML" class="headerlink" title="ç»“æ„åŒ–HTML"></a>ç»“æ„åŒ–HTML</h3><p>è¦æŠŠæŸç§è¯­è¨€ç»“æ„åŒ–ï¼ŒParseræ˜¯ä¸äºŒé€‰æ‹©ã€‚è€ŒHTMLçš„Parserå› ä¸ºç»å¸¸é¢å¯¹æ®‹ç¼ºçš„HTMLï¼Œé€šå¸¸æ”¯æŒå°†æ®‹ç¼ºçš„ç‰‡æ®µè¡¥é½ã€‚<a href="https://github.com/AngleSharp/AngleSharp" target="_blank" rel="external">AngleSharp</a>å°±æ˜¯ä¸€ä¸ªåŸºäº.NET Standardçš„HTML parserã€‚<br>ç¬¬ä¸€æ­¥æå®šã€‚æœ‰äº†ç»“æ„ï¼Œæ¥ä¸‹æ¥å°±é¡ºæ‰‹å¤šäº†ã€‚</p><h3 id="éå†DOMæ ‘"><a href="#éå†DOMæ ‘" class="headerlink" title="éå†DOMæ ‘"></a>éå†DOMæ ‘</h3><p>ä¸Šé¢æŠŠHTMLè½¬æˆMarkdownçš„æºå‡½æ•°ï¼Œé‡Œé¢çš„ä¸€å¤§å †åˆ†æ”¯çœ‹çš„äº‘é‡Œé›¾é‡Œã€‚åŠ ä¸Šå¥‡å¥‡æ€ªæ€ªçš„è¾¹ç•Œæƒ…å†µåæ›´æ˜¯è®©äººå¤´ç–¼ã€‚æœ‰æ²¡æœ‰ä¸€ç§ä»£ç çš„ç»„ç»‡æ–¹æ³•èƒ½è®©æˆ‘é’ˆå¯¹ä¸€ä¸ªæ ‡ç­¾å†™ä¸€ä¸ªå‡½æ•°ï¼Ÿç­”æ¡ˆæ˜¯ï¼š<a href="https://en.wikipedia.org/wiki/Visitor_pattern" target="_blank" rel="external">Visitor pattern</a>. ç™¾ç§‘ä¸Šå†™çš„è¯¦ç»†çš„å¤šï¼Œæˆ‘å°±åªä¸¾ä¸€ä¸ªä¾‹å­ã€‚å‡è®¾æœ‰<code>&lt;p&gt;</code>,<code>&lt;img&gt;</code>,<code>&lt;a&gt;</code>æ ‡ç­¾éœ€è¦è§£æã€‚å®šä¹‰<code>INode</code>ä½œä¸ºæ‰€æœ‰DOMå…ƒç´ çš„æ¥å£,<code>IVisitor</code>æ˜¯è¦å¯¹å…ƒç´ è®¿é—®çš„æ¥å£ã€‚<code>INode</code>çš„å®ç°è€…é€šè¿‡<code>visitor.Visit(this)</code>æŠŠæ§åˆ¶æµè¿”è¿˜ç»™<code>Visitor</code>ã€‚åªéœ€åœ¨<code>visitor</code>ä¸Šå®ç°å¯¹å„ä¸ªç±»çš„<code>Visit</code>æ–¹æ³•ï¼Œå°±è¾¾æˆç›®çš„ã€‚<br><figure class="highlight csharp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">interface</span> <span class="title">INode</span></div><div class="line">&#123;</div><div class="line">    <span class="function"><span class="keyword">void</span> <span class="title">Accept</span>(<span class="params">IVisitor visitor</span>)</span>;</div><div class="line">&#125;</div><div class="line"><span class="keyword">interface</span> <span class="title">IVisitor</span></div><div class="line">&#123;</div><div class="line">    <span class="function"><span class="keyword">void</span> <span class="title">Visit</span>(<span class="params">ANode node</span>)</span>;</div><div class="line">    <span class="function"><span class="keyword">void</span> <span class="title">Visit</span>(<span class="params">PNode node</span>)</span>;</div><div class="line">    <span class="function"><span class="keyword">void</span> <span class="title">Visit</span>(<span class="params">ImgNode node</span>)</span>;</div><div class="line">&#125;</div><div class="line"><span class="keyword">class</span> <span class="title">PNode</span>:<span class="title">INode</span></div><div class="line">&#123;</div><div class="line">    <span class="keyword">public</span> IReadonlyList&lt;INode&gt; Children&#123;<span class="keyword">get</span>;&#125;</div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">Accept</span>(<span class="params">IVisitor visitor</span>)</span></div><div class="line"><span class="function">    </span>&#123;</div><div class="line">        visitor.Visit(<span class="keyword">this</span>);</div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"><span class="keyword">class</span> <span class="title">ANode</span>:<span class="title">INode</span></div><div class="line">&#123;</div><div class="line">    <span class="keyword">public</span> <span class="keyword">string</span> Href&#123;<span class="keyword">get</span>;&#125;</div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">Accept</span>(<span class="params">IVisitor visitor</span>)</span></div><div class="line"><span class="function">    </span>&#123;</div><div class="line">        visitor.Visit(<span class="keyword">this</span>);</div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"><span class="keyword">class</span> <span class="title">ImgNode</span>:<span class="title">INode</span></div><div class="line">&#123;</div><div class="line">    <span class="keyword">public</span> <span class="keyword">string</span> src&#123;<span class="keyword">get</span>;&#125;</div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">Accept</span>(<span class="params">IVisitor visitor</span>)</span></div><div class="line"><span class="function">    </span>&#123;</div><div class="line">        visitor.Visit(<span class="keyword">this</span>);</div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title">DOMVisitor</span>:<span class="title">IVisitor</span></div><div class="line">&#123;</div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">Visit</span>(<span class="params">PNode node</span>)</span></div><div class="line"><span class="function">    </span>&#123;</div><div class="line">        <span class="keyword">foreach</span>(<span class="keyword">var</span> item <span class="keyword">in</span> node.Children)</div><div class="line">            item.Accept(<span class="keyword">this</span>);</div><div class="line">    &#125;</div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">Visit</span>(<span class="params">ANode node</span>)</span></div><div class="line"><span class="function">    </span>&#123;</div><div class="line">        <span class="comment">//new Hyper link</span></div><div class="line">    &#125;</div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">Visit</span>(<span class="params">ImgNode node</span>)</span></div><div class="line"><span class="function">    </span>&#123;</div><div class="line">        <span class="comment">//new Image</span></div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p><h3 id="RichTextBlock"><a href="#RichTextBlock" class="headerlink" title="RichTextBlock"></a>RichTextBlock</h3><p>æ¥ä¸‹æ¥è€ƒè™‘å¦‚ä½•å‘ˆç°ã€‚æ ¹æ®<a href="https://docs.microsoft.com/en-us/uwp/api/Windows.UI.Xaml.Controls.RichTextBlock" target="_blank" rel="external">æ–‡æ¡£</a>ï¼Œ<code>RichTextBlock</code>å¯ä»¥åŒ…å«å¤šä¸ª<code>Block</code>ï¼Œä¸€ä¸ª<code>Block</code>åˆå¯ä»¥åŒ…å«è‹¥å¹²<code>Inline</code>ã€‚ä¸HTMLæ ‡ç­¾åˆšå¥½å¯¹åº”ï¼<code>InlineUIContainer</code>è‡ªå·±æ˜¯<code>Inline</code>ï¼Œä½†<code>Child</code>å±æ€§å¯ä»¥å¡ä¸‹ä»»ä½•<code>UIElement</code>ã€‚å¦‚æœå¡è¿›å»å¦ä¸€ä¸ª<code>RichTextBlock</code>å°±å®ç°äº†å¯¹å¼•ç”¨<code>&lt;quote&gt;</code>å‘ˆç°ã€‚è¿™æ ·å¯ä»¥å®ç°ä»»æ„çº§å¼•ç”¨<code>&lt;quote&gt;</code>çš„å‘ˆç°ã€‚å…·ä½“å®ç°ä¸Šï¼Œæä¾›ä¸€ä¸ª<code>Stack&lt;Block&gt;</code>ä¾›ä½¿ç”¨ã€‚è½¬åŒ–ä¸º<code>Inline</code>çš„å…ƒç´ æ¯æ¬¡æ·»åŠ åˆ°æ ˆé¡¶çš„<code>Block</code>ä¸­ã€‚é‡åˆ°è½¬åŒ–ä¸º<code>Block</code>çš„å…ƒç´ åˆ™å‹æ ˆã€‚æœ€åæŠŠ<code>Block</code>æŒ‰ç…§å…ˆåé¡ºåºåŠ å…¥<code>RichTextBlock</code>å³å¯ã€‚</p><p>ä¸‹é¢æ˜¯æ•ˆæœï¼Œå…·ä½“ä»£ç è¯·å‚è€ƒ<a href="https://github.com/Verrickt/BogNMB.UWP/tree/develop/HTMLParser" target="_blank" rel="external">HTML Parser</a>å’Œ<a href="https://github.com/Verrickt/BogNMB.UWP/blob/develop/BogNMB.UWP/CustomControl/RichTextBlockRenderer.cs" target="_blank" rel="external">RichTextBlockRenderer</a></p><p><img src="https://user-images.githubusercontent.com/11483783/79775074-48d45800-8366-11ea-839d-83283cbb3b4a.jpg" alt="picture"></p><p>æå®šäº†ä¸€ä¸ªå›°æ‰°å¤šå¹´çš„éš¾é¢˜ï¼Œå¯å–œå¯è´º(ï¼¾oï¼¾)ï¾‰</p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;æ²¡å•¥ç”¨çš„å‰è¨€&quot;&gt;&lt;a href=&quot;#æ²¡å•¥ç”¨çš„å‰è¨€&quot; class=&quot;headerlink&quot; title=&quot;æ²¡å•¥ç”¨çš„å‰è¨€&quot;&gt;&lt;/a&gt;æ²¡å•¥ç”¨çš„å‰è¨€&lt;/h3&gt;&lt;p&gt;è¯´ç€å†åšUWPå°±å‰æ‰‹ï¼Œæˆ‘è¿˜æ˜¯å¼€äº†ä¸€ä¸ªæ–°å‘ğŸ¤£&lt;br&gt;è¿™æ¬¡æ˜¯&lt;a href=&quot;http://bog.ac&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Bå²›&lt;/a&gt;çš„&lt;a href=&quot;https://github.com/Verrickt/BogNMB.UWP&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;UWPç«¯&lt;/a&gt;&lt;br&gt;è®ºå›å®¢æˆ·ç«¯çš„ä¸€ä¸ªè€å¤§éš¾é—®é¢˜æ˜¯å†…å®¹çš„å‘ˆç°ã€‚è®ºå›ä¸€èˆ¬ä»¥ç½‘é¡µç«¯ä¸ºä¸»ï¼Œç½‘é¡µåšå¥½ï¼Œè®ºå›æ´»è·ƒèµ·æ¥åä¹‹åæ‰ä¼šå¼€å‘å®¢æˆ·ç«¯/æœ‰å¼€å‘è€…æ„¿æ„åšç¬¬ä¸‰æ–¹çš„å®¢æˆ·ç«¯ã€‚å› æ­¤ï¼ŒAPIç»å¤§å¤šæ•°æƒ…å†µæ˜¯ä¸ºç½‘é¡µç«¯ä¸ºä¸€ç­‰å…¬æ°‘çš„ã€‚æ­¤å¤–ï¼Œå„ä¸ªUIæ¡†æ¶å±•ç¤ºå†…å®¹çš„æ ¼å¼ä¹Ÿå„æœ‰ä¸åŒã€‚ä»¥ä¸Šä¸¤ä¸ªåŸå› å¯¼è‡´HTMLè¢«é€‰åšå¯Œæ–‡æœ¬å±•ç¤ºçš„é€šç”¨è¯­è¨€ã€‚&lt;/p&gt;
    
    </summary>
    
    
      <category term="UWP" scheme="https://verrickt.github.io/tags/UWP/"/>
    
      <category term="HTML" scheme="https://verrickt.github.io/tags/HTML/"/>
    
  </entry>
  
  <entry>
    <title>PAT 1097 Deduplication on a Linked List</title>
    <link href="https://verrickt.github.io/2020/04/03/PAT-1097-Deduplication-on-a-Linked-List/"/>
    <id>https://verrickt.github.io/2020/04/03/PAT-1097-Deduplication-on-a-Linked-List/</id>
    <published>2020-04-03T05:31:51.000Z</published>
    <updated>2021-01-11T12:07:36.803Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Deduplication-on-a-Linked-List"><a href="#Deduplication-on-a-Linked-List" class="headerlink" title="Deduplication on a Linked List"></a>Deduplication on a Linked List</h2><p>Given a singly linked list L with integer keys, you are supposed to remove the nodes with duplicated absolute values of the keys. That is, for each value K, only the first node of which the value or absolute value of its key equals K will be kept. At the mean time, all the removed nodes must be kept in a separate list. For example, given L being <code>21â†’-15â†’-15â†’-7â†’15</code>, you must output <code>21â†’-15â†’-7</code>, and the removed list <code>-15â†’15</code>.</p><a id="more"></a><p><strong>Input Specification</strong></p><p>Each input file contains one test case. For each case, the first line contains the address of the first node, and a positive N (â‰¤10^â€‹5â€‹â€‹) which is the total number of nodes. The address of a node is a 5-digit nonnegative integer, and NULL is represented by âˆ’1.</p><p>Then N lines follow, each describes a node in the format:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">Address Key Next</div></pre></td></tr></table></figure></p><p>where Address is the position of the node, Key is an integer of which absolute value is no more than 10â€‹^4â€‹â€‹, and Next is the position of the next node.</p><p><strong>Output Specification</strong></p><p>For each case, output the resulting linked list first, then the removed list. Each node occupies a line, and is printed in the same format as in the input.</p><p><strong>Sample Input</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">00100 5</div><div class="line">99999 -7 87654</div><div class="line">23854 -15 00000</div><div class="line">87654 15 -1</div><div class="line">00000 -15 99999</div><div class="line">00100 21 23854</div></pre></td></tr></table></figure></p><p><strong>Sample Output</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">00100 21 23854</div><div class="line">23854 -15 99999</div><div class="line">99999 -7 -1</div><div class="line">00000 -15 87654</div><div class="line">87654 15 -1</div></pre></td></tr></table></figure></p><hr><p>ç±»ä¼¼<a href="https://www.liuchuo.net/archives/2118" target="_blank" rel="external">è¿™æ ·</a>çš„éªšæ“ä½œç½‘ä¸Šæœ‰å¾ˆå¤šï¼Œè¿™é‡Œä¸åœ¨å¼•ç”¨ã€‚æœ¬æ–‡ä¸»è¦è€ƒè™‘å¸¸è§„æ€è·¯ã€‚æ€è·¯éå¸¸ç®€å•ï¼Œä¾¿åˆ©é“¾è¡¨ï¼Œå¦‚æœæ›¾ç»å‡ºç°è¿‡å°±æ’å…¥å¦ä¸€ä¸ªé“¾è¡¨ï¼Œæ²¡å‡ºç°è¿‡å°±ç»§ç»­ï¼Œæœ€åå¾—åˆ°ä¸¤ä¸ªé“¾è¡¨ï¼Œåˆ†åˆ«æ˜¯å»é‡åçš„å’Œé‡å¤çš„ã€‚çœ‹ä¼¼æ¯”è¾ƒç®€å•ï¼Œæƒ³æŠŠæ‰€æœ‰æƒ…å†µè€ƒè™‘å…¨ä¹Ÿæ˜¯è¦èŠ±ç‚¹å¿ƒæ€çš„ã€‚</p><ul><li>é‡å¤éƒ¨åˆ†çš„é“¾è¡¨æ˜¯å’ŒåŸé“¾è¡¨é¡ºåºæ˜¯ä¸€æ ·çš„ï¼Œé‡‡ç”¨å°¾æ’æ³•ã€‚éœ€è¦å¤´ã€å°¾ä¸¤ä¸ªæŒ‡é’ˆã€‚</li><li>åŸé“¾è¡¨çš„å¤´èŠ‚ç‚¹æ˜¯ç¬¬ä¸€ä¸ªèŠ‚ç‚¹ï¼Œä¸å¯èƒ½è¢«æ”¾å…¥é‡å¤é“¾è¡¨ï¼Œé‚£ä¹ˆå»é‡åçš„é“¾è¡¨å¤´èŠ‚ç‚¹ä¸å˜ã€‚</li><li>éå†è¿‡ç¨‹ä¸­éœ€è¦å°†èŠ‚ç‚¹åˆ é™¤(å³æ’å…¥é‡å¤é“¾è¡¨)ï¼Œæ‰€ä»¥éœ€è¦<code>prev</code>æŒ‡é’ˆè®°å½•å»é‡é“¾è¡¨çš„ä¸Šä¸€ä¸ªèŠ‚ç‚¹åœ°å€ï¼Œä»¥ä¾¿ä¿®æ”¹<code>next</code>æŒ‡é’ˆã€‚</li></ul><p>å¦‚æœä½ åƒæˆ‘ä¸€æ ·ä»¥ä¸ºä¸Šé¢çš„æè¿°å°±æŠŠæ‰€æœ‰çš„æƒ…å†µéƒ½è€ƒè™‘å®Œå…¨äº†ï¼Œé‚£ä¹ˆæ¬¢è¿ä½ å’Œæˆ‘æ¥åˆ°BUGçš„æµ·æ´‹ğŸ˜‰</p><p>å…ˆæ¥çœ‹ACçš„ä»£ç </p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"vector"</span></span></div><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"iostream"</span></span></div><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"algorithm"</span></span></div><div class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</div><div class="line"><span class="class"><span class="keyword">struct</span> <span class="title">node</span></span></div><div class="line"><span class="class">&#123;</span></div><div class="line"><span class="keyword">int</span> addr;</div><div class="line"><span class="keyword">int</span> key; </div><div class="line"><span class="keyword">int</span> next;</div><div class="line">&#125;;</div><div class="line">node nodes[<span class="number">100086</span>];</div><div class="line"><span class="keyword">bool</span> <span class="built_in">map</span>[<span class="number">10024</span>];</div><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">print</span><span class="params">(<span class="keyword">int</span> head)</span></span></div><div class="line"><span class="function"></span>&#123;</div><div class="line"><span class="keyword">auto</span> cur = head;</div><div class="line"><span class="keyword">while</span> (cur != <span class="number">-1</span>)</div><div class="line">&#123;</div><div class="line"><span class="keyword">if</span> (nodes[cur].next != <span class="number">-1</span>)</div><div class="line"><span class="built_in">printf</span>(<span class="string">"%05d %d %05d\n"</span>, cur, nodes[cur].key, nodes[cur].next);</div><div class="line"><span class="keyword">else</span> <span class="built_in">printf</span>(<span class="string">"%05d %d -1\n"</span>, cur, nodes[cur].key);</div><div class="line">cur = nodes[cur].next;</div><div class="line">&#125;</div><div class="line">&#125;</div><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></div><div class="line"><span class="function"></span>&#123;</div><div class="line"><span class="keyword">int</span> head, n;</div><div class="line"><span class="built_in">cin</span> &gt;&gt; head &gt;&gt; n;</div><div class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n; i++)</div><div class="line">&#123;</div><div class="line"><span class="keyword">int</span> id;</div><div class="line"><span class="built_in">cin</span> &gt;&gt; id;</div><div class="line">nodes[id].addr = id;</div><div class="line"><span class="built_in">cin</span> &gt;&gt; nodes[id].key &gt;&gt; nodes[id].next;</div><div class="line">&#125;</div><div class="line">fill(<span class="built_in">map</span>, <span class="built_in">map</span> + <span class="number">10024</span>, <span class="literal">false</span>);</div><div class="line"><span class="keyword">int</span> dupli_head = <span class="number">-1</span>;</div><div class="line"><span class="keyword">int</span> dupli_tail = <span class="number">-1</span>;</div><div class="line"><span class="keyword">int</span> cur = head;</div><div class="line"><span class="keyword">int</span> prev = <span class="number">-1</span>;</div><div class="line"><span class="keyword">while</span> (cur != <span class="number">-1</span>)</div><div class="line">&#123;</div><div class="line"><span class="keyword">while</span> (<span class="built_in">map</span>[<span class="built_in">abs</span>(nodes[cur].key)])</div><div class="line">&#123;</div><div class="line"><span class="keyword">int</span> next = nodes[cur].next;</div><div class="line"><span class="keyword">if</span> (dupli_head == <span class="number">-1</span>) dupli_head = dupli_tail = cur;</div><div class="line"><span class="keyword">else</span></div><div class="line">&#123;</div><div class="line">nodes[dupli_tail].next = cur;</div><div class="line">dupli_tail = cur;</div><div class="line">nodes[cur].next = <span class="number">-1</span>;</div><div class="line">&#125;</div><div class="line">cur = next;</div><div class="line"><span class="keyword">if</span> (cur == <span class="number">-1</span>) <span class="keyword">break</span>;</div><div class="line">&#125;</div><div class="line"><span class="keyword">if</span> (cur == <span class="number">-1</span>) &#123; nodes[prev].next = <span class="number">-1</span>; <span class="keyword">break</span>; &#125;</div><div class="line"><span class="keyword">if</span> (prev != <span class="number">-1</span>) nodes[prev].next = cur;</div><div class="line"><span class="built_in">map</span>[<span class="built_in">abs</span>(nodes[cur].key)] = <span class="literal">true</span>;</div><div class="line">prev = cur;</div><div class="line">cur = nodes[cur].next;</div><div class="line">&#125;</div><div class="line">print(head);</div><div class="line">print(dupli_head);</div><div class="line"><span class="keyword">return</span> <span class="number">0</span>;</div><div class="line">&#125;</div></pre></td></tr></table></figure><blockquote><p>ä¸ºå•¥<code>while</code>å¾ªç¯é‡Œåˆå¥—äº†ä¸ª<code>while</code>å¾ªç¯ï¼Œç”¨<code>if</code>ä¸å°±å¤Ÿäº†ä¹ˆï¼Ÿ</p></blockquote><p>æ­¤ä¹ƒç¬¬ä¸€å‘ğŸ˜‰<br>ä¸Šæ–‡è¯´åˆ°</p><blockquote><p><code>prev</code>æŒ‡é’ˆè®°å½•å»é‡é“¾è¡¨çš„ä¸Šä¸€ä¸ªèŠ‚ç‚¹åœ°å€</p></blockquote><p>é‚£ä¹ˆ<code>prev</code>çš„<code>next</code>åº”è¯¥æ˜¯<strong>å»é‡é“¾è¡¨</strong>çš„ä¸‹ä¸€ä¸ªåœ°å€ã€‚è€Œ<code>if</code>å¾—åˆ°çš„ä»…ä»…æ˜¯ä¸‹ä¸€ä¸ªåœ°å€ï¼Œæ­¤æ—¶è¿˜ä¸çŸ¥é“ä»–æ˜¯å¦å±äº<strong>å»é‡é“¾è¡¨</strong>ã€‚è¿™æ—¶ä¸èƒ½åš<code>prev.next=cur</code>ã€‚é‚£ä»€ä¹ˆæ—¶å€™å¯ä»¥å‘¢ï¼Ÿå½“ç„¶æ˜¯<code>map[abs(nodes[cur].key)</code>ä¸ºå‡å•¦ã€‚</p><blockquote><p>å¥½å¤š<code>-1</code>çœ‹çš„æˆ‘å¤´æ™•</p></blockquote><p>æ­å–œä½ ï¼Œè¿›å…¥äº†ç¬¬äºŒå‘ğŸ˜‰</p><ol><li><p><code>dupli_head</code>å’Œ<code>dupli_tail</code>æ˜¯å¤´å°¾æŒ‡é’ˆï¼Œ<code>-1</code>ä½œä¸ºé“¾è¡¨ä¸ºç©ºæ—¶çš„ç‰¹æ®Šå€¼ã€‚</p></li><li><p>æ’å…¥<code>dupli</code>é“¾è¡¨åï¼Œè¦æŠŠ<code>cur.next</code>ç½®ä¸º<code>-1</code>ï¼Œè¡¨ç¤ºè¿™æ˜¯æ–°é“¾è¡¨çš„æœ€åä¸€ä¸ªèŠ‚ç‚¹ã€‚ä¸ºé˜²æ­¢é“¾è¡¨æ–­è£‚ï¼Œç”¨<code>next=cur.next</code>è®°ä½ä¸‹ä¸€ä¸ªåœ°å€ã€‚</p></li><li><p>æ’å…¥é‡å¤é“¾è¡¨æ˜¯åœ¨<code>while</code>å¾ªç¯é‡Œè¿›è¡Œçš„ï¼Œé‚£ä¹ˆå°±æœ‰å¯èƒ½åœ¨è¿™é‡Œè€—å…‰æ‰€æœ‰çš„å…ƒç´ ã€‚æ­¤å³<code>if (cur == -1) break;</code>ã€‚</p></li><li><p>å½“åœ¨<code>while</code>å¾ªç¯é‡Œè€—å°½æ‰€æœ‰å…ƒç´ åï¼Œå·²ç»ä¸å¯èƒ½æœ‰æ–°çš„å…ƒç´ å¯ä»¥ä½œä¸º<code>prev.next</code>ï¼Œå› æ­¤<code>if (cur == -1) { nodes[prev].next = -1; break; }</code></p></li><li>ä¸Šæ–‡è¯´è¿‡ï¼ŒåŸé“¾è¡¨çš„å¤´èŠ‚ç‚¹ä¸€å®šæ˜¯å»é‡é“¾è¡¨çš„å¤´èŠ‚ç‚¹ã€‚é‚£ä¹ˆå½“åˆšè®¿é—®è¿‡å¤´èŠ‚ç‚¹æ—¶ï¼Œæ˜¯ä¸éœ€è¦å¯¹<code>prev</code>æ“ä½œçš„ã€‚å³<code>if (prev != -1) nodes[prev].next = cur;</code>ã€‚å½“ç„¶ï¼Œä½ ä¹Ÿå¯ä»¥ç›´æ¥è·³è¿‡å¤´èŠ‚ç‚¹<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">map</span>[head]=<span class="literal">true</span>;</div><div class="line">cur = nodes[head].next;</div><div class="line">prev = head;</div></pre></td></tr></table></figure></li></ol><p>æ€ä¹ˆæ ·ï¼Œè¿˜è§‰å¾—è€ƒè™‘æ‰€æœ‰æƒ…å†µç®€å•ä¹ˆğŸ˜‰</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Deduplication-on-a-Linked-List&quot;&gt;&lt;a href=&quot;#Deduplication-on-a-Linked-List&quot; class=&quot;headerlink&quot; title=&quot;Deduplication on a Linked List&quot;&gt;&lt;/a&gt;Deduplication on a Linked List&lt;/h2&gt;&lt;p&gt;Given a singly linked list L with integer keys, you are supposed to remove the nodes with duplicated absolute values of the keys. That is, for each value K, only the first node of which the value or absolute value of its key equals K will be kept. At the mean time, all the removed nodes must be kept in a separate list. For example, given L being &lt;code&gt;21â†’-15â†’-15â†’-7â†’15&lt;/code&gt;, you must output &lt;code&gt;21â†’-15â†’-7&lt;/code&gt;, and the removed list &lt;code&gt;-15â†’15&lt;/code&gt;.&lt;/p&gt;
    
    </summary>
    
    
      <category term="PAT" scheme="https://verrickt.github.io/tags/PAT/"/>
    
  </entry>
  
  <entry>
    <title>PAT 1021 Deepest Root</title>
    <link href="https://verrickt.github.io/2020/03/23/PAT-1021-Deepest-Root/"/>
    <id>https://verrickt.github.io/2020/03/23/PAT-1021-Deepest-Root/</id>
    <published>2020-03-23T13:32:29.000Z</published>
    <updated>2021-01-11T12:07:36.774Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Deepest-Root"><a href="#Deepest-Root" class="headerlink" title="Deepest Root"></a>Deepest Root</h2><p>A graph which is connected and acyclic can be considered a tree. The height of the tree depends on the selected root. Now you are supposed to find the root that results in a highest tree. Such a root is called <strong>the deepest root.</strong></p><a id="more"></a><p><strong>Input Specification</strong></p><p>Each input file contains one test case. For each case, the first line contains a positive integer N (â‰¤10â€‹^4â€‹â€‹) which is the number of nodes, and hence the nodes are numbered from 1 to N. Then Nâˆ’1 lines follow, each describes an edge by given the two adjacent nodesâ€™ numbers.</p><p><strong>Output Specification</strong></p><p>For each test case, print each of the deepest roots in a line. If such a root is not unique, print them in increasing order of their numbers. In case that the given graph is not a tree, print Error: K components where K is the number of connected components in the graph.</p><p><strong>Sample Input 1</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">5</div><div class="line">1 2</div><div class="line">1 3</div><div class="line">1 4</div><div class="line">2 5</div></pre></td></tr></table></figure><p><strong>Sample Output 1</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td></tr></table></figure></p><p><strong>Sample Input 2</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">5</div><div class="line">1 3</div><div class="line">1 4</div><div class="line">2 5</div><div class="line">3 4</div></pre></td></tr></table></figure></p><p><strong>Sample Output 2</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">Error: 2 components</div></pre></td></tr></table></figure></p><hr><p>å»å¹´è¿ç­”æ¡ˆéƒ½çœ‹ä¸æ‡‚çš„é¢˜åœ¨ä»Šå¤©å‘æ„£çš„æ—¶å€™çªç„¶æƒ³å‡ºæ¥ï¼ŒçœŸæ˜¯å¯å–œå¯è´ºã€‚</p><p>é¢˜ç›®è¦æ±‚æ˜¯æ±‚æ ‘é«˜ï¼Œä½†å…¶å®æ ‘é«˜å…¶å®å’Œæ±‚è·¯å¾„é•¿åº¦æ˜¯ä¸€è‡´çš„ï¼šåœ¨ç®€å•å›¾é‡Œï¼Œåªè¦å®šäº†èµ·ç‚¹å’Œç»ˆç‚¹ï¼Œè·¯å¾„å°±æ˜¯ä»æ ¹åˆ°å¶èŠ‚ç‚¹çš„è½¨è¿¹ã€‚æ±‚<strong>æŸç‚¹</strong>çš„æœ€å¤§è·¯å¾„é•¿åº¦å’Œå…¨å›¾å¼ºè¿é€šåˆ†é‡å¯ä»¥ç”¨dfsä¸€å¹¶è§£å†³ï¼Œé—®é¢˜è½¬åŒ–ä¸ºæ±‚<strong>å…¨å›¾</strong>çš„æœ€å¤§è·¯å¾„é•¿åº¦</p><p>ç®€å•ç²—æš´çš„ç©·ä¸¾ä¼šè¢«å‡ ä¸ªæµ‹è¯•ç‚¹å¡æ—¶é—´ï¼Œæ‰€ä»¥éœ€è¦ä¸€äº›â€œèªæ˜â€çš„å°åŠæ³•ã€‚ä»”ç»†æ€è€ƒï¼Œç©·ä¸¾è¢«å¡æ—¶é—´çš„åŸå› åº”è¯¥æ˜¯åšäº†å¤§é‡é‡å¤çš„è®¡ç®—ã€‚å¦‚æœèƒ½åŒºåˆ†å“ªäº›è®¡ç®—æ˜¯ä¸å¿…è¦çš„ï¼Œå°±å¯ä»¥è§£å†³é—®é¢˜ã€‚æ²¿ç€è¿™ä¸ªæ€è·¯ï¼Œè€ƒè™‘ä¸¤æ¡æœ€é•¿è·¯å¾„æ˜¯å¦æœ‰å…³ç³»ã€‚è€ƒè™‘<code>å</code>å­—å‹çš„å›¾ï¼Œä¸¤æ¡æœ€é•¿è·¯å¾„ç›¸äº¤äºå›¾ä¸Šä¸€ç‚¹ï¼Œè¿™ä¹ˆçœ‹æ¥æœ€é•¿è·¯å¾„ä¹‹é—´å¾ˆå¯èƒ½æœ‰äº¤ç‚¹ã€‚å°±ä»äº¤ç‚¹å‡ºå‘å§ã€‚</p><p>è€ƒè™‘è¿é€šå›¾çš„æƒ…å†µã€‚è‹¥<code>A</code>ä¸ºå›¾ä¸Šä¸€ç‚¹ï¼Œ<code>B</code>ä¸ºå›¾ä¸Šå¦ä¸€ç‚¹ã€‚ä¸¤ç‚¹é—´çš„æœ€é•¿è·¯å¾„ï¼Œè®°ä½œ<code>MaxPath(A,B)</code></p><p>è®¾<code>S</code>ä¸ºå›¾ä¸Šä»»æ„ä¸€ç‚¹ï¼Œä»<code>S</code>å‡ºå‘çš„æœ€é•¿è·¯å¾„çš„ç»ˆç‚¹ä¸º<code>P</code>ã€‚é‚£ä¹ˆ<code>P</code>ç‚¹çš„å‡ºåº¦ä¸€å®šä¸º0ï¼šè‹¥å‡ºåº¦ä¸ä¸º0ï¼Œæ€»å¯ä»¥æ‰¾åˆ°æ›´é•¿çš„è·¯å¾„ã€‚ä»è€Œä¸<code>S</code>ï¼Œ<code>P</code>ä¹‹é—´è·¯å¾„ä¸ºæœ€é•¿çŸ›ç›¾ã€‚</p><p>è®¾ä»<code>P</code>ç‚¹å‡ºå‘çš„æœ€é•¿è·¯å¾„çš„ç»ˆç‚¹ä¸º<code>Q</code>ï¼Œåˆ™<code>MaxPath(P,Q)</code>ä¸€å®šç»è¿‡<code>S</code>ç‚¹ï¼šå‡è®¾è¿™æ¡è·¯ä¸ç»è¿‡<code>S</code>ç‚¹ã€‚è®¾<code>T</code>ä¸ºå±äº<code>Path(P,Q)</code>ä¸”ä¸å±äº<code>Path(P,S)</code>çš„ä»»æ„ä¸€ç‚¹ã€‚å› ä¸ºå›¾è”é€šï¼Œåˆ™<code>S</code>ï¼Œ<code>T</code>é—´å­˜åœ¨é•¿åº¦è‡³å°‘ä¸º1çš„è·¯å¾„ã€‚é‚£ä¹ˆ<code>S</code>ï¼Œ<code>P</code>é—´ç»è¿‡<code>T</code>çš„è·¯å¾„é•¿åº¦æ¯”<code>Path(S,P)</code>æ›´é•¿ï¼Œä¸<code>Path(S,P)</code>æ˜¯<code>S</code>å’Œ<code>P</code>ç‚¹é—´æœ€é•¿è·¯å¾„çš„å‡è®¾çŸ›ç›¾ã€‚æ‰€ä»¥<code>MaxPath(P,Q)</code>ä¸€å®šç»è¿‡Sç‚¹ã€‚è€Œ<code>P</code>ç‚¹å’Œ<code>Q</code>ç‚¹å¿…å®šä¸€ä¸ªå…¥åº¦ä¸º0ï¼Œä¸€ä¸ªå‡ºåº¦ä¸º0(<code>P</code>ï¼Œ<code>Q</code>ä½äºå›¾çš„ä¸¤ç«¯)ã€‚å› æ­¤ï¼Œ<code>Path(P,Q)</code>å³ä¸ºæ•´å¼ å›¾çš„æœ€é•¿è·¯å¾„ã€‚</p><p>å›åˆ°æœ¬é¢˜ï¼Œè¦æ±‚æ‰€æœ‰æœ€é•¿è·¯å¾„çš„èµ·å§‹èŠ‚ç‚¹ï¼Œä»»é€‰ä¸€ä¸ªèŠ‚ç‚¹<code>S</code>ï¼Œæ‰€æœ‰ä½¿è·¯å¾„æœ€é•¿çš„<code>P</code>éƒ½æ˜¯è·¯å¾„çš„èµ·å§‹èŠ‚ç‚¹ã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œå½“<code>Path(Pï¼ŒQ)</code>åœ¨æœ€é•¿è·¯å¾„æ—¶ï¼Œ<code>P</code>ç‚¹å’Œ<code>Q</code>ç‚¹éƒ½å¯ä»¥ä½œä¸ºèŠ‚ç‚¹ã€‚è¿™æ˜¯å› ä¸ºè·¯å¾„æ˜¯å¯ä»¥åå‘çš„ã€‚æ±‚å‡ºæ‰€æœ‰çš„<code>P</code>ï¼Œ<code>Q</code>å»é‡æ’åºåå³å¯ã€‚</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"cstdio"</span></span></div><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"vector"</span></span></div><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"algorithm"</span></span></div><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"set"</span></span></div><div class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</div><div class="line"><span class="keyword">int</span> graph[<span class="number">10086</span>][<span class="number">10086</span>];</div><div class="line"><span class="keyword">int</span> n;</div><div class="line"><span class="keyword">int</span> max_depth = <span class="number">-1</span>;</div><div class="line"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; roots;</div><div class="line"><span class="built_in">set</span>&lt;<span class="keyword">int</span>&gt; ans = <span class="built_in">set</span>&lt;<span class="keyword">int</span>&gt;();</div><div class="line"><span class="keyword">bool</span> visited[<span class="number">10086</span>];</div><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">dfs</span><span class="params">(<span class="keyword">int</span> source, <span class="keyword">int</span> depth)</span></span></div><div class="line"><span class="function"></span>&#123;</div><div class="line">visited[source] = <span class="literal">true</span>;</div><div class="line"><span class="keyword">if</span> (depth &gt; max_depth)</div><div class="line">&#123;</div><div class="line">max_depth = depth;</div><div class="line">roots.clear();</div><div class="line">&#125;</div><div class="line"><span class="keyword">if</span> (depth == max_depth) roots.push_back(source);</div><div class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= n; i++)</div><div class="line"><span class="keyword">if</span> (graph[source][i] == <span class="number">1</span> &amp;&amp; !visited[i])</div><div class="line">dfs(i, depth + <span class="number">1</span>);</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></div><div class="line"><span class="function"></span>&#123;</div><div class="line"><span class="built_in">scanf</span>(<span class="string">"%d"</span>, &amp;n);</div><div class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt; n; i++)</div><div class="line">&#123;</div><div class="line"><span class="keyword">int</span> j, k;</div><div class="line"><span class="built_in">scanf</span>(<span class="string">"%d %d"</span>, &amp;j, &amp;k);</div><div class="line">graph[j][k] = graph[k][j] = <span class="number">1</span>;</div><div class="line">&#125;</div><div class="line">fill(visited, visited + <span class="number">10086</span>, <span class="literal">false</span>);</div><div class="line"><span class="keyword">int</span> cnt = <span class="number">0</span>;</div><div class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= n; i++)</div><div class="line">&#123;</div><div class="line"><span class="keyword">if</span> (!visited[i]) &#123; dfs(i, <span class="number">0</span>); cnt++; &#125;</div><div class="line">&#125;</div><div class="line"><span class="keyword">if</span> (cnt != <span class="number">1</span>) &#123; <span class="built_in">printf</span>(<span class="string">"Error: %d components\n"</span>, cnt); <span class="keyword">return</span> <span class="number">0</span>; &#125;</div><div class="line"><span class="keyword">for</span> (<span class="keyword">auto</span> i : roots) ans.insert(i);</div><div class="line">fill(visited, visited + <span class="number">10086</span>, <span class="literal">false</span>);</div><div class="line">dfs(roots[<span class="number">0</span>], <span class="number">0</span>);</div><div class="line"><span class="keyword">for</span> (<span class="keyword">auto</span> i : roots) ans.insert(i);</div><div class="line"><span class="keyword">for</span> (<span class="keyword">auto</span> i : ans) <span class="built_in">printf</span>(<span class="string">"%d\n"</span>, i);</div><div class="line"><span class="keyword">return</span> <span class="number">0</span>;</div><div class="line">&#125;</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Deepest-Root&quot;&gt;&lt;a href=&quot;#Deepest-Root&quot; class=&quot;headerlink&quot; title=&quot;Deepest Root&quot;&gt;&lt;/a&gt;Deepest Root&lt;/h2&gt;&lt;p&gt;A graph which is connected and acyclic can be considered a tree. The height of the tree depends on the selected root. Now you are supposed to find the root that results in a highest tree. Such a root is called &lt;strong&gt;the deepest root.&lt;/strong&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="PAT" scheme="https://verrickt.github.io/tags/PAT/"/>
    
  </entry>
  
  <entry>
    <title>é‡å­¦ã€Œå†’æ³¡æ’åºã€</title>
    <link href="https://verrickt.github.io/2020/03/17/yeah-that-naive-bubble-sort/"/>
    <id>https://verrickt.github.io/2020/03/17/yeah-that-naive-bubble-sort/</id>
    <published>2020-03-17T06:22:12.000Z</published>
    <updated>2021-01-11T12:07:36.836Z</updated>
    
    <content type="html"><![CDATA[<p>PATé‡Œé“¾è¡¨é¢˜æœ‰å„å¼å„æ ·çš„<a href="https://www.liuchuo.net/archives/2116" target="_blank" rel="external">éªšæ“ä½œ</a>ã€‚è¿™äº›éå¸¸è§„æ“ä½œæ˜“å­¦æ˜“ç”¨ï¼Œä½†æ˜¯ä¹ æƒ¯äº†è¿™äº›åï¼Œåè€Œå¯¹é¢˜ç›®çœŸæ­£æƒ³è€ƒå¯Ÿçš„çŸ¥è¯†ç”Ÿç–äº†ã€‚ä»Šå¤©å°±ç¢°åˆ°ä¸€é“è¿™æ ·çš„é¢˜ï¼Œæƒ³ç”¨æ­£ç»çš„ç®—æ³•å†™å´å†™ä¸å‡ºæ¥ã€‚å¸Œæœ›å¤§å®¶ä»¥æˆ‘ä¸ºæˆ’ï¼Œä¸è¦è¿‡å¤šçš„å­¦ä¹ è¿™äº›ã€Œå¥‡æŠ€æ·«å·§ã€<br><a id="more"></a></p><hr><p><a href="https://pintia.cn/problem-sets/994805342720868352/problems/994805425780670464" target="_blank" rel="external">Linked List Sorting</a>è¿™é“é¢˜å¾ˆå¸¸è§„ï¼Œæ˜¯ä¸ªå¾ˆä¸€èˆ¬çš„é“¾è¡¨æ’åºé¢˜ã€‚é“¾è¡¨ä¸èƒ½éšæœºè®¿é—®ï¼Œå› æ­¤ç®—æ³•ä¸Šçš„é€‰æ‹©é™åˆ¶å¾ˆå¤§ï¼Œåªæœ‰å†’æ³¡ã€æ’å…¥ã€é€‰æ‹©æ’åºå¯ä»¥é€‰ã€‚æˆ‘é€‰äº†å†’æ³¡ç»ƒæ‰‹ï¼Œæ²¡æƒ³åˆ°æ²¡åšå‡ºæ¥ã€‚</p><p>å†’æ³¡æ’åºçš„å…³é”®æ­¥éª¤æ˜¯äº¤æ¢ä¸¤ä¸ªç›¸é‚»çš„é€†åºå…ƒç´ ã€‚æ¯å®Œæˆä¸€è¶Ÿåéƒ½æœ‰ä¸€ä¸ªå…ƒç´ è¢«é¡ºåˆ©çš„æ”¾åˆ°äº†æœ€ç»ˆä½ç½®ä¸Šã€‚</p><p>æŒ‰æˆ‘çš„æƒ³æ³•ï¼Œ<code>i in (0,n),j in (i+1,n)</code>ï¼Œæ¯æ¬¡<code>j</code>èµ°åˆ°<code>n</code>æ—¶ï¼Œå°±æŠŠã€Œæœ€å°ã€çš„å…ƒç´ æ”¾åˆ°äº†<code>i</code>çš„ä½ç½®ï¼Œä¾æ¬¡å¢åŠ <code>i</code>å°±å¾—åˆ°äº†å‡åºæ’åˆ—ã€‚<strong>å¾ˆé—æ†¾ï¼Œè¿™æ˜¯é”™çš„</strong><br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n; i++)</div><div class="line"><span class="keyword">for</span> (<span class="keyword">int</span> j = i + <span class="number">1</span>; j &lt; n; j++)</div><div class="line"><span class="keyword">if</span> (n[j] &lt; n[j<span class="number">-1</span>]) swap(n[j], n[j<span class="number">-1</span>]);</div></pre></td></tr></table></figure></p><p>åœ¨<code>[5,3,1,2,6,7]</code>ä¸Šè€ƒè™‘è¿™æ®µä»£ç ï¼Œ<code>if</code>è¯­å¥èƒ½ä¿è¯ï¼Œåªè¦<code>n[j]</code>æ¯”<code>n[j-1]</code>å¤§ï¼Œå°±æŠŠè¿™ä¸ªå¤§çš„å¾€<strong>å</strong>ç§»ã€‚ã€‚è€Œå¯¹äºæœ€å°å€¼<code>1</code>ï¼Œä»…ä»…åœ¨<code>j=2</code>çš„æ—¶å€™è¢«<code>swap</code>åˆ°äº†<code>n[1]</code>ï¼Œå¹¶æ²¡æœ‰çœŸæ­£çš„ç§»åŠ¨åˆ°<code>n[0]</code>è¿™ä¸ªæˆ‘æ‰€æœŸå¾…çš„ä½ç½®ã€‚è§‚å¯Ÿ<code>j</code>æ‰«æçš„æ–¹å‘ä¹Ÿæ˜¯å‘<strong>å</strong>çš„ã€‚</p><p>å†çœ‹å¦ä¸€ç§å†™æ³•<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n; i++)</div><div class="line"><span class="keyword">for</span> (<span class="keyword">int</span> j = n<span class="number">-1</span>; j &gt; i; j--)</div><div class="line"><span class="keyword">if</span> (n[j] &lt; n[j<span class="number">-1</span>]) swap(n[j], n[j<span class="number">-1</span>]);</div></pre></td></tr></table></figure></p><p>è¿™æ—¶<code>if</code>è¯­å¥èƒ½ä¿è¯çš„æ˜¯åªè¦<code>n[j]</code>æ¯”<code>n[j-1]</code>å°ï¼Œå°±æŠŠè¿™ä¸ªå°çš„å¾€<strong>å‰</strong>ç§»ã€‚åœ¨<code>[5,3,1,2,6,7]</code>ï¼Œç¬¬ä¸€è½®ä¹‹å<code>1</code>å°±è¢«æ”¾åœ¨äº†<code>n[0]</code>ä¸Šã€‚æ³¨æ„åˆ°<code>j</code>çš„ç§»åŠ¨æ–¹å‘æ˜¯å‘<strong>å‰</strong>çš„ã€‚</p><p>æˆ‘ä»¬æƒ³å¾—åˆ°å‡åºçš„åºåˆ—ï¼Œé‚£ä¹ˆå°±è¦æŠŠå¤§çš„å¾€åç§»ï¼Œå°çš„å¾€å‰ç§»ã€‚<code>j</code>ä»å‰å¾€<strong>å</strong>ç§»åŠ¨çš„è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬æ˜¯æŠŠæœ€å¤§çš„å…ƒç´ å¾€<strong>åç§»ä¸€ä½</strong>ï¼Œåœ¨ä¸‹ä¸€æ¬¡æ¯”è¾ƒæ—¶æœ€å¤§çš„å…ƒç´ è¿˜ä¼šç»§ç»­è¢«ç§»å¾€åæ–¹ï¼Œæœ€ç»ˆæ”¾åœ¨<code>a[n-1]</code>åŒç†ï¼Œ<code>j</code>ä»åå¾€<strong>å‰</strong>çš„è¿‡ç¨‹ä¸­ï¼Œæœ€å°çš„å…ƒç´ ä¼šè¢«ä¸€ç›´å‰ç§»ï¼Œç›´åˆ°<code>a[0]</code>ã€‚</p><p>å› æ­¤ä¸€è¶Ÿå®Œæˆååˆ°åº•æ˜¯æœ€å¤§çš„å…ƒç´ è¢«æ”¾åˆ°æœ€åäº†è¿˜æ˜¯æœ€å°çš„å…ƒç´ è¢«æ”¾åˆ°æœ€å‰äº†ï¼Œè¦çœ‹<code>j</code>ç§»åŠ¨çš„æ–¹å‘ã€‚åœ¨é“¾è¡¨ä¸­ä»å‰å¾€åæ˜¯æ–¹ä¾¿çš„ï¼Œå› è€Œæœ€å¤§çš„å…ƒç´ ä¼šè¢«å…ˆæ”¾åˆ°æœ€åã€‚æ‰€ä»¥æˆ‘ä»¬éœ€è¦åšçš„æ˜¯ï¼Œæ¯ä¸€è¶Ÿå‡å°‘<code>j</code><strong>ç§»åŠ¨ç»“æŸ</strong>çš„èŒƒå›´ã€‚</p><p>æƒ³åˆ°è¿™é‡ŒåŸºæœ¬ä¸Šå°±èƒ½ç†æ¸…äº†ã€‚åŠ ä¸Šå¤„ç†å¤´èŠ‚ç‚¹çš„ä¸€äº›ç»†èŠ‚åï¼Œä¸éš¾å†™å‡ºä»£ç <br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"cstdio"</span></span></div><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"vector"</span></span></div><div class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</div><div class="line"><span class="class"><span class="keyword">struct</span> <span class="title">node</span> &#123;</span></div><div class="line"><span class="keyword">int</span> addr;</div><div class="line"><span class="keyword">int</span> key;</div><div class="line"><span class="keyword">int</span> next;</div><div class="line">&#125;;</div><div class="line"><span class="built_in">vector</span>&lt;node&gt; nodes = <span class="built_in">vector</span>&lt;node&gt;(<span class="number">100086</span>);</div><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></div><div class="line"><span class="function"></span>&#123;</div><div class="line"><span class="keyword">int</span> n, head;</div><div class="line"><span class="built_in">scanf</span>(<span class="string">"%d %d"</span>, &amp;n, &amp;head);</div><div class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n; i++)</div><div class="line">&#123;</div><div class="line"><span class="keyword">int</span> addr, key, next;</div><div class="line"><span class="built_in">scanf</span>(<span class="string">"%d %d %d"</span>, &amp;addr, &amp;key, &amp;next);</div><div class="line">nodes[addr] = node(&#123; addr,key,next &#125;);</div><div class="line">&#125;</div><div class="line"><span class="keyword">int</span> newHead = head;</div><div class="line"><span class="keyword">int</span> end = <span class="number">-1</span>;</div><div class="line"><span class="keyword">while</span> (end != newHead)</div><div class="line">&#123;</div><div class="line"><span class="keyword">int</span> t = newHead;</div><div class="line"><span class="keyword">int</span> pre = <span class="number">-1</span>;</div><div class="line"><span class="keyword">while</span> (nodes[t].next != end)</div><div class="line">&#123;</div><div class="line"><span class="keyword">int</span> next = nodes[t].next;</div><div class="line"><span class="keyword">if</span> (nodes[t].key &gt; nodes[next].key)</div><div class="line">&#123;</div><div class="line"><span class="keyword">if</span> (pre != <span class="number">-1</span>)</div><div class="line">nodes[pre].next = next;</div><div class="line"><span class="keyword">else</span></div><div class="line">newHead = next;</div><div class="line">nodes[t].next = nodes[next].next;</div><div class="line">nodes[next].next = t;</div><div class="line">&#125;</div><div class="line">pre = t;</div><div class="line">t = next;</div><div class="line">&#125;</div><div class="line">end = t;</div><div class="line">&#125;</div><div class="line"><span class="keyword">int</span> h = newHead;</div><div class="line"><span class="keyword">int</span> cnt = <span class="number">0</span>;</div><div class="line"><span class="keyword">while</span> (h != <span class="number">-1</span>)</div><div class="line">&#123;</div><div class="line">cnt++; h = nodes[h].next;</div><div class="line">&#125;</div><div class="line"><span class="keyword">if</span> (cnt == <span class="number">0</span>)</div><div class="line"><span class="built_in">printf</span>(<span class="string">"0 -1\n"</span>);</div><div class="line"><span class="keyword">else</span></div><div class="line"><span class="built_in">printf</span>(<span class="string">"%d %05d\n"</span>, cnt, newHead);</div><div class="line">h = newHead;</div><div class="line"><span class="keyword">while</span> (h != <span class="number">-1</span>)</div><div class="line">&#123;</div><div class="line"><span class="keyword">if</span> (nodes[h].next == <span class="number">-1</span>)</div><div class="line"><span class="built_in">printf</span>(<span class="string">"%05d %d -1\n"</span>, nodes[h].addr, nodes[h].key);</div><div class="line"><span class="keyword">else</span></div><div class="line"><span class="built_in">printf</span>(<span class="string">"%05d %d %05d\n"</span>, nodes[h].addr, nodes[h].key, nodes[h].next);</div><div class="line">h = nodes[h].next;</div><div class="line">&#125;</div><div class="line"><span class="keyword">return</span> <span class="number">0</span>;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p><p>æ¬¢å¤©å–œåœ°çš„å»æäº¤ï¼Œå‘ç°æœ‰ä¸€ä¸ªç‚¹è¶…æ—¶äº†ã€‚è¯šç„¶ï¼Œå†’æ³¡æ’åºçš„æ—¶é—´å¤æ‚åº¦æ˜¯<code>Î˜(n^2)</code>ï¼Œéå¸¸è§„æ“ä½œé‡Œç”¨<code>std::sort</code>å¤„ç†åè¾“å‡ºçš„æ—¶é—´å¤æ‚åº¦æ˜¯<code>Î˜(nlogn)</code>ï¼Œè¶…æ—¶æ˜¯ç†æ‰€åº”å½“ã€‚æ”¾çœ¼æœ›å»ç»å¤§å¤šæ•°ACçš„ä»£ç éƒ½ç”¨çš„ä¸è¿™ç§æŠ•æœºå–å·§çš„æ–¹æ³•ã€‚ä¸çŸ¥é“è¿™æ˜¯ä¸æ˜¯è¿èƒŒå‡ºé¢˜äººçš„åˆè¡·å‘¢ï¼Ÿ</p><p>å½“ç„¶ï¼Œè¿™é—®é¢˜è½®ä¸åˆ°æˆ‘è¯„å¤´è®ºè¶³ï¼Œæˆ‘åªæ˜¯ä¸ªè¿å†’æ³¡æ’åºéƒ½å¿˜å¾—ä¸€å¹²äºŒå‡€çš„æ¸£æ¸£ğŸ™‚</p><hr><p>æ€»ç»“ï¼š</p><ul><li>å†’æ³¡æ’åºä¸€è¶Ÿåè¢«æ”¾åˆ°æœ€ç»ˆä½ç½®çš„å…ƒç´ ä¸æ‰«ææ–¹å‘æœ‰å…³</li><li>åŸºæœ¬åŠŸè¦æ‰“æ‰å®ï¼Œåˆ‡å¿Œã€Œçœ¼é«˜æ‰‹ä½ã€</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;PATé‡Œé“¾è¡¨é¢˜æœ‰å„å¼å„æ ·çš„&lt;a href=&quot;https://www.liuchuo.net/archives/2116&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;éªšæ“ä½œ&lt;/a&gt;ã€‚è¿™äº›éå¸¸è§„æ“ä½œæ˜“å­¦æ˜“ç”¨ï¼Œä½†æ˜¯ä¹ æƒ¯äº†è¿™äº›åï¼Œåè€Œå¯¹é¢˜ç›®çœŸæ­£æƒ³è€ƒå¯Ÿçš„çŸ¥è¯†ç”Ÿç–äº†ã€‚ä»Šå¤©å°±ç¢°åˆ°ä¸€é“è¿™æ ·çš„é¢˜ï¼Œæƒ³ç”¨æ­£ç»çš„ç®—æ³•å†™å´å†™ä¸å‡ºæ¥ã€‚å¸Œæœ›å¤§å®¶ä»¥æˆ‘ä¸ºæˆ’ï¼Œä¸è¦è¿‡å¤šçš„å­¦ä¹ è¿™äº›ã€Œå¥‡æŠ€æ·«å·§ã€&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="PAT" scheme="https://verrickt.github.io/tags/PAT/"/>
    
  </entry>
  
</feed>
