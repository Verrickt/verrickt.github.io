{"meta":{"title":"Melchior on CLR","subtitle":"Verrickt as a Programmer","description":"A programmer focus mainly on Microsoft Technologies","author":"Verrickt","url":"https://verrickt.github.io"},"pages":[{"title":"About","date":"2018-08-04T09:08:39.000Z","updated":"2018-08-04T09:18:10.589Z","comments":true,"path":"about/index.html","permalink":"https://verrickt.github.io/about/index.html","excerpt":"","text":"程序员。 关键词： Windows desktop C# XAML UWP WPF GNU/Linux 1public string 📧=&gt;\"von@hohm.in\";"}],"posts":[{"title":"使用CUDA为Tensorflow加速","slug":"gpu-accelerated-tranining-with-cuda","date":"2020-07-28T13:39:05.000Z","updated":"2020-07-28T14:29:22.421Z","comments":true,"path":"2020/07/28/gpu-accelerated-tranining-with-cuda/","link":"","permalink":"https://verrickt.github.io/2020/07/28/gpu-accelerated-tranining-with-cuda/","excerpt":"梯度下降法大部分时间都在进行向量和矩阵运算。这些运算是天然可以并行化的。因此使用GPU进行运算会比CPU运算快得多。而常用的框架Tensorflow就通过CUDA提供了GPU运算的支持。","text":"梯度下降法大部分时间都在进行向量和矩阵运算。这些运算是天然可以并行化的。因此使用GPU进行运算会比CPU运算快得多。而常用的框架Tensorflow就通过CUDA提供了GPU运算的支持。根据官方页面，对软硬件有如下要求: The following GPU-enabled devices are supported: NVIDIA® GPU card with CUDA® architectures 3.5 or higher. See the list of CUDA®-enabled GPU cards. For GPUs with unsupported CUDA® architectures, or to avoid JIT compilation from PTX, or to use different versions of the NVIDIA® libraries, see the Linux build from source guide.-On systems with NVIDIA® Ampere GPUs (CUDA architecture 8.0) or newer, kernels are JIT-compiled from PTX and TensorFlow can take over 30 minutes to start up. This overhead can be limited to the first start up by increasing the default JIT cache size with: ‘export CUDA_CACHE_MAXSIZE=2147483648’ (see JIT Caching for details).-Packages do not contain PTX code except for the latest supported CUDA® architecture; therefore, TensorFlow fails to load on older GPUs when CUDA_FORCE_PTX_JIT=1 is set. (See Application Compatibility for details.) The following NVIDIA® software must be installed on your system: NVIDIA® GPU drivers —CUDA® 10.1 requires 418.x or higher. CUDA® Toolkit —TensorFlow supports CUDA® 10.1 (TensorFlow &gt;= 2.1.0) CUPTI ships with the CUDA® Toolkit. cuDNN SDK 7.6 换句话说，只要不是上古时代的NVIDIA GPU，都可以进行运算。 踩坑Tensorflow安装tensorflow后要还要安装tensorflow-gpu。tensorflow-gpu不是tensorflow的替代者，而是支持运算GPU的模块。不要被网上的信息误导，两者都需要安装12pip install tensorflowpip install tensorflow-gpu CUDACUDA请一定安装10.1版本，更新的和更旧的版本都不支持。要去Archive里找。 cuDNNcuDNN请一定安装7.6版本，更新的和更旧的版本都不支持。cuDNN要先去注册NVIDIA developer再去Archive里找7.6的 Coding全部安装好后去跑hello world。tensorflow可能会卡在1I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0 现在去泡杯咖啡，坐和放宽，大概过几个小时就好了。这么大的延迟只会第一次出现。原因似乎是因为GPU那边在做JIT🙃 等出现这一行1I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1376 MB memory) -&gt; physical GPU (device: 0, name: GeForce 840M, pci bus id: 0000:01:00.0, compute capability: 5.0) 就说明成功了。 我的渣渣840M跑训练比CPU快了一个数量级;-) 吐槽已经2020年了，CUDA已经出到11.0了的RC了，tensorflow居然还只支持2019年2月发布的10.1。cuDNN同理，也用的是很老的版本。 CUDA作为NVIDIA家私有的一套API，形成了事实标准，这很不好。而AMD家的搞得叫做ROCm的一套东西，很遗憾的还没成什么气候。ROCm的tensorflow是官方版的一份fork，binary还是社区自己编译的，可以想象坑是无比的多。 希望开源的标准尽快取代掉私有的CUDA，让A家的GPU也能无痛的跑科学计算。","categories":[],"tags":[{"name":"Machine learning","slug":"Machine-learning","permalink":"https://verrickt.github.io/tags/Machine-learning/"},{"name":"CUDA","slug":"CUDA","permalink":"https://verrickt.github.io/tags/CUDA/"},{"name":"Tensorflow","slug":"Tensorflow","permalink":"https://verrickt.github.io/tags/Tensorflow/"}]},{"title":"循环神经网络","slug":"recurrent-neural-network","date":"2020-07-23T10:21:45.000Z","updated":"2020-07-23T14:31:59.010Z","comments":true,"path":"2020/07/23/recurrent-neural-network/","link":"","permalink":"https://verrickt.github.io/2020/07/23/recurrent-neural-network/","excerpt":"Recurrent Neural network(RNN)是一种专门处理序列的神经网络。正如CNN可以轻易地处理大规模的“网格状”数据，RNN能够处理其他网络架构都无法处理的长的序列。","text":"Recurrent Neural network(RNN)是一种专门处理序列的神经网络。正如CNN可以轻易地处理大规模的“网格状”数据，RNN能够处理其他网络架构都无法处理的长的序列。 Why RNN考虑这样的问题：航空公司要从文字中提取顾客的目的地和出发地。 对于 I’ll arrive from Shanghai to ShenZhen 目的地是ShenZhen，出发地是Shanghai 而对于 I’ll arrive to Shanghai from ShenZhen 则目的地是Shanghai，出发地是ShenZhen。 使用基于地点在句中位置的方法无法解决问题。到底是出发地还是目的地，与前后的词语都有关。这些信息这称为Context(上下文) 若使用MLP，则需要在每个位置都重复的学会人类语言的规则，大量重复的参数不但使得计算代价激增，而且也显著增加over-fitting的几率。 因此，需要一种专门的结构来处理序列数据。RNN应运而生。 Vanilla-RNNRNN企图使用Memory(记忆)来解决问题：当前数据处理过程中隐藏单元的值被保留，下个数据处理时作为额外的输入，这样RNN就拥有了“记住”已经看过数据的能力。 根据“记忆”的来自隐藏单元还是输出的不同，可以将RNN分为Elman network和Jordan network两种 现在的RNN只能往一个方向看，有时只“往前看”的网络并不能解决问题，我们还需要“往后看”。既往前看又往后看的RNN叫做bidirectional RNN(双向RNN) 这时只要训练两个RNN：一个从前往后，一个从后往前，再把他们的输出丢到另一个MLP去做处理就可以了 Exploding/Vanishing gradientsRNN的性能十分出色，但训练的过程往往困难重重。其中一个重要的因素就是Exploding/Vanishing gradients(梯度爆炸/消失)。 在RNN中，t-1时刻的隐藏单元的输出会影响到t时刻的隐藏单元。 h^{t}=Wh^{t-1}重复使用这个式子，则 h^{t}=W^t\\prod_{i=1}^{i=t}h_{i}若W可进行特征值分解，则 W=Q^{\\top}A Q \\\\ W^t=Q^{\\top}A^{t}Q 若W的特征值λ&gt;1，则经过t次相乘后，λ^t的值会变得非常大(梯度爆炸)。 若λ&lt;1，则经过t次相乘后，λ^t的值会变得非常接近零(梯度消失) 梯度爆炸/消失对梯度下降法的干扰非常大，以至于训练过程中出现匪夷所思的结果 解决这个问题的关键在于给“记忆”可变的权重。 LSTMLong short-term memory(长短期记忆)是一个解决梯度爆炸/消失问题效果较好的方案。在LSTM中，在存储“记忆”的地方(cell)加上了三个gate： input gate output gate forget gate 这些gate的控制信号经过sigmoid函数后得到的0~1之间的数值表示这些门“打开”的程度 input gate决定“记忆”是否接受新的输入 output gate决定“记忆”是否被读出 forget gate决定“记忆是否被遗忘” 图中z是输入,zi,zo,zf分别是上述三个gate的控制信号。设原来存储在“记忆”中的值为c,则新的值c’如此产生 输入z经过激活函数的值g(z)与input gate的值相乘，得到g(z)*f(zi) 原来“记忆”中的值c与forget gate的值相乘得到c*f(zf) 将输入与原“记忆”相加的结果作为新的记忆c’=g(z)f(zi)+cf(zf) 新的值c’与output gate的值相乘得到c’*f(zo)，结果作为“记忆”的输出 其中zi,zo,zf是网络的参数，由训练过程中自己学得。将多个LSTM的cell连接起来，就得到了能够处理复杂问题的RNN。当然在实际使用中，zi,zo,和zf也可以接受网络其他部分的参数，这就要看具体的问题了。","categories":[{"name":"Machine learning","slug":"Machine-learning","permalink":"https://verrickt.github.io/categories/Machine-learning/"}],"tags":[{"name":"Machine learning","slug":"Machine-learning","permalink":"https://verrickt.github.io/tags/Machine-learning/"},{"name":"RNN","slug":"RNN","permalink":"https://verrickt.github.io/tags/RNN/"}]},{"title":"卷积神经网络","slug":"convolutional-neural-network","date":"2020-07-18T11:31:16.000Z","updated":"2020-07-18T14:42:13.972Z","comments":true,"path":"2020/07/18/convolutional-neural-network/","link":"","permalink":"https://verrickt.github.io/2020/07/18/convolutional-neural-network/","excerpt":"任何一个机器学习的任务都可以被拆解为三步 找到一组函数(model) 找到评价函数好坏的标准(loss) 找出最好的函数(optimization) 这三步在神经网络中同样成立。但在神经网络中，要找的不是一组函数，而是一种网络架构(architecture)。本文中的卷积神经网络(CNN)就是一种网络架构","text":"任何一个机器学习的任务都可以被拆解为三步 找到一组函数(model) 找到评价函数好坏的标准(loss) 找出最好的函数(optimization) 这三步在神经网络中同样成立。但在神经网络中，要找的不是一组函数，而是一种网络架构(architecture)。本文中的卷积神经网络(CNN)就是一种网络架构根据universal approximation theorem，MLP已经足够表达任意函数。而在实际中，因为相邻层级的神经元之间的任意互联导致MLP容易过拟合。而在处理一些具有结构化的输入时，MLP中又会有大量的参数重复。为了解决这些问题，引入了CNN。 Why CNN?考虑这样一个问题：如何让机器认出图片里的一张鸟？ 回顾人类的思考过程，若图片中的物体具有鸟的基本特征，如喙、翅膀和爪，则认为它是鸟。在这个过程中，我们并不关心翅膀在图片中的位置：只要有喙就可以了。对翅膀和爪也是同样成立。 继续思考，怎么认出喙？喙可以由它的边界(geometry)来定义：两条斜线围成的尖尖的多西。翅膀和爪也可以有对应的边界来定义。同样的，我们并不关心边界出现的位置，只要它能确定这个物体是个喙就可以了。 实际上这就是CNN出现的动机之一：我们只关心一些模式是否出现，而不关心它们出现的位置。换句话说，我们所期待的模式会在图片的不同区域出现。 回到图像的例子上，人们发现了三个属性： 比起整张图片，模式通常是比较小的 相同的模式会在不同的区域出现 对图片做subsampling(如删掉奇数行和偶数列)并不会改变图中的物体 基于这三种属性，人们提出了convolution和pooling两种操作。convolution对应属性1、2，pooling对应属性3。convolution和pooling交替就是CNN的架构。 ConvolutionConvolution的基础单位是filter(kernel)。filter是一个矩阵，用来检测模式考虑如下的情景 输入是6*6的图片，filter是3*3的矩阵。在图片的左上角找到一个与filter一样大的矩阵，做element-wise product，会得到一个数值。将filter往右移动一格，继续这个步骤，就又得到新的数值。移动的距离称为stride(步长) filter通常比输入更小，而filter在移动的过程中可以在不同的位置上检测出对应的模式。这就体现了属性1和2。 filter移动完毕后会得到一个新的矩阵。我们可以使用多个filter,将这些filter的输出放在一起会得到一个三维的张量，称为feature map. 输入也不一定是黑白的。若输入彩色图片，则需要一个三维的张量来表示 \\mathrm{V}_{c,i,j}这时的filter也是一个三维张量。这里的重点是，不管输入是什么样，filter的“z轴”要与输入的“z轴”一样高，然后在其他轴上以stride移动，得到的标量集合起来成为新的张量作为下一层的输入。Convolution得到的feature map会缩小，这可以补零(zero padding)来避免 实际中通过在MLP中共享参数来实现。使用了更少的参数来做同样的事情就不容易过拟合。 PoolingPooling的思路更简单了。对一些区域进行采样，得到一张更小的image,这就是属性3。 pooling有很多选择，可以取算术平均，可以取L2 norm。一般比较常见的是max pooling。 通过pooling，网络就可以对输入中比较规律的变化不那么敏感。这称为invariance。 Flatten做完pooling后得到的是一个张量。把它“拉直”成一个向量，丢给MLP就可以实现我们所要做的事情了。 CNN并不只能用于图像处理，只要要处理的问题具有上述的三个属性，就可以用CNN来解决。下面是一些用CNN效果比较好的任务:","categories":[{"name":"Machine learning","slug":"Machine-learning","permalink":"https://verrickt.github.io/categories/Machine-learning/"}],"tags":[{"name":"Machine learning","slug":"Machine-learning","permalink":"https://verrickt.github.io/tags/Machine-learning/"},{"name":"CNN","slug":"CNN","permalink":"https://verrickt.github.io/tags/CNN/"}]},{"title":"神经网络中的Regularization","slug":"regularization-of-neural-network","date":"2020-07-15T02:22:44.000Z","updated":"2020-07-15T06:33:00.639Z","comments":true,"path":"2020/07/15/regularization-of-neural-network/","link":"","permalink":"https://verrickt.github.io/2020/07/15/regularization-of-neural-network/","excerpt":"Deep learning algorithms are typically applied to extremely complicated domains where the true generation process essentially involves simulating the entire universe… Controlling the complexity of the model is not a simple matter of finding the model of the right size, with the right number of parameters. Instead, we might find—and indeed in practical deep learning scenarios, we almost always do find—that the best fitting model (in the sense of minimizing generalization error) is a large model that has been regularized appropriately","text":"Deep learning algorithms are typically applied to extremely complicated domains where the true generation process essentially involves simulating the entire universe… Controlling the complexity of the model is not a simple matter of finding the model of the right size, with the right number of parameters. Instead, we might find—and indeed in practical deep learning scenarios, we almost always do find—that the best fitting model (in the sense of minimizing generalization error) is a large model that has been regularized appropriately regularization是我们应对overfitting的常用手段。在比较简单的模型，例如linear-regresseion中，人对的capacity的估计通常比较准确，一般不需要做regularization。而在神经网络动辄成千上万个节点，数以百万的参数让估测capacity变得极为困难。正如花书所说，泛化性能最好的往往是复杂的模型加上合理的regularization而得到的 Parameter Penalities正如三次式最高次的系数为零时退化为二次式，有越多的参数接近0，模型的capacity就越低，即「绑住手脚」。一般来说regularization不会考虑bias，因为bias对capacity的影响并不如weight那么大，但有需要仍可以给bias加上惩罚。 参数惩罚的思路是，给Loss加上所有参数θ的函数Ω(θ)这一项，让模型更偏好于θ接近0的归纳假设。 \\hat{L}(\\theta,\\alpha)=L(\\theta)+\\alpha \\Omega(\\theta)L2 normL2 norm中所选的函数Ω是 \\Omega(\\theta)=\\frac{1}{2}\\theta^{\\top}\\thetaLoss的梯度变为 \\nabla_{\\theta} \\hat{L}=\\nabla_{\\theta}L+\\alpha\\theta带入梯度下降公式 \\theta^{N+1} \\leftarrow\\theta^N-\\epsilon\\nabla_\\theta \\hat{L} =\\theta^N-\\epsilon\\nabla_\\theta L - \\alpha\\epsilon \\theta^N = (1-\\alpha \\epsilon)\\theta ^N-\\epsilon\\nabla_\\theta L每次梯度下降更新参数时会把原来的参数乘上一个固定的值1-αε，使得θ接近于0 L1 normL1 norm中的函数Ω是 \\Omega(\\theta)=\\sum_{i=1}^N|\\theta_i|Loss的梯度变为 \\nabla_{\\theta} \\hat{L}=\\nabla_{\\theta}L+\\alpha \\mathrm{sgn}(\\theta)带入梯度下降公式 \\theta^{N+1} \\leftarrow\\theta^N-\\epsilon\\nabla_\\theta \\hat{L} =\\theta^N-\\epsilon\\nabla_{\\theta}L-\\alpha\\epsilon \\mathrm{sgn}(\\theta^N) = \\theta ^N-\\epsilon\\nabla_\\theta L -\\alpha\\epsilon \\mathrm{sgn}(\\theta^N)发现参数更新时会在每一个分量i中减掉一个固定值αε*sgn(i) L2 norm中，每次都乘上一个小于1的数，当参数大于1时，减小的速度就非常快；当参数小于1时，减小的速度就比较慢。因此L2中最终各个参数比较接近，但不会出现很多0。 而L1 norm中，每次减小一个固定值。对很大的参数，减法的步长很小，减小的效果就不是很明显。而参数很小时，减法的步长相对很大，就很容易出现0。因此L1中很容易出现很大的参数和较多的0。 含较多0的矩阵叫做稀疏矩阵(Sparse matrix)，因此用L1能够得到比较稀疏的参数。 Early stopping early stopping的思路就更简单了。既然复杂的模型训练时会关注一些无关的特征，那干脆不要跑那么多次训练就好了。 early stopping的形式化描述：12345678910111213141516171819202122Let n be the number of steps between evaluations.Let p be the “patience,” the number of times to observe worsening validation set error before giving up. Let θ^o be the initial parameters. θ ← θ^o i ← 0 j ← 0 v ←∞ θ∗ ← θ i∗ ← i while j &lt; p do θ Update by running the training algorithm for n steps. i ← i + n v&apos; ← ValidationSetError(θ) if v&apos; &lt; v then j ← 0 θ∗ ← θ i∗ ← i v ← v&apos; else j ← j + 1 end ifend while 12nodes:Best parameters are θ∗ , best number of training steps is i∗ EnsembleEnsemble认为，模型中出现的错误是随机的，那么使用同样的数据训练出k个不同的模型，将它们的结果取平均就可以得到比较好的结果。 复杂模型error主要来自variance。从直觉上说，对多个模型取平均就能很好的抵消一部分variance。ensemble的坏处在于，它需要训练k个不同的模型，导致计算量大增。有时这样的计算量是无法承受的，因此希望有一个代价不那么大的ensemble方法，这就是dropout dropoutdropout和以往的方法不同，它把当前训练的模型当作是很多模型的叠加：每次训练时，每个单元都有p的概率从网络中被移除，每次只针对这些被留下来的单元做参数的更新。有n个单元时，因为每一个单元都可以被留下/丢掉，这就可能产生2^N个不同的网络结构。dropout就认为当前的模型是这2^N个网络的ensemble。 下面是一个2层的神经网络。它有2个隐藏单元。dropout可以产生的网络由16个。使用dropout的网络经过一次训练后相当于训练了2^N个子网络，计算量的问题就这样解决了。 使用dropout的网络训练完成后，在测试时不再移除神经元，而是给网络整体的参数乘上1-p。一个直觉的解释：12345设w是dropout得到的参数，w&apos;是训练时的参数。在训练时神经元有p的概率被丢弃，所以网络中实际神经元个数的期望是n*(1-p)。现在要让网络的输出尽可能的一致，则n*w*(1-p) = n*w&apos;即w&apos;=w*(1-p) 实际的分析发现，在整个网络是线性时，上面的论述是精确成立的，而加上激活函数的网络往往不是线性的。但实际使用上dropout的性能也确实比较接近ensemble的结果。为什么会这样我们不清楚，总之拿来主义了。世界各地有很多科学家也在探究背后的奥秘，希望有一天能找出一个精确的解释吧。","categories":[{"name":"Machine learning","slug":"Machine-learning","permalink":"https://verrickt.github.io/categories/Machine-learning/"}],"tags":[{"name":"Machine learning","slug":"Machine-learning","permalink":"https://verrickt.github.io/tags/Machine-learning/"},{"name":"Regularization","slug":"Regularization","permalink":"https://verrickt.github.io/tags/Regularization/"}]},{"title":"后向传播-进阶版的梯度下降算法","slug":"back-propagation-gradient-descent-improved","date":"2020-07-11T03:21:40.000Z","updated":"2020-07-11T14:11:29.102Z","comments":true,"path":"2020/07/11/back-propagation-gradient-descent-improved/","link":"","permalink":"https://verrickt.github.io/2020/07/11/back-propagation-gradient-descent-improved/","excerpt":"本文是NTU ML 2020中BackPropagation部分的笔记。 梯度下降是非常常用的优化算法。而在深度学习中，一个神经网络的参数动辄几百万，像线性模型那样人手工算好再告诉机器的方式已经不合适了，需要一种更高效的算法来计算梯度。","text":"本文是NTU ML 2020中BackPropagation部分的笔记。 梯度下降是非常常用的优化算法。而在深度学习中，一个神经网络的参数动辄几百万，像线性模型那样人手工算好再告诉机器的方式已经不合适了，需要一种更高效的算法来计算梯度。后向传播是计算梯度的高效算法，它的理论基础是链式法则。 p=\\phi(x),q=\\varphi(x),g=f(p,q) \\\\ \\frac{dg}{dx}=\\frac{\\partial g}{\\partial p}\\frac{dp }{dx}+\\frac{\\partial g}{\\partial q}\\frac{dq}{dx} 梯度下降的目标函数total loss是各个example的和 L(\\theta)=\\sum_{n=1}^N C^{n}(\\theta)所以只需能计算出其中一项的loss就可求total loss的梯度。 考虑如下的一个3x2的神经网络 第一层第一个神经元的参数w1，输入激活函数的值z=w1*x1+w2*x2+b1则由链式法则 \\frac{\\partial L}{\\partial w_1}=\\frac{\\partial L}{\\partial z}\\frac{\\partial z}{\\partial w_1}=\\frac{\\partial L}{\\partial z}x_1转化为求 \\frac{\\partial L}{\\partial z}考虑该神经元的下一层，z经过激活后的输出a被当作下一层的输入：则由链式法则， \\frac{\\partial L}{\\partial z} = \\frac{\\partial L}{\\partial a}\\frac{\\partial a}{\\partial z}=\\frac{\\partial L}{\\partial a}\\sigma'(z)观察到a已经变成了下一层网络的输入，问题又化为了最初求loss对网络输入的微分，但问题的规模-求梯度的“层数”却减少了一层。思维敏锐的同学可能已经发现了，这是分治法。 让我们更进一步 \\frac{\\partial L}{\\partial a}=\\frac{\\partial L}{\\partial z'}\\frac{\\partial z'}{\\partial a}+\\frac{\\partial L}{\\partial z''}\\frac{\\partial z''}{\\partial a} = \\frac{\\partial L}{\\partial z'} w_3+ \\frac{\\partial L}{\\partial z''} w_4从而 \\frac{\\partial L}{\\partial z}=\\sigma'(z) \\left[\\frac{\\partial L}{\\partial z'} w_3+ \\frac{\\partial L}{\\partial z''} w_4 \\right] 看起来就像是一个以偏微分为输入,y=σ&#39;(z)x为激活函数的神经元的输出 考虑所有激活函数的输入z，当前层的偏微分z与后一层之间的偏微分z&#39;存在计算上的依赖关系。因此计算loss对z的偏微分时，要从后往前算。因此得名backward pass。 而计算本层神经元的输入a时是从前往后算的，因此叫做foreward pass 总结一下，要计算L对参数w的偏微分需要进行两个步骤： foreward pass. 算出L对当前层神经元的输入a的偏微分。从前往后算。 backward pass. 计算出L对当前层激活函数的输入z的偏微分。从后往前算。 两者相乘，即得 \\frac{\\partial L}{\\partial w}","categories":[{"name":"Machine learning","slug":"Machine-learning","permalink":"https://verrickt.github.io/categories/Machine-learning/"}],"tags":[{"name":"Machine learning","slug":"Machine-learning","permalink":"https://verrickt.github.io/tags/Machine-learning/"},{"name":"Backpropagation","slug":"Backpropagation","permalink":"https://verrickt.github.io/tags/Backpropagation/"}]},{"title":"从分类到对率回归再到到神经网络","slug":"from-logistic-regression-to-neural-network","date":"2020-07-04T14:01:07.000Z","updated":"2020-07-05T06:18:15.799Z","comments":true,"path":"2020/07/04/from-logistic-regression-to-neural-network/","link":"","permalink":"https://verrickt.github.io/2020/07/04/from-logistic-regression-to-neural-network/","excerpt":"前言本文的是台湾大学2020机器学习的Classification1和2的笔记，奉行Lazy evaluation策略，对这些知识更深层次的探究只在绝对必要时完成。","text":"前言本文的是台湾大学2020机器学习的Classification1和2的笔记，奉行Lazy evaluation策略，对这些知识更深层次的探究只在绝对必要时完成。 分类在分类问题，我们要找一个函数f，f的输入是代表样本的向量，输出是一个代表类别的标量。将样本分为两类的叫做二元分类，多于两类的叫做多元分类。我们先考虑二元分类问题。 引例XCOM中面对的敌人有变种人(ADVENT)和外星人(Alien)两种，长老(The elder)送来了新的物种，现有训练资料变种人数据15组，外星人10组，你能据此帮助被关在外星人网络中充当首脑的人类指挥官分辨新物种的种类吗？ 先考虑这个问题，蓝绿两个盒子，其中各有蓝绿球若干。从蓝盒子抽球的概率是1/3，从绿盒子抽球的概率是2/3。已知抽到了蓝球，问从蓝色盒子里抽到的概率是多少? P(蓝盒子)=\\frac{1}{3} \\\\ P(绿盒子)=\\frac{2}{3} \\\\ P(抽到蓝球|蓝盒子)=\\frac{4}{5} \\\\ P(绿盒子|抽到蓝球)=\\frac{2}{5} \\\\ P(抽到蓝球)=P(蓝盒子)P(抽到蓝球|蓝盒子)+P(绿盒子)P(绿盒子|抽到蓝球)=\\frac{1*4}{3*5}*\\frac{2*2}{3*5}=\\frac{8}{15} \\\\ P(蓝盒子|抽到蓝球)=\\frac{P(抽到蓝球|蓝盒子)P(蓝盒子)}{P(抽到蓝球)}=\\frac{\\frac{1*4}{3*5}}{\\frac{8}{15}}=\\frac{1}{2}贝叶斯公式我们预测的依据是贝叶斯公式： P(A|B)=\\frac{P(B|A)P(A)}{P(B)}用C1表示敌人属于变种人，C2表示敌人属于外星人，那么 P(C_1|x)=\\frac{P(x|C_1)P(C_1)}{P(x|C_1)P(C_1)+P({x|C_2})P(C_2))}training set中一共有15组变种人，10组外星人。则 P(C_1)=\\frac{15}{15+10}=\\frac{3}{5} \\\\ P(C_2)=\\frac{10}{15+10}=\\frac{2}{5}接下来是确定P(x|C1)。思考一下，testing-set上的数据我们的算法从来没有见过，那么P(x|C1)应该是0咯。可这样贝叶斯公式就变成0/0了，还怎么预测? P(C1|x)在此，我们大胆假设，目前所见过的C1的example是由一个概率分布产生的，这样就可以对testing-set上没见过的数据了求概率了。 这是课件的一张图，对于图中79个example，认为是一个Gaussian产生的，但好多个Gaussian都可以产生这样的点，具体是哪一个呢？这里使用极大似然的思想，就认为使得产生训练数据的概率最大的Gaussian好了 回到我们的例子，似然函数是 L(\\mu,\\Sigma)=\\prod_i^{15}f_{\\mu,\\Sigma}(x^i) \\\\ f_{\\mu,\\Sigma}=\\frac{1}{2\\pi^{D/2}}\\frac{1}{|\\Sigma|^{1/2}}\\exp{\\{-\\frac{1}{2}(x-\\mu)^T\\Sigma^{-1}(x-\\mu)\\}} \\\\ \\mu^*,\\Sigma^*=\\argmax_{\\mu,\\Sigma} L(\\mu,\\Sigma)经过一番运算后，得到μ和Σ的闭式解: \\mu^*=\\frac{1}{15}\\sum_{n=1}^{15}x^n \\Sigma^*=\\frac{1}{15}\\sum_{n=1}^{15}(x^n-\\mu^*)(x^n-\\mu^*)^T这样就得到了C1的分布G1(μ,Σ)，同理可以得到C2的分布G2(μ,Σ)，有了这两项就可以计算条件概率从而完成预测了。12if(x|C1)&gt;0.5 output ADVENTelse output ALIEN 回到上课的例子，这个模型的效果并不好:(。 想到Probability中Gaussion的PS 如果用高斯分布来做分类问题，让分布共用协方差矩阵Σ通常效果会比使用各自的协方差矩阵效果好。 那就试试共用协方差矩阵吧 共用协方差矩阵似然函数变为 L(\\mu_1,\\mu_2,\\Sigma)=\\prod_i^{15}f_{\\mu1,\\Sigma}(x^i)\\prod_{j=16}^{25}f_{\\mu2,\\Sigma}(x^i) \\\\求解过程省略，来看一下结果，准确度一下子就上来了。这时发现分界线变成了一条直线。这与线性模型有关系吗？推一下看看 Why linear?一些推导 P(C_1|x)=\\frac{P(x|C_1)P(C_1)}{P(x|C_1)P(C_1)+P({x|C_2})P(C_2))} \\\\上下同除以分子 P(C_1|x)=\\frac{1}{1+\\frac{P({x|C_2})P(C_2)}{P(x|C_1)P(C_1)}} 令z=\\ln \\frac{P({x|C_1})P(C_1)}{P(x|C_2)P(C_2)}， 则P(C_1|x)=\\frac{1}{1+\\exp(-z)}=\\sigma(z)sigmoid函数就这样出现了!继续分解z z=\\ln \\frac{P({x|C_1})P(C_1)}{P(x|C_2)P(C_2)}=\\ln \\frac{P({x|C_1})}{P(x|C_2)}+\\ln\\frac{P(C_1)}{P(C_2)} \\lg \\frac{P(C_1)}{P(C_2)}=\\lg \\frac{\\frac{N_1}{N_1+N_2}}{\\frac{N_2}{N_1+N_2}}=\\lg \\frac{N_1}{N2}是个常数 P(x|C_1)=\\frac{1}{2\\pi^{D/2}}\\frac{1}{|\\Sigma|^{1/2}}\\exp{\\{-\\frac{1}{2}(x-\\mu^1)^T(\\Sigma^1)^{-1}(x-\\mu^1)\\}} P(x|C_2)=\\frac{1}{2\\pi^{D/2}}\\frac{1}{|\\Sigma|^{1/2}}\\exp{\\{-\\frac{1}{2}(x-\\mu^2)^T(\\Sigma^2)^{-1}(x-\\mu^2)\\}}带入，则 \\ln \\frac{P({x|C_1})}{P(x|C_2)} = \\ln \\frac{\\frac{1}{2\\pi^{D/2}}\\frac{1}{|\\Sigma|^{1/2}}\\exp{\\{-\\frac{1}{2}(x-\\mu^1)^T(\\Sigma^1)^{-1}(x-\\mu^1)\\}}}{\\frac{1}{2\\pi^{D/2}}\\frac{1}{|\\Sigma|^{1/2}}\\exp{\\{-\\frac{1}{2}(x-\\mu^2)^T(\\Sigma^2)^{-1}(x-\\mu^2)\\}}} =\\ln \\frac{|\\Sigma^2|^{1/2}}{|\\Sigma^1|^{1/2}} +\\ln \\exp\\lgroup -\\frac{1}{2}[(x-\\mu^1)^T(\\Sigma^1)^{-1}(x-\\mu^1)-(x-\\mu^2)^T(\\Sigma^2)^{-1}(x-\\mu^2)]\\rgroup = \\ln \\frac{|\\Sigma^2|^{1/2}}{|\\Sigma^1|^{1/2}} +\\lgroup -\\frac{1}{2}[(x-\\mu^1)^T(\\Sigma^1)^{-1}(x-\\mu^1)-(x-\\mu^2)^T(\\Sigma^2)^{-1}(x-\\mu^2)]\\rgroup = \\ln \\frac{|\\Sigma^2|^{1/2}}{|\\Sigma^1|^{1/2}} +\\lgroup -\\frac{1}{2}[x^T(\\Sigma^1)^{-1}x-2(\\mu^1)^T(\\Sigma^1)^{-1}x+(\\mu^1)^T(\\Sigma^1)^{-1}\\mu^1-x^T(\\Sigma^2)^{-1}x+2(\\mu^1)^T(\\Sigma^2)^{-1}x-(\\mu^2)^T(\\Sigma^2)^{-1}\\mu^2]\\rgroup z= \\ln \\frac{|\\Sigma^2|^{1/2}}{|\\Sigma^1|^{1/2}} +\\lgroup -\\frac{1}{2}[x^T(\\Sigma^1)^{-1}x-2(\\mu^1)^T(\\Sigma^1)^{-1}x+(\\mu^1)^T(\\Sigma^1)^{-1}\\mu^1-x^T(\\Sigma^2)^{-1}x+2(\\mu^1)^T(\\Sigma^2)^{-1}x-(\\mu^2)^T(\\Sigma^2)^{-1}\\mu^2]\\rgroup + \\lg \\frac{N_1}{N2}共用协方差矩阵时， \\Sigma^1=\\Sigma^2=\\Sigma \\ln \\frac{|\\Sigma^2|^{1/2}}{|\\Sigma^1|^{1/2}} =0 x^T(\\Sigma^1)^{-1}x - x^T(\\Sigma^2)^{-1}x=0 此时 z = (\\mu^1-\\mu^2)^T\\Sigma^{-1}x-\\frac{1}{2}(\\mu^1)^T(\\Sigma)^{-1}\\mu^1+\\frac{1}{2}(\\mu^2)^T\\Sigma^{-1}\\mu^2+\\ln \\frac{N_1}{N2} 令w^T=(\\mu^1-\\mu^2)^T\\Sigma^{-1},b=-\\frac{1}{2}(\\mu^1)^T(\\Sigma)^{-1}\\mu^1+\\frac{1}{2}(\\mu^2)^T\\Sigma^{-1}\\mu^2+\\ln \\frac{N_1}{N2}则z=w^T+bP(C_1|x)= \\sigma(z) 是z的广义线性模型，给它一个名字，即logistic regression。 由 \\mu^2,\\mu^2,\\Sigma^1,\\Sigma^2 得到w和b的模型称为生成模型(generative model)，因为各个x可以由一组分布生成出来。 直接找出w和b的模型称为判别模型(discriminative model) logistic regressionStep1, find a function setlogistic regression问题中，我们要找一组参数w和b，σ(wTx+b)给出的值是x属于正例的概率。 Step2, determine goodness of function这里使用似然函数 最小化对数似然函数(NLL)实际上就是最小化交叉熵 Step3, find the best function梯度下降走你 Why likelihood instead of MSE? 若使用MSE，则偏微分的值一直是0，无法进行梯度下降。由图可见，在logistic regression这个问题上，交叉熵(极大似然)确实比MSE的梯度更大，更适合当作评价函数好坏的标准。 多元分类 Step 1以三元分类为例，要做三元分类，要找三组参数 C_1:w^1,b^1 \\text{ } z_1=w^1x+b_1 \\\\ C_2:w^2,b^2 \\text{ } z_2=w^2x+b_2 \\\\ C_3:w^3,b^3 \\text{ } z_3=w^3x+b_3 \\\\ \\mathrm{softmax}: \\vec{z} \\in \\mathbb{R}^n\\to \\vec{y} \\in\\mathbb{R}^n \\\\ \\text{}\\\\\\\\ y_i = \\frac{\\exp{z_i}}{\\sum_{i=1}^3}Step 2 把z经过softmax函数变换后得到的概率分布y与真实值计算交叉熵，即为需要最小化的函数 Step 3梯度下降 线性模型的限制线性模型的函数是直线，而稍微复杂点的问题是无法用直线解决的。例如XOR问题 feature transformation 通过对feature做一些处理，是有可能突破线性模型的限制的。但需要人工参与，而且没有一般规律。这就变成人工学习而不是机器学习了:( feature transformation with linear unit但我们可以把feature transformation看作是一个线性的变化，这样就可以由logistic regression来解决。这样把多个logistic regression串起来，就可以对feature做复杂的变化。 上图是使用两个logistic regression进行feature transformation的结果，只要再接上一个logistic regression接收他俩的输出就可以进行分类了。 Neural network由此推广，把若干个线性模型串起来就可以实现很多复杂的功能。那么给logistic regression的单元起个新名字叫做神经元(neuron)，把它们互相连接形成的结构叫做神经网络(neural network)，瞬间就高大上起来了。 现在你可以骗麻瓜说「我们在模拟人类大脑的运作实现人工智慧」了;-)","categories":[{"name":"Machine learning","slug":"Machine-learning","permalink":"https://verrickt.github.io/categories/Machine-learning/"}],"tags":[{"name":"Machine learning","slug":"Machine-learning","permalink":"https://verrickt.github.io/tags/Machine-learning/"},{"name":"classification","slug":"classification","permalink":"https://verrickt.github.io/tags/classification/"},{"name":"logistic regression","slug":"logistic-regression","permalink":"https://verrickt.github.io/tags/logistic-regression/"},{"name":"neural network","slug":"neural-network","permalink":"https://verrickt.github.io/tags/neural-network/"}]},{"title":"机器学习基础——纸上谈兵","slug":"machine-learning-in-pure-paperworks","date":"2020-07-03T13:29:45.000Z","updated":"2020-07-04T07:52:17.877Z","comments":true,"path":"2020/07/03/machine-learning-in-pure-paperworks/","link":"","permalink":"https://verrickt.github.io/2020/07/03/machine-learning-in-pure-paperworks/","excerpt":"前言本文的主体是Ian Goodfellow 「Deep Learning」第五章「MACHINE LEARNING BASICS」，未经动手实做，故称纸上谈兵。目前奉行Lazy evaluation，对相关知识的补充会在实践后进行。","text":"前言本文的主体是Ian Goodfellow 「Deep Learning」第五章「MACHINE LEARNING BASICS」，未经动手实做，故称纸上谈兵。目前奉行Lazy evaluation，对相关知识的补充会在实践后进行。 what is machine learning A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E The Task T Classification Regression Structured outputThe performance measure P Usually we are interested in how well the machine learning algorithm performson data that it has not seen before, since this determines how well it will work when deployed in the real world. We therefore evaluate these performance measures using a test set of data that is separate from the data used for training the machine learning system. It’s often difficult to choose a performance measure that corresponds well to the desired behavior of the system for two reasons The performance measure can be difficult to decide. When performing a regression task, should we penalize the system more if it frequently makes medium-sized mistakes or if it rarely makes very large mistakes? These kinds of design choices depend on the application we know what quantity we would ideally like to measure, but measuring it is impractical. Computing the actual probability value assigned to a specific point in space in many density estimation models is intractable The Experience EMachine learning algorithms can be broadly categorized as unsupervised or supervised by what kind of experience they are allowed to have during the learning process. Unsupervised learning algorithms experience a dataset containing many features, then learn useful properties of the structure of this dataset. In the context of deep learning, we usually want to learn the entire probability distribution that generated a dataset, whether explicitly as in density estimation or implicitly for tasks like synthesis or denoising. Some other unsupervised learning algorithms perform other roles, like clustering, which consists of dividing the dataset into clusters of similar examples. Supervised learning algorithms experience a dataset containing features,but each example is also associated with a label or target. Roughly speaking, unsupervised learning involves observing several examplesof a random vector x, and attempting to implicitly or explicitly learn the probability distribution p(x), or some interesting properties of that distribution, while supervised learning involves observing several examples of a random vector x and an associated value or vector y, and learning to predict y from x, usually by estimating p(y|x) The term supervised learning originates from the view of the target y being provided by an instructor or teacher who shows the machine learning system what to do. In unsupervised learning, there is no instructor or teacher, and the algorithm must learn to make sense of the data without this guide Some machine learning algorithms do not just experience a fixed dataset. Forexample, reinforcement learning algorithms interact with an environment, so there is a feedback loop between the learning system and its experiences Example: Linear regression The task T: linear regression solves a regression problem. The goal is to build a system that can take a vector x ∈ R^n as input and predict the value of a scalar y ∈ R as its output. \\hat{y}=w^{\\top}x+bw is a vector of the weights over different features. b is the bias(not the bias in statistic) Together ,w and b are called the parameters y is the label of the data. y-hat is the prediction value The performance measurement P: mean squared error(MSE) \\mathrm{MSE_{test}}=\\frac{1}{m}\\sum_i(\\hat{y}^{(test)}-y^{(test)})Capacity, Overfitting and UnderfittingThe central challenge in machine learning is that we must perform well on new, previously unseen inputs—not just those on which our model was trained. The ability to perform well on previously unobserved inputs is called generalization. The factors determining how well a machine learning algorithm will perform are its ability to: Make the training error small. Make the gap between training and test error small. These two factors are underfitting and overfitting. Underfitting occurs when the model is not able to obtain a sufficiently low error value on the training set. Overfitting occurs when the gap between the training error and test error is too large. Informally, a model’s capacity is its ability to fit a wide variety of functions. Models with low capacity may struggle to fit the training set. Models with high capacity can overfit by memorizing properties of the training set that do not serve them well on the test set. Usually,the error mainly comes from bias in the case of underfitting and variance in the case of overfitting. Bayes errorThe error incurred by an oracle making predictions from the true distributionp(x,y) is called the Bayes error. Why are there errors if we know the true probability distribution ? because there may still be some noise in the distribution How about training sizeTraining and generalization error vary as the size of the training set varies.Expected generalization error can never increase as the number of training examples increases. For non-parametric models, more data yields better generalization until the best possible error is achieved. Any fixed parametric model with less than optimal capacity will asymptote to an error value that exceeds the Bayes error. See figure5.4 for an illustration. Note that it is possible for the model to have optimalcapacity and yet still have a large gap between training and generalization error. In this situation, we may be able to reduce this gap by gathering more training examples. Hyperparametershyperparameters are use to control the behavior of the learning algorithm. Eg. In linear regression, we could use a model \\hat{y}=b+\\sum_i^kw_ix^ik controls degree of the polynomial,which acts as a capacity hyperparameter Why hyperparameters? Sometimes a setting is chosen to be a hyperparameter that the learning algorithm does not learn because it is difficult to optimize. The setting must be a hyperparameter because it is not appropriate to learn that hyperparameter on the training set. This applies to all hyperparameters that control model capacity(e.g:k). If learned on the training set, such hyperparameters would always choose the maximum possible model capacity, resulting in overfitting How to adjust hyperparameters? Validation set! RegularizationWe can give a learning algorithm a preference for one solution in itshypothesis space to another. This means that both functions are eligible, but one is preferred.(即西瓜书提到的归纳假设) For example, we can modify the training criterion for linear regression to includeweight decay J(w)=\\mathrm{MSE_{train}}+\\lambda w^\\top wwhere λ is a value chosen ahead of time that controls the strength of our preference for smaller weights. in this sense, λ is also a hyperparameter Frequentist vs baysian Frequentist statistics:we assume that the true parameter value θ is fixed but unknown, while the point estimate θ_hat is a function of the data. Since the data is drawn from a random process, any function of the data is random. Therefore θ_hat is a random variable Bayesian Statistics:the dataset is directly observed and so is not random. On the other hand, the true parameter θ is unknown or uncertain and thus is represented as a random variable. Before observing the data, we represent our knowledge of θ using the prior probability distribution, p(θ). Generally, the machine learning practitioner selects a prior distribution that is quite broad (i.e. with high entropy) to reflect a high degree of uncertainty in the value of θ before observing any data Maximum likelihood1最大似然估计：使得现有观测值出现概率最大的参数θ The maximum likelihood estimator for is defined as \\theta_{ML}=\\argmax _\\theta\\prod_{i=1}^mp_{model}(x^i;\\theta)log-likelihood estimator \\theta_{ML}=\\argmax _\\theta\\sum_{i=1}^m\\log p_{model}(x^i;\\theta)观察KL-Divergence: D_{KL}(\\hat{p}_{data}\\|p_{model})=\\mathbb{E}_{x\\sim \\hat{p}_{data}}[\\log \\hat{p}_{data}(x) - \\log \\hat{p}_{model}(x)]为了最小化KL-Divergence，只需最小化 - \\mathbb{E}_{x\\sim \\hat{p}_{data}} \\log \\hat{p}_{model}(x)]这与最大化log-likelihood是等价的 Simple machine learning algorithmSVMOne of the most influential approaches to supervised learning is the support vector machine This model is similar to logistic regression in that it is driven by a linear function y=w^\\top x+b. The SVM predicts that the positive class is present when y is positive. Likewise, it predicts that the negative class is present when y is negative. One key innovation associated with support vector machines is the kernel trick. The kernel trick consists of observing that many machine learning algorithms can be written exclusively in terms of dot products between examples. For example, it can be shown that the linear function used by the support vector machine can be re-written as w^\\top x+b=b+\\sum_i^m \\alpha_i x^\\top x^{(i)}where x^i is a training example and α is a vector of coefficients.Rewriting the learning algorithm this way allows us to replace x by the output of a given feature function φ(x) and the dot product with a function k(x,x^i) = φ(x)· φ(x^i) called a kernel. The most commonly used kernel is the Gaussian kernel k(u,v)=\\mathbin{N}(u-v;0;\\sigma^2I)this kernel is also known as the radial basis function (RBF) kernel. The Gaussian kernel is performing a kind of template matching. A training example x associated with training label y becomes a template for class y. When a test point x’ is near x according to Euclidean distance, the Gaussian kernel has a large response, indicating that x’ is very similar to the x template. The model then puts a large weight on the associated training label y. Overall, the prediction will combine many such training labels weighted by the similarity of the corresponding training examples Gradient descent improved: stochastic gradient descentIdea: take a step over a ‘minibatch’ instead of observing the whole training-set step cost does not depend on size of training set, thus achieving convergence much faster. A recurring problem in machine learning is that large training sets are necessaryfor good generalization, but large training sets are also more computationally expensive. the insight of stochastic gradient descent is that the gradient is an expectation.The expectation may be approximately estimated using a small set of samples. Specifically, on each step of the algorithm, we can sample a minibatch of examples For a fixed model size, the cost per SGD update does not depend on the training set size m. In practice, we often use a larger model as the training set size increases, but we are not forced to do so. The number of updates required to reach convergence usually increases with training set size. However, as m approaches infinity, the model will eventually converge to its best possible test error before SGD has sampled every example in the training set. Increasing m further will not extend the amount of training time needed to reach the model’s best possible test error. From this point of view, one can argue that the asymptotic cost of training a model with SGD is O(1) as a function of m Why deep learning?curse of dimensionalityMany machine learning problems become exceedingly difficult when the number of dimensions in the data is high. dimension number of states 1 n 2 n^2 3 n^3 Traditional machine learning algorithm can’t distinguish a state that’s not seen in the training set. Local Constancy and Smoothness RegularizationIn order to generalize well, machine learning algorithms need to be guided by prior beliefs about what kind of function they should learn. Among the most widely used of these implicit “priors” is the smoothnessprior or local constancy prior. This prior states that the function we learn should not change very much within a small region.Many simpler algorithms rely exclusively on this prior to generalize well, andas a result they fail to scale to the statistical challenges involved in solving AIlevel tasks. How do we deal with curse of dimensionality?The key insight is that a very large number of regions, e.g., O(2^k), can be defined with O(k) examples, so long as we introduce some dependencies between the regions via additional assumptions about the underlying data generating distribution The core idea in deep learning is that we assume that the data was generated by the composition of factors or features, potentially at multiple levels in a hierarchy. Manifold Learning &amp; manifold hypothesisManifold learning algorithms assuming that most of R^nconsists of invalid inputs, and that interesting inputs occur only alonga collection of manifolds containing a small subset of points. manifold hypothesis: probability distribution over images, text strings, and sounds that occur in real life is highly concentrated. we can also imagine such neighborhoods and transformations, at least informally. In the case of images, we can certainly think of many possible transformations that allow us to trace out a manifold in image space: we can gradually dim or brighten the lights, gradually move or rotate objects in the image, gradually alter the colors on the surfaces of objects 流形假说: 大部分有结构的数据的分布函数并不是分散的，而是集中聚集在某些范围。支持证据：随机取像素试图产生而期待其产生日常生活中的照片的概率是微乎其微的。随机取字母指望它生成一篇文章的概率也是微乎其微的。所以有结构的数据的分布函数必然是在某个范围内聚集，而在大部分区域分散 在流形的表面移动，将可以得到流形所代表的全部数据。如在代表人脸的流形上移动，A点代表微笑的人，B点代表流泪的人。若从直线直接过去，则在A，B中间点所对应的数据可能不是人脸；若从流形的表面移动到B，则数据一直都是人脸。（微笑-&gt;皱眉-&gt;哭泣） 请参考Youtube视频：My understanding of the Manifold Hypothesis | Machine learning","categories":[{"name":"Machine learning","slug":"Machine-learning","permalink":"https://verrickt.github.io/categories/Machine-learning/"}],"tags":[{"name":"Machine learning","slug":"Machine-learning","permalink":"https://verrickt.github.io/tags/Machine-learning/"},{"name":"The flower book","slug":"The-flower-book","permalink":"https://verrickt.github.io/tags/The-flower-book/"}]},{"title":"机器学习基础——数值计算","slug":"machine-learning-basics-numerical-computation","date":"2020-06-28T03:00:27.000Z","updated":"2020-07-04T07:51:28.575Z","comments":true,"path":"2020/06/28/machine-learning-basics-numerical-computation/","link":"","permalink":"https://verrickt.github.io/2020/06/28/machine-learning-basics-numerical-computation/","excerpt":"前言本文的主体是机器学习中所用到的数值计算知识，因此奉行Lazy evaluation，对这些知识更深层次的探究只在绝对必要时完成。","text":"前言本文的主体是机器学习中所用到的数值计算知识，因此奉行Lazy evaluation，对这些知识更深层次的探究只在绝对必要时完成。 Source of errorsOverflow and Underflow计算机的内存是有限的，而有限的内存上无法实现无限精度的数值计算，因此数值计算是有误差的。 接近零的非零数值因为舍入变为0称为Underflow。 超出表示范围的wrap-around称为Overflow positive-overflow 指超过范围的正数被截断为负数 negative-overflow 指超过范围的负数被截断为正数 Poor ConditioningCondition指输入的微小变化引起函数值变化的剧烈程度。计算机中的数字是有表示范围的，因此Condition大的函数会遇到很多问题。 对于 f(\\mathbf{x})=A_{-1}\\mathbf{x},A \\in \\mathbb{R}^{nxn}如果A可以做特征值分解，那么f的condition number定义为 \\max{i,j}\\left|\\frac{\\lambda_i}{\\lambda_j}\\right|Optimization解出使函数f(x)取得最大值/最小值的x的过程称为优化(optimization)。求f(x)最大值可以用求-f(x)的最小值实现，因此主要讨论求最小值。 naive gradient descent梯度(gradient)给出了函数增长最快的方向，那么沿着梯度的反方向就可以将函数值减少。梯度下降法(gradient descent)就是采用这个思路一种方法: x^{i+1} = x^{i} - \\epsilon \\nabla_xf(x)其中ε是学习率(learning rate)。一般来说ε有两种取法 设为一个固定的很小的常量 取多个ε，选其中让f(x-ε∇f(x))最小的那个ε。这种方法又称为line search Beyond the Gradient: Jacobian and Hessian Matrices对于一个输入和输出都是向量的函数f，含有它的所有偏微分的矩阵称为雅可比矩阵(Jacobian matrix) f:\\mathbb{R^m} \\to \\mathbb{R^n}J_{i,j}=\\frac{\\partial}{\\partial x_j}f(x)_i 其中J \\in \\mathbb{R^{n\\times m}}同样的，含有f的所有二阶偏微分的矩阵称为海希矩阵(Hessian matrix) \\mathbf{}{H}(f)(x)_{i,j}=\\frac{\\partial ^2}{\\partial x_i \\partial x_j}f(x)海希矩阵就是梯度的雅可比矩阵。 当二阶偏微分连续时，微分运算符具有交换律，即 H_{i,j}=H_{j,i}机器学习中大部分函数f都具有连续的二阶偏微分，因此海希矩阵是实对称矩阵，因而可以做特征值分解，并且U是正交矩阵。 f对某个单位向量u方向u的二阶偏微分由 u^{\\top} Hu给出。 在x0处对f做二阶泰勒展开 f(x)\\approx f(x^0)+(x-x^0)^{\\top}g+\\frac{1}{2}(x-x^0)^\\top H(x-x^0)H为海希矩阵，g为梯度。当采用学习率ε时，梯度下降的新坐标是x0-εg，带入泰勒展开，有 f(x^0-\\epsilon g) \\approx f(x^0) - \\epsilon g^{\\top}g+\\frac{1}{2}\\epsilon^2g^{\\top}Hg当最后一项小于等于零时，ε可以任选。当最后一项大于零时，有 \\epsilon^*=\\frac{g^{\\top}g}{g^{\\top}Hg}时下降最快。最坏情况下，g与H重合，因此学习率的数量级由 \\frac{1}{\\lambda_{\\max}}决定。 学习率设的过大或过小都会造成问题，这可以由海希矩阵解决。其中最简单的就是牛顿法 牛顿法的步骤基本思路是，在x点做二阶泰勒展开。 f(x)\\approx f(x^0)+(x-x^0)^{\\top}g+\\frac{1}{2}(x-x^0)^\\top H(x-x^0)解出函数的critical point x^*=x^0-H(f)(x^0)^{-1}\\nabla_xf(x^0)f是正定二次函数时，一次就找到了最低点。当f不是二次函数但是可以用二次函数局部近似时，多次迭代x*即可。 牛顿法只适用于解最小值，而梯度下降没有这个限制。 只用到梯度的优化算法，如梯度下降，叫做一阶优化算法。使用海希矩阵的算法，如牛顿法，叫做二阶优化算法。 Constrained optimization有时我们希望在某集合S上而不是R^n上找出函数的最大/最小值，这叫做constrainted optimization(受限优化)。落在s中的点称为feasible point(可行点) KKT方法是一个受限优化问题的通用方法。使用KKT方法，首先定义广义拉格朗日函数。 为了定义广义拉格朗日函数，首先需要将S定义为函数的集合 \\mathbb{S}=\\{x| \\forall i,g^{(i)}(x)=0 \\text{ and } \\forall j,h^{(j)}(x) \\le0 \\}含有g(i)的方程称为equality constraints，含有h(j)的方程称为inequality constraints。 对于每个函数，定义λi和αj。他们叫做KKT乘数。 现在定义广义拉格朗日函数 L(x,\\lambda,\\alpha)=f(x)+\\sum_i \\lambda_i g^{(i)}(x)+\\sum_j \\alpha_jh^{(j)}(x)接下来在广义拉格朗日函数上使用非受限优化算法，就可以解决受限优化算法了。","categories":[{"name":"Machine learning","slug":"Machine-learning","permalink":"https://verrickt.github.io/categories/Machine-learning/"}],"tags":[{"name":"Machine learning","slug":"Machine-learning","permalink":"https://verrickt.github.io/tags/Machine-learning/"},{"name":"The flower book","slug":"The-flower-book","permalink":"https://verrickt.github.io/tags/The-flower-book/"},{"name":"Numberical computation","slug":"Numberical-computation","permalink":"https://verrickt.github.io/tags/Numberical-computation/"}]},{"title":"机器学习基础——概率论与信息论","slug":"machine-learning-basics-probability-and-information-theory","date":"2020-06-26T14:25:05.000Z","updated":"2020-06-27T12:01:07.788Z","comments":true,"path":"2020/06/26/machine-learning-basics-probability-and-information-theory/","link":"","permalink":"https://verrickt.github.io/2020/06/26/machine-learning-basics-probability-and-information-theory/","excerpt":"前言本文的主体是机器学习中所用到的概率论知识，因此奉行Lazy evaluation，对这些知识更深层次的探究只在绝对必要时完成。本文假设你学过《概率论与数理统计》，仅指出花书的现有定义的不同，同时补充花书中特有的知识","text":"前言本文的主体是机器学习中所用到的概率论知识，因此奉行Lazy evaluation，对这些知识更深层次的探究只在绝对必要时完成。本文假设你学过《概率论与数理统计》，仅指出花书的现有定义的不同，同时补充花书中特有的知识 ProbabilityConditional probabilityP(A|B)=\\frac{P(AB)}{P(B)}Chain rule/product rule of conditiona probability P(x^{(1)},...,x^{(n)})=P(x^{(1)})\\prod_{i=2}^{n}P(x^{1}|x^{(1)},...,x^{(n)})eg P(a,b,c)=P(a|b,c)P(b,c)P(b,c)=P(b|c)P(c)P(a,b,c)=P(a|b,c)P(b|c)P(c)IndependenceIndenpendence: \\forall x \\in x,y \\in y,p(x=x,y=y)=p(x=x)p(y=y)则x,y互相独立。记作 x \\perp yConditional independent: \\forall x \\in x,y \\in y ,z\\in z,p(x=x,y=y|z=z)=p(x=x|z=z)p(y=y|z=z)称x,y在z下相互独立，记作 x \\perp y \\mid zCovarianceCov(f(x),g(y))=\\mathbb{E}[(f(x))-\\mathbb{E}[f(x)])(g(x))-\\mathbb{E}[g(x)]]协方差值越高，意味着f和g的变化非常大，并且同时距离各自的均值很远。如果协方差是正值，那么f和g倾向于同时相对大的值。如果是负值，则一个取高值的同时另一个取低值。 向量 x \\in \\mathbb{R}^n的covariance matrix(协方差矩阵)是一个nxn的方阵，其中 Cov(x)_{i,j}=Cov(x_i,x_j)对于对角线的元素， Cov(x_i,x_i)=Var(x_i)Gaussian Distribution高斯分布，又称正态分布。概率密度函数(PDF): \\mathcal{N}(x;\\mu,\\sigma)=\\sqrt{\\frac{1}{2 \\pi \\sigma^2}}exp(-\\frac{1}{2\\sigma^2}(x-\\mu)^2)计算概率密度时，经常要取σ的平方倒数，工程中常使用另一个参数β∈(0,∞)表示高斯分布的精准度 \\mathcal{N}(x;\\mu,\\beta^{-1})=\\sqrt{\\frac{\\beta}{2 \\pi \\sigma^2}}exp(-\\frac{1}{2}\\beta(x-\\mu)^2)高斯分布的特点 现实中很多复杂的系统可以由高斯分布建模（中心极限定理） 在方差相同的所有分布中，高斯分布在实数范围上的“不确定度”最高。换句话说，高斯分布是所有分布中对样本做出最少先验假设的 N维高斯分布： \\mathcal{N}(x;\\mu,\\Sigma)=\\sqrt{\\frac{1}{(2\\pi)^n det(\\Sigma)}}exp(-\\frac{1}{2}(x-\\mu)^T\\Sigma^{-1}(x-\\mu))其中Σ是正定对称矩阵。μ是矢量形式的分布均值，Σ给出分布的协方差矩阵。为了便于计算，对于N维高斯分布，常用准确度矩阵β作为参数： \\mathcal{N}(x;\\mu,\\beta^{-1})=\\sqrt{\\frac{det(\\beta)}{(2\\pi)^n }}exp(-\\frac{1}{2}(x-\\mu)^T\\beta(x-\\mu))实践上通常将协方差矩阵固定为对角阵。更简单的方式是将isotropic matrix作为协方差矩阵，其中isotropic matrix指标量数乘单位矩阵的结果。 Dirac delta distribution &amp; empirical distribution有时我们希望所有的概率密度都聚集在一个点附近。这可以通过Dirac delta函数 $\\delta(x)$$$实现： $$p(x)=\\delta(x-\\mu)Dirac delta分布常常被用作empirical distribution（经验分布）的一个组件： \\hat{p}(x)=\\frac{1}{m}\\sum_{i=1}^m\\delta(x-x^{(i)})empirical distribution在全部m个点 x^{(1)},...x^{(m)}上放置 \\frac{1}{m}概率密度 Mixture distribution用其他简单的概率分布来定义概率分布是十分普遍的，混合分布（mixture distribution)就是这样一种方式。混合分布由好几个组件(component)组成。每次采样时，由一个多重分布的结果选择组件标识(component identity)，由此最终结果是由哪一个分布给出的。 P(x)=\\sum_iP(c=i)P(x\\mid c=i)其中P(c)是所有组件上的多重分布。 一种常见且强大的混合模型是高斯混合模型。高斯混合模型所有组件都是高斯分布，他们具有不同的参数μ和Σ。有些分布可以增加限制，如所有组件共用协方差矩阵等。 高斯混合分布是概率密度的通用近似方式。具有足够分量的高斯混合模型可以用任何特定的非零误差量来近似任何平滑密度。 PS，如果用高斯分布来做分类问题，让两个分布共用协方差矩阵Σ通常效果会比使用各自的协方差矩阵效果好。 Latent variable隐含变量（Latent variable)指无法直接观测的随机变量。混合分布中的组件标识变量c就是隐含变量。 P(x,c)=P(x\\mid c)P(c)隐含变量的分布P(c)和条件分布P(x|c)共同决定了P(x)的分布。尽管P(x)可以在没有隐含变量的条件下被计算出来。 Useful properties of Common Functionslogistic sigmoid: \\sigma(x)=\\frac{1}{1+exp(-x)}softplus: \\zeta(x)=log(1+exp(x)) why the name softplus?It’s a “softeded” version of x^=max(0,x) Bayes’ Rule已知P(y|x)和P(x)求P(x|y)时可以使用贝叶斯公式： P(x|y)=\\frac{P(x)P(y \\mid x)}{P(y)}其中 P(y)=\\sum_xP(y \\mid x)P(x)prior-probability and posterior-probability prior-probability即先验概率。指根据以往经验和分析得到的概率 posterior-probability后验概率是在考虑和给出相关证据或数据后所得到的条件概率 考虑bayes’ Rule P(\\theta \\mid x)=\\frac{P(x\\mid\\theta)P(\\theta)}{P(x)} θ：parameter x：observed value P(x)：evidence P(θ)：prior P(x|θ)：likelihood P(θ|x)：posterior PDF of y where y=g(x)假设有现随机变量x，y，其中y=g(x)，求y的PDF PDF根据pdf的定义, P(x)_{x \\in \\delta}=\\int_{x}p(x)dxp(x)dx为x落在某一邻域δ内的概率。现保留该属性，则有 |p_y(g(x))dy|=|p_x(x)dx|p_y(y)=p_x(g^{-1}(y))\\mid \\frac{\\partial x}{\\partial y} \\midp_x(x)=p_y(g(x))\\mid \\frac{\\partial g(x)}{\\partial y} \\mid考虑高维情况，x与y为向量，定义雅可比矩阵J，其中 J_{i,j}=\\frac{\\partial x_i}{\\partial y_j}则 p_x(x)=p_y(g(x))\\mid \\det \\left\\{ \\frac{\\partial g(x)}{\\partial x}\\right\\} \\midInformation theorybasic assemption: Likely events should have low information content, and in the extreme case, events that are guaranteed to happen should have no information content whatsoever. Less likely events should have higher information content. Independent events should have additive information. For example, finding out that a tossed coin has come up as heads twice should convey twice as much information as finding out that a tossed coin has come up as heads once. Self-information &amp; Shanon entropy单一事件所含的信息，单位为nat I(x)=-\\log P(x)整个概率分布上的不确定性，即香农熵 H(x)=E_{x\\sim P}[I(x)]=-\\mathbb{E}_{x\\sim P}[\\log P(x)]也记作H(P). 当P(x)和Q(x)为相同随机变量x的分部时，两种分布间的“距离”用Kullback-Leibler (KL) divergence定义： D_{KL}(P\\|Q)=\\mathbb{E}_{x\\sim P}\\left[\\log\\frac{P(x)}{Q(x)}\\right]=\\mathbb{E}_{x\\sim p}[\\log P(x) - \\log Q(x)]the value means the extra amount of informationneeded to send a message containing symbols drawn from probability distribution P, when we use a code that was designed to minimize the length of messages drawn from probability distribution . KL divergence non-negarive not symmetricD_{KL}(P\\|Q)\\not ={D_{KL}(Q\\|P)} 与KL divergence密切相关的一种度量是cross-entropy定义为 H(P,Q) = H(P)+D_{KL}(P\\|Q)=-\\mathbb{E}_{x\\sim P}\\log Q(x)Minimizing the cross-entropy with respect to Q is equivalent to minimizing the KL divergence, because Q does not participate in the omitted term. Structed Probabilistic Models机器学习中的参数成千上万，使用含有这么多参数的分布不切实际。根据条件概率的乘法公式，可以把大分布拆成小分布的乘积（这一过程叫factorization)。当使用CS中的图来表示这种factorization时，就把模型称为structured probabilistic model或graphical model。structed probabilistic model分为两类，分别使用DAG和UAG。 DAGDirected models use graphs with directed edges, and they represent factorizations into conditional probability distributions, Specifically, a directed model contains one factor for every random variable xi in the distribution p(\\mathbf{x})=\\prod_ip(x_i|Pa\\mathcal{G}(x_i))where Pa\\mathcal{G}(x_i)is the parents of xi. UAGUndirected models use graphs with undirected edges, and they representfactorizations into a set of functions; unlike in the directed case, these functions are usually not probability distributions of any kind. Any set of nodes that are all connected to each other in G is called a clique. Each clique Ci in an undirected model is associated with a factor φi . These factors are just functions, not probability distributions. The output of each factor must be non-negative, but there is no constraint that the factor must sum or integrate to 1 like a probability distribution. The probability of a configuration of random variables is proportional to theproduct of all of these factors—assignments that result in larger factor values are more likely. Of course, there is no guarantee that this product will sum to 1. We therefore divide by a normalizing constant Z, defined to be the sum or integral over all states of the product of the φ functions, in order to obtain a normalized probability distribution: p(x)=\\frac{1}{Z}\\prod_i\\phi^{(i)}(\\mathcal{C}^{(i)})DAG和UAG都是描述概率分布的方法，他们并不是互斥的概率分布。使用DAG还是UAG并不是概率分布的属性，而是某种特定描述方式的属性","categories":[{"name":"Machine learning","slug":"Machine-learning","permalink":"https://verrickt.github.io/categories/Machine-learning/"}],"tags":[{"name":"Machine learning","slug":"Machine-learning","permalink":"https://verrickt.github.io/tags/Machine-learning/"},{"name":"The flower book","slug":"The-flower-book","permalink":"https://verrickt.github.io/tags/The-flower-book/"},{"name":"Probability","slug":"Probability","permalink":"https://verrickt.github.io/tags/Probability/"},{"name":"Information theory","slug":"Information-theory","permalink":"https://verrickt.github.io/tags/Information-theory/"}]},{"title":"机器学习基础——线性代数","slug":"machine-learning-basics-linear-algebra","date":"2020-06-24T01:38:56.000Z","updated":"2020-06-27T12:00:34.274Z","comments":true,"path":"2020/06/24/machine-learning-basics-linear-algebra/","link":"","permalink":"https://verrickt.github.io/2020/06/24/machine-learning-basics-linear-algebra/","excerpt":"前言本文的主体是机器学习中所用到的线性代数知识，因此奉行Lazy evaluation，对这些知识更深层次的探究只在绝对必要时完成。","text":"前言本文的主体是机器学习中所用到的线性代数知识，因此奉行Lazy evaluation，对这些知识更深层次的探究只在绝对必要时完成。 ConceptsTensor矩阵是向量的延申，而Tensor(张量)则是矩阵的延申。向量可用一维数组表示，矩阵可用二维数组表示，而张量可用三维数组表示。表示张量A的一个分量 A_{i,j,k}Identify matrixIdentity matrix为单位矩阵的另一名称。一般用I表示，而单位矩阵用E表示。 matrix inversion花书定义： A^{-1}A=I并未限定A为方阵，并且是左逆 花书也定义了右逆 AA^{-1}=I同样未限定A为方阵 而后说明，方阵的左逆和右逆是相同的 For square matrices, the left inverse and right inverse are equal 而课本认为非方阵没有逆矩阵。对左逆和右逆也不做显式区分。 在花书接下来提到逆矩阵时，这可能是一个坑点 Singular matrix非满秩的方阵称为奇异矩阵。 Norms又称范式。范式是一个将向量映射为非负数量的函数，用以描述向量的“大小”。 正式定义： f(x) = 0 ⇒ x = 0 f(x+y) ≤ f(x) + f(y)(三角不等式) ∀α∈R,f(αx) = |α|f(x) 常用的范式: L2 norm向量与原点的距离 ||x||_2=(\\sum_{i} |x_i|^2)^{\\frac{1}{2}} Squared L2 normL2 norm的平方形式，某些情况下易于分析和计算 L1 norm ||x||_1=\\sum_{i} |x_i| max norm ||x||_{\\infty }={\\max} |x_i| Frobenius norm计算矩阵的“大小” ||A||_F=\\sqrt{(\\sum_{i,j} A_{i,j}^2)}OperationsDecomposition将矩阵分解为某些更基础的成分可以更好的帮助我们分析其中的一般规律。 Eigendecomposition基于特征值和特征向量分解。与课本相同，不再赘述。 Singular value decomposition(SVD)又称奇异值分解。对矩阵的形状没有要求，且任意实矩阵都可以做奇异值分解，因而比特征值分解更具通用性。 对于一个m*n的矩阵A，奇异值分解希望将其分解为三个矩阵的乘积： A=UDV^T U是大小m*m的正交矩阵 D是大小m*n的对角矩阵 V是大小n*n的正交矩阵 D中对角线的元素称为A的奇异值。U的列向量称为A的左奇异向量，V的列向量称为A的右奇异向量 A的奇异值分解可以理解为「A的函数」的特征值分解： A的左奇异值向量是AA^T的特征向量。 A的右奇异值向量是A^TA的特征向量 A的非零奇异值是A^TA的特征值，也是AA^T的特征值 SVD的一个应用：部分场景下将矩阵求逆推广到非方阵，如下 The Moore-Penrose Pseudoinverse又称摩尔－彭若斯广义逆（好长的名字…） A^+=\\lim_{\\alpha \\searrow 0}(A^TA+\\alpha I)^{-1}A^{T}实际上，一般用 A^+=VD^+U^T来计算广义逆 U，D，V是A的奇异值分解。D的广义逆D+由非零元素取倒数，然后转置得到。 当A是Ax=y的系数矩阵时，若x有解，则x=A+y是所有解中具有最小L2范式的那一个。 若x无解，Ax给出了L2范式中y-Ax的最小值 Trace又称迹，是矩阵主对角线元素的和。 \\mathrm{Tr}(A)=\\sum_i{A_{i,i}}迹和矩阵乘法可以代替一些需要求和符号的操作： ||A||_F=\\sqrt{\\mathrm{Tr(AA^T)}}矩阵的迹具有循环不变性： \\mathrm{Tr(ABC)=Tr(CAB)=Tr(BCA)}更通用来说， \\mathrm{Tr}(\\prod_{i=1}^nF^{(i)})=\\mathrm{Tr}(F^{(n)}\\prod_{i=1}^{n-1}F^{(i)})","categories":[{"name":"Machine learning","slug":"Machine-learning","permalink":"https://verrickt.github.io/categories/Machine-learning/"}],"tags":[{"name":"Machine learning","slug":"Machine-learning","permalink":"https://verrickt.github.io/tags/Machine-learning/"},{"name":"Linear algebra","slug":"Linear-algebra","permalink":"https://verrickt.github.io/tags/Linear-algebra/"},{"name":"The flower book","slug":"The-flower-book","permalink":"https://verrickt.github.io/tags/The-flower-book/"}]},{"title":"HTML渲染为UWP的原生控件","slug":"render-html-natively-in-uwp","date":"2020-04-21T13:24:32.000Z","updated":"2020-07-11T14:18:07.693Z","comments":true,"path":"2020/04/21/render-html-natively-in-uwp/","link":"","permalink":"https://verrickt.github.io/2020/04/21/render-html-natively-in-uwp/","excerpt":"没啥用的前言说着再做UWP就剁手，我还是开了一个新坑🤣这次是B岛的UWP端论坛客户端的一个老大难问题是内容的呈现。论坛一般以网页端为主，网页做好，论坛活跃起来后之后才会开发客户端/有开发者愿意做第三方的客户端。因此，API绝大多数情况是为网页端为一等公民的。此外，各个UI框架展示内容的格式也各有不同。以上两个原因导致HTML被选做富文本展示的通用语言。","text":"没啥用的前言说着再做UWP就剁手，我还是开了一个新坑🤣这次是B岛的UWP端论坛客户端的一个老大难问题是内容的呈现。论坛一般以网页端为主，网页做好，论坛活跃起来后之后才会开发客户端/有开发者愿意做第三方的客户端。因此，API绝大多数情况是为网页端为一等公民的。此外，各个UI框架展示内容的格式也各有不同。以上两个原因导致HTML被选做富文本展示的通用语言。 对于客户端来说，HTML的呈现就成了问题。可以嵌入浏览器来渲染HTML，但存在两个难以解决的问题 与应用原生部分交互困难， 可能有性能问题。 因此，客户端的做法一般是绕过WebView，将HTML直接渲染为原生控件。那么问题来了，怎么做呢？Android的TextView可以渲染部分HTML，但UWP里就没有相应的API了。//@微软，出来挨打 先来看看手上有什么工具。首先是原生的XAML控件。Windows SDK 1903与.Net starndard 2.0兼容。又有一大堆.NET Standard 2.0的类库可以用了。 API返回的结果是1报个BUG。主岛的API p模式的data2，文档说小于1按1处理，实际上取0时返回的是'['，&lt;br /&gt;&lt;br /&gt;curl http://bog.ac/api/p/0/0&lt;br /&gt;] 尝试搜索搜索一下，找到了WinRT-RichTextBlock.Html2Xaml，它能把HTML渲染到RichTextBlock上，但是很遗憾，它不支持UWP。继续搜索，找到HTML2XAML的一个支持UWP的fork，使用后发现有不支持的标签。查看他的代码，似乎用到了xslt。面对HTML已经够头疼了，还是别引入另一个标记语言了。出师不利。 继续搜索，找到了一个MarkdownTextBlock的库。我以前做另一个论坛的客户端时用过它。当时是先想办法把HTML转成Markdown，再用它来渲染。但是在处理多级嵌套引用(&lt;quote&gt;)的时候会出错。况且时隔这么久我已经看不懂当年写的代码了🤣 通过这两次搜索我们得到了以下信息： RichTextBlock很可能能够作为我们渲染的容器 HTML标签和使用的原生控件有关 结构化的输出处理起来更方便，如果能把HTML转化为DOM树，靠dfs就可以实现转换。 第一步，先要把HTML结构化。 结构化HTML要把某种语言结构化，Parser是不二选择。而HTML的Parser因为经常面对残缺的HTML，通常支持将残缺的片段补齐。AngleSharp就是一个基于.NET Standard的HTML parser。第一步搞定。有了结构，接下来就顺手多了。 遍历DOM树上面把HTML转成Markdown的源函数，里面的一大堆分支看的云里雾里。加上奇奇怪怪的边界情况后更是让人头疼。有没有一种代码的组织方法能让我针对一个标签写一个函数？答案是：Visitor pattern. 百科上写的详细的多，我就只举一个例子。假设有&lt;p&gt;,&lt;img&gt;,&lt;a&gt;标签需要解析。定义INode作为所有DOM元素的接口,IVisitor是要对元素访问的接口。INode的实现者通过visitor.Visit(this)把控制流返还给Visitor。只需在visitor上实现对各个类的Visit方法，就达成目的。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950interface INode&#123; void Accept(IVisitor visitor);&#125;interface IVisitor&#123; void Visit(ANode node); void Visit(PNode node); void Visit(ImgNode node);&#125;class PNode:INode&#123; public IReadonlyList&lt;INode&gt; Children&#123;get;&#125; public void Accept(IVisitor visitor) &#123; visitor.Visit(this); &#125;&#125;class ANode:INode&#123; public string Href&#123;get;&#125; public void Accept(IVisitor visitor) &#123; visitor.Visit(this); &#125;&#125;class ImgNode:INode&#123; public string src&#123;get;&#125; public void Accept(IVisitor visitor) &#123; visitor.Visit(this); &#125;&#125;public class DOMVisitor:IVisitor&#123; public void Visit(PNode node) &#123; foreach(var item in node.Children) item.Accept(this); &#125; public void Visit(ANode node) &#123; //new Hyper link &#125; public void Visit(ImgNode node) &#123; //new Image &#125;&#125; RichTextBlock接下来考虑如何呈现。根据文档，RichTextBlock可以包含多个Block，一个Block又可以包含若干Inline。与HTML标签刚好对应！InlineUIContainer自己是Inline，但Child属性可以塞下任何UIElement。如果塞进去另一个RichTextBlock就实现了对引用&lt;quote&gt;呈现。这样可以实现任意级引用&lt;quote&gt;的呈现。具体实现上，提供一个Stack&lt;Block&gt;供使用。转化为Inline的元素每次添加到栈顶的Block中。遇到转化为Block的元素则压栈。最后把Block按照先后顺序加入RichTextBlock即可。 下面是效果，具体代码请参考HTML Parser和RichTextBlockRenderer 搞定了一个困扰多年的难题，可喜可贺(＾o＾)ﾉ","categories":[],"tags":[{"name":"UWP","slug":"UWP","permalink":"https://verrickt.github.io/tags/UWP/"},{"name":"HTML","slug":"HTML","permalink":"https://verrickt.github.io/tags/HTML/"}]},{"title":"PAT 1097 Deduplication on a Linked List","slug":"PAT-1097-Deduplication-on-a-Linked-List","date":"2020-04-03T05:31:51.000Z","updated":"2020-04-03T06:15:54.728Z","comments":true,"path":"2020/04/03/PAT-1097-Deduplication-on-a-Linked-List/","link":"","permalink":"https://verrickt.github.io/2020/04/03/PAT-1097-Deduplication-on-a-Linked-List/","excerpt":"Deduplication on a Linked ListGiven a singly linked list L with integer keys, you are supposed to remove the nodes with duplicated absolute values of the keys. That is, for each value K, only the first node of which the value or absolute value of its key equals K will be kept. At the mean time, all the removed nodes must be kept in a separate list. For example, given L being 21→-15→-15→-7→15, you must output 21→-15→-7, and the removed list -15→15.","text":"Deduplication on a Linked ListGiven a singly linked list L with integer keys, you are supposed to remove the nodes with duplicated absolute values of the keys. That is, for each value K, only the first node of which the value or absolute value of its key equals K will be kept. At the mean time, all the removed nodes must be kept in a separate list. For example, given L being 21→-15→-15→-7→15, you must output 21→-15→-7, and the removed list -15→15. Input Specification Each input file contains one test case. For each case, the first line contains the address of the first node, and a positive N (≤10^​5​​) which is the total number of nodes. The address of a node is a 5-digit nonnegative integer, and NULL is represented by −1. Then N lines follow, each describes a node in the format:1Address Key Next where Address is the position of the node, Key is an integer of which absolute value is no more than 10​^4​​, and Next is the position of the next node. Output Specification For each case, output the resulting linked list first, then the removed list. Each node occupies a line, and is printed in the same format as in the input. Sample Input12345600100 599999 -7 8765423854 -15 0000087654 15 -100000 -15 9999900100 21 23854 Sample Output1234500100 21 2385423854 -15 9999999999 -7 -100000 -15 8765487654 15 -1 类似这样的骚操作网上有很多，这里不在引用。本文主要考虑常规思路。思路非常简单，便利链表，如果曾经出现过就插入另一个链表，没出现过就继续，最后得到两个链表，分别是去重后的和重复的。看似比较简单，想把所有情况考虑全也是要花点心思的。 重复部分的链表是和原链表顺序是一样的，采用尾插法。需要头、尾两个指针。 原链表的头节点是第一个节点，不可能被放入重复链表，那么去重后的链表头节点不变。 遍历过程中需要将节点删除(即插入重复链表)，所以需要prev指针记录去重链表的上一个节点地址，以便修改next指针。 如果你像我一样以为上面的描述就把所有的情况都考虑完全了，那么欢迎你和我来到BUG的海洋😉 先来看AC的代码 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364#include \"vector\"#include \"iostream\"#include \"algorithm\"using namespace std;struct node&#123; int addr; int key; int next;&#125;;node nodes[100086];bool map[10024];void print(int head)&#123; auto cur = head; while (cur != -1) &#123; if (nodes[cur].next != -1) printf(\"%05d %d %05d\\n\", cur, nodes[cur].key, nodes[cur].next); else printf(\"%05d %d -1\\n\", cur, nodes[cur].key); cur = nodes[cur].next; &#125;&#125;int main()&#123; int head, n; cin &gt;&gt; head &gt;&gt; n; for (int i = 0; i &lt; n; i++) &#123; int id; cin &gt;&gt; id; nodes[id].addr = id; cin &gt;&gt; nodes[id].key &gt;&gt; nodes[id].next; &#125; fill(map, map + 10024, false); int dupli_head = -1; int dupli_tail = -1; int cur = head; int prev = -1; while (cur != -1) &#123; while (map[abs(nodes[cur].key)]) &#123; int next = nodes[cur].next; if (dupli_head == -1) dupli_head = dupli_tail = cur; else &#123; nodes[dupli_tail].next = cur; dupli_tail = cur; nodes[cur].next = -1; &#125; cur = next; if (cur == -1) break; &#125; if (cur == -1) &#123; nodes[prev].next = -1; break; &#125; if (prev != -1) nodes[prev].next = cur; map[abs(nodes[cur].key)] = true; prev = cur; cur = nodes[cur].next; &#125; print(head); print(dupli_head); return 0;&#125; 为啥while循环里又套了个while循环，用if不就够了么？ 此乃第一坑😉上文说到 prev指针记录去重链表的上一个节点地址 那么prev的next应该是去重链表的下一个地址。而if得到的仅仅是下一个地址，此时还不知道他是否属于去重链表。这时不能做prev.next=cur。那什么时候可以呢？当然是map[abs(nodes[cur].key)为假啦。 好多-1看的我头晕 恭喜你，进入了第二坑😉 dupli_head和dupli_tail是头尾指针，-1作为链表为空时的特殊值。 插入dupli链表后，要把cur.next置为-1，表示这是新链表的最后一个节点。为防止链表断裂，用next=cur.next记住下一个地址。 插入重复链表是在while循环里进行的，那么就有可能在这里耗光所有的元素。此即if (cur == -1) break;。 当在while循环里耗尽所有元素后，已经不可能有新的元素可以作为prev.next，因此if (cur == -1) { nodes[prev].next = -1; break; } 上文说过，原链表的头节点一定是去重链表的头节点。那么当刚访问过头节点时，是不需要对prev操作的。即if (prev != -1) nodes[prev].next = cur;。当然，你也可以直接跳过头节点123map[head]=true;cur = nodes[head].next;prev = head; 怎么样，还觉得考虑所有情况简单么😉","categories":[],"tags":[{"name":"PAT","slug":"PAT","permalink":"https://verrickt.github.io/tags/PAT/"}]},{"title":"PAT 1021 Deepest Root","slug":"PAT-1021-Deepest-Root","date":"2020-03-23T13:32:29.000Z","updated":"2020-03-23T14:29:30.905Z","comments":true,"path":"2020/03/23/PAT-1021-Deepest-Root/","link":"","permalink":"https://verrickt.github.io/2020/03/23/PAT-1021-Deepest-Root/","excerpt":"Deepest RootA graph which is connected and acyclic can be considered a tree. The height of the tree depends on the selected root. Now you are supposed to find the root that results in a highest tree. Such a root is called the deepest root.","text":"Deepest RootA graph which is connected and acyclic can be considered a tree. The height of the tree depends on the selected root. Now you are supposed to find the root that results in a highest tree. Such a root is called the deepest root. Input Specification Each input file contains one test case. For each case, the first line contains a positive integer N (≤10​^4​​) which is the number of nodes, and hence the nodes are numbered from 1 to N. Then N−1 lines follow, each describes an edge by given the two adjacent nodes’ numbers. Output Specification For each test case, print each of the deepest roots in a line. If such a root is not unique, print them in increasing order of their numbers. In case that the given graph is not a tree, print Error: K components where K is the number of connected components in the graph. Sample Input 1 1234551 21 31 42 5 Sample Output 1123345 Sample Input 21234551 31 42 53 4 Sample Output 21Error: 2 components 去年连答案都看不懂的题在今天发愣的时候突然想出来，真是可喜可贺。 题目要求是求树高，但其实树高其实和求路径长度是一致的：在简单图里，只要定了起点和终点，路径就是从根到叶节点的轨迹。求某点的最大路径长度和全图强连通分量可以用dfs一并解决，问题转化为求全图的最大路径长度 简单粗暴的穷举会被几个测试点卡时间，所以需要一些“聪明”的小办法。仔细思考，穷举被卡时间的原因应该是做了大量重复的计算。如果能区分哪些计算是不必要的，就可以解决问题。沿着这个思路，考虑两条最长路径是否有关系。考虑十字型的图，两条最长路径相交于图上一点，这么看来最长路径之间很可能有交点。就从交点出发吧。 考虑连通图的情况。若A为图上一点，B为图上另一点。两点间的最长路径，记作MaxPath(A,B) 设S为图上任意一点，从S出发的最长路径的终点为P。那么P点的出度一定为0：若出度不为0，总可以找到更长的路径。从而与S，P之间路径为最长矛盾。 设从P点出发的最长路径的终点为Q，则MaxPath(P,Q)一定经过S点：假设这条路不经过S点。设T为属于Path(P,Q)且不属于Path(P,S)的任意一点。因为图联通，则S，T间存在长度至少为1的路径。那么S，P间经过T的路径长度比Path(S,P)更长，与Path(S,P)是S和P点间最长路径的假设矛盾。所以MaxPath(P,Q)一定经过S点。而P点和Q点必定一个入度为0，一个出度为0(P，Q位于图的两端)。因此，Path(P,Q)即为整张图的最长路径。 回到本题，要求所有最长路径的起始节点，任选一个节点S，所有使路径最长的P都是路径的起始节点。需要注意的是，当Path(P，Q)在最长路径时，P点和Q点都可以作为节点。这是因为路径是可以反向的。求出所有的P，Q去重排序后即可。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748#include \"cstdio\"#include \"vector\"#include \"algorithm\"#include \"set\"using namespace std;int graph[10086][10086];int n;int max_depth = -1;vector&lt;int&gt; roots;set&lt;int&gt; ans = set&lt;int&gt;();bool visited[10086];void dfs(int source, int depth)&#123; visited[source] = true; if (depth &gt; max_depth) &#123; max_depth = depth; roots.clear(); &#125; if (depth == max_depth) roots.push_back(source); for (int i = 1; i &lt;= n; i++) if (graph[source][i] == 1 &amp;&amp; !visited[i]) dfs(i, depth + 1);&#125;int main()&#123; scanf(\"%d\", &amp;n); for (int i = 1; i &lt; n; i++) &#123; int j, k; scanf(\"%d %d\", &amp;j, &amp;k); graph[j][k] = graph[k][j] = 1; &#125; fill(visited, visited + 10086, false); int cnt = 0; for (int i = 1; i &lt;= n; i++) &#123; if (!visited[i]) &#123; dfs(i, 0); cnt++; &#125; &#125; if (cnt != 1) &#123; printf(\"Error: %d components\\n\", cnt); return 0; &#125; for (auto i : roots) ans.insert(i); fill(visited, visited + 10086, false); dfs(roots[0], 0); for (auto i : roots) ans.insert(i); for (auto i : ans) printf(\"%d\\n\", i); return 0;&#125;","categories":[],"tags":[{"name":"PAT","slug":"PAT","permalink":"https://verrickt.github.io/tags/PAT/"}]},{"title":"重学「冒泡排序」","slug":"yeah-that-naive-bubble-sort","date":"2020-03-17T06:22:12.000Z","updated":"2020-03-17T09:47:08.570Z","comments":true,"path":"2020/03/17/yeah-that-naive-bubble-sort/","link":"","permalink":"https://verrickt.github.io/2020/03/17/yeah-that-naive-bubble-sort/","excerpt":"PAT里链表题有各式各样的骚操作。这些非常规操作易学易用，但是习惯了这些后，反而对题目真正想考察的知识生疏了。今天就碰到一道这样的题，想用正经的算法写却写不出来。希望大家以我为戒，不要过多的学习这些「奇技淫巧」","text":"PAT里链表题有各式各样的骚操作。这些非常规操作易学易用，但是习惯了这些后，反而对题目真正想考察的知识生疏了。今天就碰到一道这样的题，想用正经的算法写却写不出来。希望大家以我为戒，不要过多的学习这些「奇技淫巧」 Linked List Sorting这道题很常规，是个很一般的链表排序题。链表不能随机访问，因此算法上的选择限制很大，只有冒泡、插入、选择排序可以选。我选了冒泡练手，没想到没做出来。 冒泡排序的关键步骤是交换两个相邻的逆序元素。每完成一趟后都有一个元素被顺利的放到了最终位置上。 按我的想法，i in (0,n),j in (i+1,n)，每次j走到n时，就把「最小」的元素放到了i的位置，依次增加i就得到了升序排列。很遗憾，这是错的123for (int i = 0; i &lt; n; i++) for (int j = i + 1; j &lt; n; j++) if (n[j] &lt; n[j-1]) swap(n[j], n[j-1]); 在[5,3,1,2,6,7]上考虑这段代码，if语句能保证，只要n[j]比n[j-1]大，就把这个大的往后移。。而对于最小值1，仅仅在j=2的时候被swap到了n[1]，并没有真正的移动到n[0]这个我所期待的位置。观察j扫描的方向也是向后的。 再看另一种写法123for (int i = 0; i &lt; n; i++) for (int j = n-1; j &gt; i; j--) if (n[j] &lt; n[j-1]) swap(n[j], n[j-1]); 这时if语句能保证的是只要n[j]比n[j-1]小，就把这个小的往前移。在[5,3,1,2,6,7]，第一轮之后1就被放在了n[0]上。注意到j的移动方向是向前的。 我们想得到升序的序列，那么就要把大的往后移，小的往前移。j从前往后移动的过程中，我们是把最大的元素往后移一位，在下一次比较时最大的元素还会继续被移往后方，最终放在a[n-1]同理，j从后往前的过程中，最小的元素会被一直前移，直到a[0]。 因此一趟完成后到底是最大的元素被放到最后了还是最小的元素被放到最前了，要看j移动的方向。在链表中从前往后是方便的，因而最大的元素会被先放到最后。所以我们需要做的是，每一趟减少j移动结束的范围。 想到这里基本上就能理清了。加上处理头节点的一些细节后，不难写出代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263#include \"cstdio\"#include \"vector\"using namespace std;struct node &#123; int addr; int key; int next;&#125;;vector&lt;node&gt; nodes = vector&lt;node&gt;(100086);int main()&#123; int n, head; scanf(\"%d %d\", &amp;n, &amp;head); for (int i = 0; i &lt; n; i++) &#123; int addr, key, next; scanf(\"%d %d %d\", &amp;addr, &amp;key, &amp;next); nodes[addr] = node(&#123; addr,key,next &#125;); &#125; int newHead = head; int end = -1; while (end != newHead) &#123; int t = newHead; int pre = -1; while (nodes[t].next != end) &#123; int next = nodes[t].next; if (nodes[t].key &gt; nodes[next].key) &#123; if (pre != -1) nodes[pre].next = next; else newHead = next; nodes[t].next = nodes[next].next; nodes[next].next = t; &#125; pre = t; t = next; &#125; end = t; &#125; int h = newHead; int cnt = 0; while (h != -1) &#123; cnt++; h = nodes[h].next; &#125; if (cnt == 0) printf(\"0 -1\\n\"); else printf(\"%d %05d\\n\", cnt, newHead); h = newHead; while (h != -1) &#123; if (nodes[h].next == -1) printf(\"%05d %d -1\\n\", nodes[h].addr, nodes[h].key); else printf(\"%05d %d %05d\\n\", nodes[h].addr, nodes[h].key, nodes[h].next); h = nodes[h].next; &#125; return 0;&#125; 欢天喜地的去提交，发现有一个点超时了。诚然，冒泡排序的时间复杂度是Θ(n^2)，非常规操作里用std::sort处理后输出的时间复杂度是Θ(nlogn)，超时是理所应当。放眼望去绝大多数AC的代码都用的与这种投机取巧的方法。不知道这是不是违背出题人的初衷呢？ 当然，这问题轮不到我评头论足，我只是个连冒泡排序都忘得一干二净的渣渣🙂 总结： 冒泡排序一趟后被放到最终位置的元素与扫描方向有关 基本功要打扎实，切忌「眼高手低」","categories":[],"tags":[{"name":"PAT","slug":"PAT","permalink":"https://verrickt.github.io/tags/PAT/"}]},{"title":"PAT-1038 Recover the Smallest Number","slug":"PAT-1038-Recover-the-Smallest-Number","date":"2020-03-16T04:04:00.000Z","updated":"2020-03-17T06:23:51.266Z","comments":true,"path":"2020/03/16/PAT-1038-Recover-the-Smallest-Number/","link":"","permalink":"https://verrickt.github.io/2020/03/16/PAT-1038-Recover-the-Smallest-Number/","excerpt":"Recover the Smallest NumberGiven a collection of number segments, you are supposed to recover the smallest number from them. For example, given { 32, 321, 3214, 0229, 87 }, we can recover many numbers such like 32-321-3214-0229-87 or 0229-32-87-321-3214 with respect to different orders of combinations of these segments, and the smallest number is 0229-321-3214-32-87.","text":"Recover the Smallest NumberGiven a collection of number segments, you are supposed to recover the smallest number from them. For example, given { 32, 321, 3214, 0229, 87 }, we can recover many numbers such like 32-321-3214-0229-87 or 0229-32-87-321-3214 with respect to different orders of combinations of these segments, and the smallest number is 0229-321-3214-32-87.Input Specification Each input file contains one test case. Each case gives a positive integer N (≤10​^4​​) followed by N number segments. Each segment contains a non-negative integer of no more than 8 digits. All the numbers in a line are separated by a space. Output Specification For each test case, print the smallest number in one line. Notice that the first digit must not be zero. Sample Input15 32 321 3214 0229 87 Sample Output122932132143287 这道题以前就没做对，这次也没做对，记录一下。 两次读完题的反应完全一致：谁“小”把谁排前面。 若两个数位数相同，按数值比较 若两个数位数不同，位数少的数字补0至位数相同，按数值比较。 我的cmp是这么写的12345678bool cmp(string a, string b)&#123; string p1 = a, p2 = b; int diff = p1.size() - p2.size(); if (diff &lt; 0) swap(p1, p2); for (int i = 0; i &lt; abs(diff); i++) p2.push_back('0'); return p1 &lt; p2;&#125; Debug模式在运行时报错，Assert failed: Invalid comparor. 根据cppreference，comparer需满足 反自反.对于所有的a,comp(a,a)==false 反对称.若comp(a,b)==true，则comp(b,a)==false 传递 若comp(a,b)==true，comp(b,c)==true,则comp(a,c)==true. 我们的comparer违背了反对称性。comp(9,10)===comp(10,9) 查到string的&lt;操作符按照字典序进行比较。而我们辛辛苦苦的补0，就是要按字典序比较。 事实证明字典序也不对。考虑32和321，按照字典序，32&lt;321，但这样拼接的32321却比32132要大。似乎还要考虑字符串的首位和末位。 查了别人的代码，最好的解决办法是，把问题原样丢给C++1234bool cmp(string s1, string s2)&#123; return s1 + s2 &gt; s2 + s1;&#125; 我居然还想了半天，整个人都不好了😶 12345678910111213141516171819202122232425262728#include \"string\"#include \"vector\"#include \"iostream\"#include \"algorithm\"using namespace std;bool cmp(string s1, string s2)&#123; //sort by lexicographical order doesn't work here. 33&lt;321,but 33321 &gt; 32133. :-( //but we can throw the problem at cpp :-) auto p1 = s1; auto p2 = s2; return p1+p2 &lt; p2+p1;&#125;int main()&#123; int n; cin &gt;&gt; n; vector&lt;string&gt; v=vector&lt;string&gt;(n); for (int i = 0; i &lt; n; i++) cin &gt;&gt; v[i]; sort(v.begin(), v.end(), cmp); string res; for (auto i : v) res.append(i); int i = 0; while (res[i] == '0') i++; res = res.substr(i); if (res.size() == 0) res = \"0\"; cout &lt;&lt; res &lt;&lt; endl; return 0;&#125;","categories":[],"tags":[{"name":"PAT","slug":"PAT","permalink":"https://verrickt.github.io/tags/PAT/"}]},{"title":"PAT 1007 Maximum Subsequence Sum","slug":"PAT-1007-Maximum-Subsequence-Sum","date":"2020-03-11T14:12:04.000Z","updated":"2020-03-11T14:57:10.182Z","comments":true,"path":"2020/03/11/PAT-1007-Maximum-Subsequence-Sum/","link":"","permalink":"https://verrickt.github.io/2020/03/11/PAT-1007-Maximum-Subsequence-Sum/","excerpt":"Maximum Subsequence SumGiven a sequence of K integers { N​1​​, N​2​​, …, N​K​​ }. A continuous subsequence is defined to be { N​i​​, N​i+1​​, …, N​j​​ } where 1≤i≤j≤K. The Maximum Subsequence is the continuous subsequence which has the largest sum of its elements. For example, given sequence { -2, 11, -4, 13, -5, -2 }, its maximum subsequence is { 11, -4, 13 } with the largest sum being 20. Now you are supposed to find the largest sum, together with the first and the last numbers of the maximum subsequence.","text":"Maximum Subsequence SumGiven a sequence of K integers { N​1​​, N​2​​, …, N​K​​ }. A continuous subsequence is defined to be { N​i​​, N​i+1​​, …, N​j​​ } where 1≤i≤j≤K. The Maximum Subsequence is the continuous subsequence which has the largest sum of its elements. For example, given sequence { -2, 11, -4, 13, -5, -2 }, its maximum subsequence is { 11, -4, 13 } with the largest sum being 20. Now you are supposed to find the largest sum, together with the first and the last numbers of the maximum subsequence. Input Specification Each input file contains one test case. Each case occupies two lines. The first line contains a positive integer K (≤10000). The second line contains K numbers, separated by a space. Output Specification For each test case, output in one line the largest sum, together with the first and the last numbers of the maximum subsequence. The numbers must be separated by one space, but there must be no extra space at the end of a line. In case that the maximum subsequence is not unique, output the one with the smallest indices i and j (as shown by the sample case). If all the K numbers are negative, then its maximum sum is defined to be 0, and you are supposed to output the first and the last numbers of the whole sequence. Sample Input 1210-10 1 2 3 4 -5 -23 3 7 -21 Sample Output110 1 4 分数线死活不出来，一为排解焦虑，二为准备复试，建了个小号重刷PAT。最大子列和这个问题自己想不出来，听别人的想法似懂非懂，抄的代码放久了还是看不懂，让我头疼不已。今天在往常的思路上多想了一步，迎刃而解。记录一番，以示庆祝。 根据历次的印象，应该用动规。根据历次的印象，应该用前一段的最大子列和与当前元素作为状态转换方程的参数。好了，到了这一步就卡住了。 以往我的思路是，新的子列和=旧子列和+最后一项。旧子列和和最后一项可正可负，但只要最后一项是正的，那么子列和就严格递增。可是这样的思路在后面是-2,5的时候就行不通了。以前走到这一步我就去求助搜索引擎了。 这次在上面的思路上稍微走了一步。加法是有交换律的。不妨把式子改写成新的子列和=最后一项+旧子列和。不考虑最后一项的正负，而去考虑旧子列和的正负。 如果旧子列和为正，那么不管最后一项符号如何，新子列和一定比最后一项要大。 如果旧子列和为负，那么不管最后一项符号如何，新子列和一定比最后一项要小。 如果一个新子列和比最后一项还要小，取仅含最后一项的子序列，则它比新子列和还要大。这时的新子列和并不是最大子列和。舍弃之。 把最后一项当作子列的第一项，重复上述的过程，则可以得到几个连续序列的正数和。最大子列和就藏在这些正数和当中。问题转化求几个数的最大值。这还用想？😛 针对这道题的要求改一改，就得到答案啦12345678910111213141516171819202122232425262728293031323334#include &lt;vector&gt;#include &lt;cstdio&gt;#include &lt;iostream&gt;using namespace std;int main()&#123; int k; cin &gt;&gt; k; vector&lt;int&gt; v = vector&lt;int&gt;(k); for (int i = 0; i &lt; k; i++) scanf(\"%d\", &amp;v[i]); int i=0, sum = 0; int max = -1; sum = 0; int p = 0, q = k-1; int s = 0; for(int i=0;i&lt;k;i++) &#123; sum += v[i]; if (sum&gt;max) &#123; max = sum; p = s; q = i; &#125; if (sum&lt;0) &#123; s = i+1; sum = 0; &#125; &#125; if (max &lt; 0) max = 0; printf(\"%d %d %d\", max, v[p], v[q]); return 0;&#125;","categories":[],"tags":[{"name":"PAT","slug":"PAT","permalink":"https://verrickt.github.io/tags/PAT/"}]},{"title":"「C++」踩坑","slug":"cpp-the-double-edged-sword","date":"2019-08-21T14:41:50.000Z","updated":"2020-03-11T14:09:05.124Z","comments":true,"path":"2019/08/21/cpp-the-double-edged-sword/","link":"","permalink":"https://verrickt.github.io/2019/08/21/cpp-the-double-edged-sword/","excerpt":"因为PAT刷题的缘故,开始再次接触C++.用过OOP语言再回来用C十分不习惯,C++就顺手多了.C没有泛型,字符串也不好用,C++就好多了,STL的容器和模板完美结合,字符串类也堪用.","text":"因为PAT刷题的缘故,开始再次接触C++.用过OOP语言再回来用C十分不习惯,C++就顺手多了.C没有泛型,字符串也不好用,C++就好多了,STL的容器和模板完美结合,字符串类也堪用. 按照欲抑先扬的套路,下面就该说说缺点了.作为老资历的语言,C++有些部分真是让人摸不着头脑.比如非void函数可以不返回值12345int do_something(int i)&#123; int j = i+1; // returns nothing;&#125; 我就在这上面被坑了.BST的构建函数忘记返回值,调了一个多小时.1234567891011121314151617181920node* insert(node* root,int data)&#123; if(root==nullptr) &#123; root = (node*)malloc(sizeof(node)); root-&gt;left = nullptr; root-&gt;right = nullptr; root-&gt;data=data; return root; &#125; if(data&lt;root-&gt;data) &#123; root-&gt;left = insert(root-&gt;left,data); &#125; else &#123; root-&gt;right = insert(root-&gt;right,data); &#125; //return root //掉了这一句&#125; 自己的机器上跑一直没问题,一提交就错.抓瞎搞了一个多小时,又被devcpp的智能提示搞得心态爆炸,切换回了Visual Studio开始调试.结果依旧,不过编译的输出里多了一行Warning1warning C4715: &apos;insert&apos;: not all control paths return a value 把掉了的return语句加回去,AC了. StackExchange上有人问了同样的问题 I forgot to write return 'a'; in function and return 0; in main function but its works fine in Code::Blocks. 12345678910#include &lt;iostream&gt;using namespace std;char show()&#123; cout&lt;&lt;&quot;this is show function&quot;&lt;&lt;endl;&#125;int main()&#123; show();&#125; i am using Code::Blocks 10.05 in Ubuntu 12.04. Why this happen and why does the same thing cause an error in TURBO C++? Answer: 123If a function is declared to return a value, and fails to do so, the result is undefined behavior (in C++). One possible result is seeming to work, which is pretty much what you&apos;re seeing here.As an aside, in C, you wouldn&apos;t actually have undefined behavior -- in C, you get undefined behavior only if you try to use the return value, and the function didn&apos;t specify a value to return. 在其他的语言里,一般会直接报错编译失败,在C++里就变成了编译器警告.去devcpp鼓捣了一下,把warning打开了.我要是选个带VS的考场就不用折腾devcpp了.真的难用. 想到初学编程的时候,编辑器和IDE党的圣战.一方说平均水平高,一方说效率高,争起来无休无止的口水战. 不过现在嘛,因为 好用的IDE成倍提升效率. IDE降低了上下文切换的开销,但对工具链不熟悉还是会吃亏. 搞熟了工具链的我选择IDE. 不过tab和space的扣税战到现在还没结束,IDE和编辑器的口水战不知道还要打多少年呢","categories":[],"tags":[{"name":"CPP","slug":"CPP","permalink":"https://verrickt.github.io/tags/CPP/"}]},{"title":"Head of a Gang","slug":"PAT-1034-Head-of-a-Gang","date":"2019-08-20T06:22:48.000Z","updated":"2020-03-11T14:10:05.447Z","comments":true,"path":"2019/08/20/PAT-1034-Head-of-a-Gang/","link":"","permalink":"https://verrickt.github.io/2019/08/20/PAT-1034-Head-of-a-Gang/","excerpt":"Head of a gangHead of a Gang One way that the police finds the head of a gang is to check people’s phone calls. If there is a phone call between A and B, we say that A and B is related. The weight of a relation is defined to be the total time length of all the phone calls made between the two persons. A “Gang” is a cluster of more than 2 persons who are related to each other with total relation weight being greater than a given threthold K. In each gang, the one with maximum total weight is the head. Now given a list of phone calls, you are supposed to find the gangs and the heads.","text":"Head of a gangHead of a Gang One way that the police finds the head of a gang is to check people’s phone calls. If there is a phone call between A and B, we say that A and B is related. The weight of a relation is defined to be the total time length of all the phone calls made between the two persons. A “Gang” is a cluster of more than 2 persons who are related to each other with total relation weight being greater than a given threthold K. In each gang, the one with maximum total weight is the head. Now given a list of phone calls, you are supposed to find the gangs and the heads. Input Specification: Each input file contains one test case. For each case, the first line contains two positive numbers N and K (both less than or equal to 1000), the number of phone calls and the weight threthold, respectively. Then N lines follow, each in the following format: 1Name1 Name2 Time where Name1 and Name2 are the names of people at the two ends of the call, and Time is the length of the call. A name is a string of three capital letters chosen from A-Z. A time length is a positive integer which is no more than 1000 minutes. Output Specification: For each test case, first print in a line the total number of gangs. Then for each gang, print in a line the name of the head and the total number of the members. It is guaranteed that the head is unique for each gang. The output must be sorted according to the alphabetical order of the names of the heads.Sample Input 1: 1234567898 59AAA BBB 10BBB AAA 20AAA CCC 40DDD EEE 5EEE DDD 70FFF GGG 30GGG HHH 20HHH FFF 10 Sample Output 1:1232AAA 3GGG 3 Sample Input 2:1234567898 70AAA BBB 10BBB AAA 20AAA CCC 40DDD EEE 5EEE DDD 70FFF GGG 30GGG HHH 20HHH FFF 10 Sample Output 2:10 题目大意:警方要抓捕犯罪分子.打过电话的两个人有关系,关系的权重是通话时间.一组有关系的人,如果关系的权重和超过阈值K,并且有大于两个人,就认为是一个犯罪团伙.通话时间最长的人是头目.现在给定一组通话记录,求其中的犯罪团伙的个数,以及各个头目的名字. 思路这是一道图论的综合题.首先要把通话记录中的名字变成下标,可以用map&lt;string,int&gt;实现.然后要求图的连通分量个数.对于每个连通分量,求该连通分量的边权和,出边权值和最大的节点,连通分量的大小. 边权和可将所有边权求和除以2得出,出边权值在访问邻接点时可以计算.正好和dfs结合起来. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115#include &lt;iostream&gt;#include &lt;vector&gt;#include &lt;map&gt;#include &lt;string&gt;#include &lt;queue&gt;#include &lt;algorithm&gt;using namespace std;int n,k;int weight[4000][4000];map&lt;string,int&gt; f;int ind=0;bool visited[4000];struct p&#123; string name; int size;&#125;;vector&lt;p&gt; result;bool cmp(p p1,p p2)&#123; return p1.name&lt;p2.name;&#125;int next_index(string str)&#123; if(f.find(str)==f.end()) &#123; f[str]=ind; ind++; &#125; return f[str];&#125;bool bfs(int index,int&amp; max_index,int&amp; size)&#123; queue&lt;int&gt; q; q.push(index); int counter = 0; int sum=0; int max_sum=-1; max_index=-1; visited[index]=true; while(!q.empty()) &#123; int f = q.front(); int acc=0; q.pop(); counter++; for(int i=0;i&lt;n;i++) &#123; if(weight[f][i]!=0) &#123; if(!visited[i]) &#123; visited[i]=true; q.push(i); &#125; sum+=weight[f][i]; acc+=weight[f][i]; &#125; &#125; if(acc&gt;max_sum) &#123; max_sum=acc; max_index=f; &#125; &#125; size = counter; return counter&gt;2&amp;&amp;sum&gt;k*2; &#125; int main()&#123; cin&gt;&gt;n&gt;&gt;k; for(int i=0;i&lt;n;i++) &#123; int w; string s1,s2; cin&gt;&gt;s1&gt;&gt;s2&gt;&gt;w; int n1 = next_index(s1); int n2 = next_index(s2); weight[n1][n2]+=w; weight[n2][n1]+=w; &#125; fill(visited,visited+n,false); for(auto it=f.begin();it!=f.end();it++) &#123; if(!visited[it-&gt;second]) &#123; int max_index; int size; if(bfs(it-&gt;second,max_index,size)) &#123; for(auto ii=f.begin();ii!=f.end();ii++) &#123; if(ii-&gt;second==max_index) &#123; p pp; pp.name=ii-&gt;first; pp.size=size; result.push_back(pp); &#125; &#125; &#125; &#125; &#125; cout&lt;&lt;result.size()&lt;&lt;endl; sort(result.begin(),result.end(),cmp); for(auto p:result) cout&lt;&lt;p.name&lt;&lt;\" \"&lt;&lt;p.size&lt;&lt;endl;&#125; 在构建从人名到下标的映射的时候可以顺便构建从下标到人名的反向映射.可以省去输出时遍历f中所有元素的反向查找,我这里就偷懒了:-P","categories":[],"tags":[{"name":"PAT","slug":"PAT","permalink":"https://verrickt.github.io/tags/PAT/"}]},{"title":"1018 Public Bike Management","slug":"PAT-1018-Public-Bike-Management","date":"2019-08-16T14:50:47.000Z","updated":"2020-06-27T12:05:32.708Z","comments":true,"path":"2019/08/16/PAT-1018-Public-Bike-Management/","link":"","permalink":"https://verrickt.github.io/2019/08/16/PAT-1018-Public-Bike-Management/","excerpt":"Public Bike Management1018 Public Bike Management There is a public bike service in Hangzhou City which provides great convenience to the tourists from all over the world. One may rent a bike at any station and return it to any other stations in the city.","text":"Public Bike Management1018 Public Bike Management There is a public bike service in Hangzhou City which provides great convenience to the tourists from all over the world. One may rent a bike at any station and return it to any other stations in the city.The Public Bike Management Center (PBMC) keeps monitoring the real-time capacity of all the stations. A station is said to be in perfect condition if it is exactly half-full. If a station is full or empty, PBMC will collect or send bikes to adjust the condition of that station to perfect. And more, all the stations on the way will be adjusted as well. When a problem station is reported, PBMC will always choose the shortest path to reach that station. If there are more than one shortest path, the one that requires the least number of bikes sent from PBMC will be chosen. The above figure illustrates an example. The stations are represented by vertices and the roads correspond to the edges. The number on an edge is the time taken to reach one end station from another. The number written inside a vertex S is the current number of bikes stored at S. Given that the maximum capacity of each station is 10. To solve the problem at S​3​​, we have 2 different shortest paths: PBMC -&gt; S​1​​ -&gt; S​3​​. In this case, 4 bikes must be sent from PBMC, because we can collect 1 bike from S​1​​ and then take 5 bikes to S​3​​, so that both stations will be in perfect conditions. PBMC -&gt; S​2​​ -&gt; S​3​​. This path requires the same time as path 1, but only 3 bikes sent from PBMC and hence is the one that will be chosen. Input Specification: Each input file contains one test case. For each case, the first line contains 4 numbers: C​max​​ (≤100), always an even number, is the maximum capacity of each station; N (≤500), the total number of stations; S​p​​, the index of the problem station (the stations are numbered from 1 to N, and PBMC is represented by the vertex 0); and M, the number of roads. The second line contains N non-negative numbers C​i​​ (i=1,⋯,N) where each C​i​​ is the current number of bikes at S​i​​ respectively. Then M lines follow, each contains 3 numbers: S​i​​, S​j​​, and T​ij​​ which describe the time T​ij​​ taken to move betwen stations S​i​​ and S​j​​. All the numbers in a line are separated by a space. Output Specification: For each test case, print your results in one line. First output the number of bikes that PBMC must send. Then after one space, output the path in the format: 0−&gt;S​1​​−&gt;⋯−&gt;S​p​​. Finally after another space, output the number of bikes that we must take back to PBMC after the condition of S​p​​ is adjusted to perfect. Note that if such a path is not unique, output the one that requires minimum number of bikes that we must take back to PBMC. The judge’s data guarantee that such a path is unique. Sample Input:123456710 3 3 56 7 00 1 10 2 10 3 31 3 12 3 1 Sample Output:13 0-&gt;2-&gt;3 0 题目大意:公共自行车站点的自行车数量是半满的时候称为完美.不完美的站点就需要调度.给定起点和终点,找到最短的一条路线,并把沿途经过的站点全部调度为完美.如果有多条路线存在,选择从起点带出自行车最小的那一条.最后输出从起点带出的自行车数,路线,从终点带回的自行车数. 思路单源最短路径问题dijkstra是跑不掉了.问题的关键在于,当松弛的对象v的最短路径预期dist[v]和当前最短路径于权值的和dist[min]+edge(min,v)相等时候的取舍.一个简单的想法是,出现一致时,计算最短路上平均节点的点权,取最大的那一条.很不幸的是,dijkstra执行过程中并不知道所要比较的两条最短路的长度. 参考了其他人的思路,与其在还未生成最短路径时想办法取舍,不如等到所有的最短路径都求出来以后再去选最优的那一条.原版算法中记录前驱结点的int path[]就要改成vector&lt;int&gt; pre[]了.松弛成功时,清空path[i],放入min.权值相同时,放入min. 1234567891011121314151617181920212223242526272829303132333435363738394041void dijkstra()&#123; bool visited[504]; fill(visited,visited+n+1,false); fill(estimate,estimate+n+1,inf); estimate[0]=0; while(true) &#123; int min=-1; int minn=inf; for(int i=0;i&lt;=n;i++) &#123; if(estimate[i]&lt;minn&amp;&amp;visited[i]==false) &#123; minn=estimate[i]; min=i; &#125; &#125; if(minn==inf) break; visited[min]=true; for(int i=0;i&lt;=n;i++) &#123; if(t[min][i]!=-1&amp;&amp;visited[i]==false) &#123; if(estimate[i]&gt;estimate[min]+t[min][i]) &#123; estimate[i]=estimate[min]+t[min][i]; path[i].clear(); path[i].push_back(min); &#125; else if(estimate[i]==estimate[min]+t[min][i]) &#123; path[i].push_back(min); &#125; &#125; &#125; &#125;&#125; 跑完dijkstra,path[i]的笛卡尔积就是所有的最短路径. 现在问题转化为,给定好几条路径,如何选出最优的那一条.dfs帮了大忙.1234567891011121314151617181920212223242526272829303132333435363738394041424344454647void dfs(int v)&#123; temp_path.push_back(v); if(v==0) &#123; int need=0; int back=0; for(int i=temp_path.size()-1;i&gt;=0;i--) &#123; int id=temp_path[i]; if(bycicle[id]&gt;0) &#123; back+=bycicle[id]; &#125; else &#123; if(back&gt;(0-bycicle[id])) &#123; back+=bycicle[id]; &#125; else &#123; need+=((0-bycicle[id])-back); back=0; &#125; &#125; &#125; if(need&lt;min_need) &#123; min_need=need; min_back=back; min_path=temp_path; &#125; else if(need==min_need&amp;&amp;back&lt;min_back) &#123; min_back=back; min_path=temp_path; &#125; temp_path.pop_back(); return; &#125; else&#123; for(auto i:path[v]) &#123; dfs(i); &#125; &#125; temp_path.pop_back();&#125; 进入dfs后二话不说先将当前节点放入temp_path,在返回之前再将其弹出.这保证了,调用dfs(sp)后,只要v是起点,temp_path就是一条完整的路径(倒序).倒过来就能推出带出和带回的自行车数. 当节点的自行车数与完美状态的差值bicycle[i]=s[i]-cmax/2为正时,在离开该节点时需要带走这么多车子.back+=bicycle[i]. 当bicycle[i]为负时,考虑两种情况. back比bicycle[i]的绝对值大时,表示携带的自行车比当前节点要多.只需分给该节点|bibycle[i]|这么多的自行车即可达成完美.back+=bicycle[i] 当back比bicycle[i]的绝对值要小时,表示携带的自行车不够分,需要从起点携带.两者的差值就是所需的车辆数.need+=(0-bicycle[i])-back,携带的车辆数置零back=0 选出need和back最小的路线即可. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133#include \"cstdio\"#include \"vector\"#include \"iostream\"using namespace std;int cmax;int n;int sp;int m;int bycicle[504];int t[504][504];const int inf = 0xFFFFFFF;vector&lt;int&gt; path[504];int estimate[504];vector&lt;int&gt; min_path;int min_need=inf;int min_back=inf;vector&lt;int&gt; temp_path;void dfs(int v)&#123; temp_path.push_back(v); if(v==0) &#123; int need=0; int back=0; for(int i=temp_path.size()-1;i&gt;=0;i--) &#123; int id=temp_path[i]; if(bycicle[id]&gt;0) &#123; back+=bycicle[id]; &#125; else &#123; if(back&gt;(0-bycicle[id])) &#123; back+=bycicle[id]; &#125; else &#123; need+=((0-bycicle[id])-back); back=0; &#125; &#125; &#125; if(need&lt;min_need) &#123; min_need=need; min_back=back; min_path=temp_path; &#125; else if(need==min_need&amp;&amp;back&lt;min_back) &#123; min_back=back; min_path=temp_path; &#125; temp_path.pop_back(); return; &#125; else&#123; for(auto i:path[v]) &#123; dfs(i); &#125; &#125; temp_path.pop_back();&#125;void dijkstra()&#123; bool visited[504]; fill(visited,visited+n+1,false); fill(estimate,estimate+n+1,inf); estimate[0]=0; while(true) &#123; int min=-1; int minn=inf; for(int i=0;i&lt;=n;i++) &#123; if(estimate[i]&lt;minn&amp;&amp;visited[i]==false) &#123; minn=estimate[i]; min=i; &#125; &#125; if(minn==inf) break; visited[min]=true; for(int i=0;i&lt;=n;i++) &#123; if(t[min][i]!=-1&amp;&amp;visited[i]==false) &#123; if(estimate[i]&gt;estimate[min]+t[min][i]) &#123; estimate[i]=estimate[min]+t[min][i]; path[i].clear(); path[i].push_back(min); &#125; else if(estimate[i]==estimate[min]+t[min][i]) &#123; path[i].push_back(min); &#125; &#125; &#125; &#125;&#125;int main()&#123; cin&gt;&gt;cmax&gt;&gt;n&gt;&gt;sp&gt;&gt;m; for(int i=1;i&lt;=n;i++) &#123; scanf(\"%d\",bycicle+i); bycicle[i]-=cmax/2; &#125; for(int i=0;i&lt;=n;i++) for(int j=0;j&lt;=n;j++) t[i][j]=-1; for(int i=0;i&lt;m;i++) &#123; int si,sj,tij; scanf(\"%d %d %d\",&amp;si,&amp;sj,&amp;tij); t[si][sj]=tij; t[sj][si]=tij; &#125; dijkstra(); dfs(sp); printf(\"%d 0\",min_need); for(int i=min_path.size()-2;i&gt;=0;i--) &#123; printf(\"-&gt;%d\",min_path[i]); &#125; printf(\" %d\\n\",min_back); &#125; 一点小想法 用dfs外加辅助存储可以实现类似笛卡尔积的效果.比排列组合要方便不少 经典算法之所以经典的原因在于,很多问题可以用经典算法的变体解决.适用面大,所以经典. 在计算back和need时又用到了递推.给递推打个星⭐","categories":[],"tags":[{"name":"PAT","slug":"PAT","permalink":"https://verrickt.github.io/tags/PAT/"}]},{"title":"回顾排队问题","slug":"Queueing-Problem-Revisited","date":"2019-08-16T14:27:51.000Z","updated":"2019-08-16T14:50:02.630Z","comments":true,"path":"2019/08/16/Queueing-Problem-Revisited/","link":"","permalink":"https://verrickt.github.io/2019/08/16/Queueing-Problem-Revisited/","excerpt":"刷PAT的过程中遇到了两类排队问题,这里总结一下 所有人在同一时刻到达,如1014 Waiting in Line 每个人到达的时刻不同,如1017 Queueing at Bank)","text":"刷PAT的过程中遇到了两类排队问题,这里总结一下 所有人在同一时刻到达,如1014 Waiting in Line 每个人到达的时刻不同,如1017 Queueing at Bank) 解决这类问题一个比较好的思路是递推.对同一时刻到达的问题,可以每个窗口设置front和rear两个变量,分别表示队首和队尾离队时间.front值最小的队列就是应该插入的队列.入队元素的离队时间可以由rear+=current.duration递推给出.队首元素离队后,原来第二个元素成为新的队首元素.更新front+=header.duration. 对到达时刻不同的问题,给队列设置一个finish,表示完成服务的时间.顾客来排队时,先找最小finish的队列.如果finish比他来的时间还早,那么他不需要等待,直接入队就好.更新finish为current.time+current.duration;如果finish比他来的时间晚,首先要等待finish-current.time这么久的时间.更新finish+=current.duration. 看懂了柳神的解法,再看自己1014的解法,十分惭愧.要继续努力啊🤠","categories":[],"tags":[{"name":"PAT","slug":"PAT","permalink":"https://verrickt.github.io/tags/PAT/"}]},{"title":"PAT 1014 Waiting in Line","slug":"PAT-1014-Waiting-in-Line","date":"2019-08-14T12:56:09.000Z","updated":"2019-08-14T13:21:38.240Z","comments":true,"path":"2019/08/14/PAT-1014-Waiting-in-Line/","link":"","permalink":"https://verrickt.github.io/2019/08/14/PAT-1014-Waiting-in-Line/","excerpt":"Waiting in Line1014 Waiting in Line Suppose a bank has N windows open for service. There is a yellow line in front of the windows which devides the waiting area into two parts. The rules for the customers to wait in line are:","text":"Waiting in Line1014 Waiting in Line Suppose a bank has N windows open for service. There is a yellow line in front of the windows which devides the waiting area into two parts. The rules for the customers to wait in line are: The space inside the yellow line in front of each window is enough to contain a line with M customers. Hence when all the N lines are full, all the customers after (and including) the (NM+1)st one will have to wait in a line behind the yellow line. Each customer will choose the shortest line to wait in when crossing the yellow line. If there are two or more lines with the same length, the customer will always choose the window with the smallest number. Customer​i​​ will take T​i​​ minutes to have his/her transaction processed. The first N customers are assumed to be served at 8:00am. Now given the processing time of each customer, you are supposed to tell the exact time at which a customer has his/her business done. For example, suppose that a bank has 2 windows and each window may have 2 custmers waiting inside the yellow line. There are 5 customers waiting with transactions taking 1, 2, 6, 4 and 3 minutes, respectively. At 08:00 in the morning, customer​1​​ is served at window​1​​ while customer​2​​ is served at window​2​​. Customer​3​​ will wait in front of window​1​​ and customer​4​​ will wait in front of window​2​​. Customer​5​​ will wait behind the yellow line. At 08:01, customer​1​​ is done and customer​5​​ enters the line in front of window​1​​ since that line seems shorter now. Customer​2​​ will leave at 08:02, customer​4​​ at 08:06, customer​3​​ at 08:07, and finally customer​5​​ at 08:10. Input Specification:Each input file contains one test case. Each case starts with a line containing 4 positive integers: N (≤20, number of windows), M (≤10, the maximum capacity of each line inside the yellow line), K (≤1000, number of customers), and Q (≤1000, number of customer queries). The next line contains K positive integers, which are the processing time of the K customers. The last line contains Q positive integers, which represent the customers who are asking about the time they can have their transactions done. The customers are numbered from 1 to K. Output Specification: For each of the Q customers, print in one line the time at which his/her transaction is finished, in the format HH:MM where HH is in [08, 17] and MM is in [00, 59]. Note that since the bank is closed everyday after 17:00, for those customers who cannot be served before 17:00, you must output Sorry instead. Sample Input: 1232 2 7 51 2 6 4 3 534 23 4 5 6 7 Sample Output:1234508:0708:0608:1017:00Sorry 银行有N个窗口,每个窗口前有黄线,可以站M个人.现在有K个人要来银行办业务.排队时优先选择人少的队伍,人数一样则优先选窗口编号小的.若黄线区全满则等到队伍空出时再排队.银行8点开门,17点后还没有开始服务的顾客无法接受服务,应输出Sorry,否则应输出服务结束的时间. 思路先考虑两个窗口,黄线站一个人的情况.前两个人直接接受服务,第三个人会排到最先有空位的那个队伍中去.最先有空位的队伍,也就是当前服务的顾客所需剩余服务时间最短的队伍.考虑这样定义结构1234567891011struct customer&#123; int start; int id; int duration; int finished;&#125;;struct window&#123; int offset; int remaining; queue&lt;int&gt; que;&#125;; offset为当前时间距离8点的偏移量.remaining是当前服务顾客所需的剩余时间.整个银行的操作过程可以描述如下: 找到一个排队人数最少的队伍 如果不存在这样的队伍,转到2. 如果存在这样的队伍,顾客入队. 找到一个顾客离开最早的队伍,完成该顾客的业务,使其离开,并把当前顾客入队. 下面四个函数对应 找到排队人数最少的队伍, 顾客入队 找到顾客离开最早的队伍 顾客离开123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566int find_next_window()&#123; int res = -1; int min = inf; for(int i=0;i&lt;n;i++) &#123; int size = windows[i].que.size(); if(size&lt;min) &#123; min = size; res = i; &#125; &#125; if(min&lt;m) return res; else return -1;&#125;void insert_next_window(int customIndex,int index)&#123; if(index==-1) return; if(windows[index].que.size()==0) &#123; windows[index].remaining=cus[customIndex].duration; cus[customIndex].start=windows[index].offset; &#125; windows[index].que.push(customIndex);&#125;int find_next_finished()&#123; int res = -1; int min = inf; for(int i=0;i&lt;n;i++) &#123; if(windows[i].que.size()==0) continue; int size = windows[i].remaining; if(size&lt;min) &#123; min = size; res = i; &#125; &#125; if(min==inf) return -1; return res;&#125;void finish_next(int index)&#123; if(index&lt;0) return; int remaining = windows[index].remaining; for(int i=0;i&lt;n;i++) &#123; windows[i].offset+=remaining; windows[i].remaining-=remaining; &#125; int front = windows[index].que.front(); windows[index].que.pop(); cus[front].finished=windows[index].offset; if(!windows[index].que.empty()) &#123; int next = windows[index].que.front(); windows[index].remaining = cus[next].duration; cus[next].start=windows[index].offset; &#125;&#125; 整个流程如下: 找出最短的队伍,把人塞进入 如果找不到最短的队伍,找到最先完成的队伍,让队首的顾客离开,把人塞进入 所有人都塞到队伍里后,等待所有队伍处理结束.123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155#include &lt;algorithm&gt;#include &lt;queue&gt;#include &lt;iostream&gt;#include &lt;vector&gt;#include &lt;cstdio&gt;using namespace std;struct customer&#123; int start; int id; int duration; int finished;&#125;;struct window&#123; int offset; int remaining; queue&lt;int&gt; que;&#125;;int n,m,k,q;vector&lt;window&gt; windows;vector&lt;customer&gt; cus;const int inf = 0xFFFFF;int find_next_window()&#123; int res = -1; int min = inf; for(int i=0;i&lt;n;i++) &#123; int size = windows[i].que.size(); if(size&lt;min) &#123; min = size; res = i; &#125; &#125; if(min&lt;m) return res; else return -1;&#125;void insert_next_window(int customIndex,int index)&#123; if(index==-1) return; if(windows[index].que.size()==0) &#123; windows[index].remaining=cus[customIndex].duration; cus[customIndex].start=windows[index].offset; &#125; windows[index].que.push(customIndex);&#125;int find_next_finished()&#123; int res = -1; int min = inf; for(int i=0;i&lt;n;i++) &#123; if(windows[i].que.size()==0) continue; int size = windows[i].remaining; if(size&lt;min) &#123; min = size; res = i; &#125; &#125; if(min==inf) return -1; return res;&#125;void finish_next(int index)&#123; if(index&lt;0) return; int remaining = windows[index].remaining; for(int i=0;i&lt;n;i++) &#123; windows[i].offset+=remaining; windows[i].remaining-=remaining; &#125; int front = windows[index].que.front(); windows[index].que.pop(); cus[front].finished=windows[index].offset; if(!windows[index].que.empty()) &#123; int next = windows[index].que.front(); windows[index].remaining = cus[next].duration; cus[next].start=windows[index].offset; &#125;&#125;int main()&#123; cin&gt;&gt;n&gt;&gt;m&gt;&gt;k&gt;&gt;q; cus = vector&lt;customer&gt;(k); windows = vector&lt;window&gt;(n); for(int i=0;i&lt;k;i++) &#123; cus[i].id=i+1; cin&gt;&gt;cus[i].duration; &#125; int i =0; while(i&lt;k) &#123; int index = find_next_window(); if(index!=-1) &#123; insert_next_window(i,index); &#125; else&#123; break; &#125; i++; &#125; for(;i&lt;k;i++) &#123; int j = find_next_finished(); if(j!=-1) &#123; finish_next(j); &#125; int index = find_next_window(); if(index!=-1) &#123; insert_next_window(i,index); &#125; &#125; while(true) &#123; int j = find_next_finished(); if(j==-1) &#123; break; &#125; finish_next(j); &#125; for(int i=0;i&lt;q;i++) &#123; int s; cin&gt;&gt;s; int offset = cus[s-1].finished; int h = offset/60+8; int m = offset%60; if(cus[s-1].start&gt;=540) &#123; cout&lt;&lt;\"Sorry\"&lt;&lt;endl; &#125; else &#123; printf(\"%02d:%02d\\n\",h,m); &#125; &#125;&#125; 踩坑误解了题意,以为五点后所有的业务都将停止,没有做完业务的顾客将被强制离开.这也和现实里的银行相悖.以后要细心读题. 把17点距离8点的offset算成了480导致一直有case不能AC.我总是能在加减法上出错🙃","categories":[],"tags":[{"name":"PAT","slug":"PAT","permalink":"https://verrickt.github.io/tags/PAT/"}]},{"title":"Molecule to atoms","slug":"molecule-to-atoms","date":"2019-08-08T14:57:35.000Z","updated":"2019-08-16T14:47:20.829Z","comments":true,"path":"2019/08/08/molecule-to-atoms/","link":"","permalink":"https://verrickt.github.io/2019/08/08/molecule-to-atoms/","excerpt":"同学发过来一道有意思的题 For a given chemical formula represented by a string, count the number of atoms of each element contained in the molecule and return an object (associative array in PHP, Dictionary&lt;string, int&gt; in C#, Map&lt;String,Integer&gt; in Java).","text":"同学发过来一道有意思的题 For a given chemical formula represented by a string, count the number of atoms of each element contained in the molecule and return an object (associative array in PHP, Dictionary&lt;string, int&gt; in C#, Map&lt;String,Integer&gt; in Java). For example: 12345678var water = 'H2O';parseMolecule(water); // return &#123;H: 2, O: 1&#125;var magnesiumHydroxide = 'Mg(OH)2';parseMolecule(magnesiumHydroxide); // return &#123;Mg: 1, O: 2, H: 2&#125;var fremySalt = 'K4[ON(SO3)2]2';parseMolecule(fremySalt); // return &#123;K: 4, O: 14, N: 2, S: 4&#125; As you can see, some formulas have brackets in them. The index outside the brackets tells you that you have to multiply count of each atom inside the bracket on this index. For example, in Fe(NO3)2 you have one iron atom, two nitrogen atoms and six oxygen atoms. Note that brackets may be round, square or curly and can also be nested. Index after the braces is optional. 一句话描述:统计化学式中出现各个原子的个数. 不含括号的化学式很好解析,带上括号只要依次处理括号里的部分,乘以括号后的倍数就完成了.而括号里的部分又可以是一个完整的化学式.完美的分治. 以K4[ON(SO3)2]2为例.首先解析K4,接着解析[ON(SO3)2]2,碰到括号解析字串ON(SO3)2,对应个数乘2再合并即可.ON(SO3)2也一样,解析出O,N后继续解析括号里的(SO3)2即可. 解析字符串一般可以转化成解析上下文无关文法,答案就在生成的AST中. 所有的括号{},[]替换为()后均不影响结果.这里就偷个懒 生成式:12345Number: [1-9][0-9]*Atom:[A-Z][a-z]*Term: Atom NumberMolscule: TermTerm* | Term*(Molscule)Number?Term* 原子以大写字母开头,后跟任意个小写字母 化学式的一项是一个原子后跟一个分子. 分子式可以由至少一个项构成 分子式可以由任意项中间插入左括号(,另一个分子式右括号)和一个可选的数字构成. 由生成式就可以动手写解析器啦.首先对每一个可能的分支都写出一个完整的解析函数,在无法确定接下来是哪个分支的时候使用超前查看来实现分支预测. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117class Program &#123; static void Main(string[] args) &#123; Console.WriteLine(\"Please enter a chemical\"); Console.WriteLine(\"Default: K4(ON(SO3)2)2\"); string source = \"K4(ON(SO3)2)2\"; var str = Console.ReadLine(); str = string.IsNullOrWhiteSpace(str) ? source : str; var dict = Parse(str, 0, 1); dict.ToList().ForEach(p =&gt; Console.WriteLine($\"&#123;p.Key&#125;:&#123;p.Value&#125;\")); Console.WriteLine(\"Press any key to continue\"); Console.ReadLine(); &#125; static bool IsUpper(char c) &#123; return c &gt;= 'A' &amp;&amp; c &lt;= 'Z'; &#125; static bool IsLower(char c) &#123; return c &gt;= 'a' &amp;&amp; c &lt;= 'z'; &#125; static bool IsNumber(char c) &#123; return c &gt;= '0' &amp;&amp; c &lt;= '9'; &#125; static int GetNextElemIndex(string str, int start) &#123; if (start &gt;= str.Length) return -1; if (!IsUpper(str[start])) &#123; return -1; &#125; for (int i = start + 1; i &lt; str.Length; i++) &#123; if (!IsLower(str[i]) &amp;&amp; !IsUpper(str[i])) return i - 1; if (IsLower(str[i])) &#123; continue; &#125; if(IsUpper(str[i])) &#123; return i - 1; &#125; &#125; return str.Length - 1; &#125; static int GetNextNumber(string str,int start) &#123; if (start &gt;= str.Length) return -1; if (!IsNumber(str[start])) return -1; for(int i=start;i&lt;str.Length;i++) &#123; if (!IsNumber(str[i])) return i - 1; &#125; if (IsNumber(str.Last())) return str.Length - 1; else return -1; &#125; static int GetRightParin(string str,int start) &#123; for(int i=start;i&lt;str.Length;i++) &#123; if (str[i] == ')') return i; &#125; return -1; &#125; static Dictionary&lt;string, int&gt; Parse(string str,int startIndex,int multiplex) &#123; Dictionary&lt;string, int&gt; dict = new Dictionary&lt;string, int&gt;(); while (true) &#123; if (startIndex == str.Length) &#123; return dict; &#125; var elem = GetNextElemIndex(str, startIndex); if (elem!=-1) &#123; // N个微元的情况,直接解析 var name = str.Substring(startIndex, elem - startIndex + 1); int value = 1; startIndex = elem + 1; var num = GetNextNumber(str, startIndex); if(num!=-1) &#123; value = int.Parse(str.Substring(startIndex, num - startIndex + 1)); startIndex = num + 1; &#125; if (!dict.ContainsKey(name)) dict[name] = 0; dict[name] += value; &#125; else if(str[startIndex]=='(') &#123; // 碰到括号,解析子化学式 var right = GetRightParin(str, startIndex + 1)+1; var nextNum = GetNextNumber(str, right); //子化学式的倍数 int mul = int.Parse(str.Substring(nextNum, nextNum - right + 1)); var recursion = Parse(str, startIndex + 1, mul); startIndex = nextNum + 1; //合并字典 recursion.Keys.ToList().ForEach(k =&gt; &#123; if (!dict.ContainsKey(k)) dict[k] = 0; &#125;); recursion.ToList().ForEach(p =&gt; dict[p.Key] += p.Value); &#125; else if(str[startIndex]==')') &#123; // 返回 return dict.ToDictionary(i =&gt; i.Key, i =&gt; i.Value * multiplex); &#125; &#125; &#125; &#125;","categories":[],"tags":[{"name":"C#","slug":"C","permalink":"https://verrickt.github.io/tags/C/"},{"name":"Parser","slug":"Parser","permalink":"https://verrickt.github.io/tags/Parser/"}]},{"title":"PAT 1003 Emergency","slug":"PAT-1003-Emergency","date":"2019-08-08T14:10:09.000Z","updated":"2019-08-08T14:41:08.401Z","comments":true,"path":"2019/08/08/PAT-1003-Emergency/","link":"","permalink":"https://verrickt.github.io/2019/08/08/PAT-1003-Emergency/","excerpt":"Emergency1003 Emergency As an emergency rescue team leader of a city, you are given a special map of your country. The map shows several scattered cities connected by some roads. Amount of rescue teams in each city and the length of each road between any pair of cities are marked on the map. When there is an emergency call to you from some other city, your job is to lead your men to the place as quickly as possible, and at the mean time, call up as many hands on the way as possible.","text":"Emergency1003 Emergency As an emergency rescue team leader of a city, you are given a special map of your country. The map shows several scattered cities connected by some roads. Amount of rescue teams in each city and the length of each road between any pair of cities are marked on the map. When there is an emergency call to you from some other city, your job is to lead your men to the place as quickly as possible, and at the mean time, call up as many hands on the way as possible.Input Specification Each input file contains one test case. For each test case, the first line contains 4 positive integers: N (≤500) - the number of cities (and the cities are numbered from 0 to N−1), M - the number of roads, C​1​​ and C​2​​ - the cities that you are currently in and that you must save, respectively. The next line contains N integers, where the i-th integer is the number of rescue teams in the i-th city. Then M lines follow, each describes a road with three integers c​1​​, c​2​​ and L, which are the pair of cities connected by a road and the length of that road, respectively. It is guaranteed that there exists at least one path from C​1​​ to C​2​​. Output Specification For each test case, print in one line two numbers: the number of different shortest paths between C​1​​ and C​2​​, and the maximum amount of rescue teams you can possibly gather. All the numbers in a line must be separated by exactly one space, and there is no extra space allowed at the end of a line. Sample Input 123456785 6 0 21 2 1 5 30 1 10 2 20 3 11 2 12 4 13 4 1 Sample Output12 4 题目大意:给定一张图,找出从起点到终点带权最短路径的个数,并输出其中最大的点权 思路这道题大体上是单源最短路径的变体,dijkstra跑不了了. dijkstra算法可以解决非负权最短路径的问题.算法的大体思路如下: 对每个顶点,存储一个最短路径估计.起点为0,其余为无穷大. 每次从未被访问的节点中找出最短路径估计最短的那个节点,访问它,对它的所有邻接顶点,进行松弛操作.设当前顶点为u,邻接节点为v,边的权值为w,松弛定义如下: 1234567relax(u,v,w)&#123; if(u.estimate+w&lt;v.estimate) &#123; v.estimate = u.estimate+w; &#125;&#125; 可见松弛是减小最短路径估计的过程. 松弛完成后,继续从未被访问的顶点中找出最短路径估计最小的节点,松弛其邻接节点,直到所有节点都访问完毕. 伪码如下12345678910111213141516171819202122232425262728293031323334const int INF 99999999void dijkstra()&#123; int estimate[v]; bool visited[v]; fill(estiamte,estimate+v,INF); fill(visited,visited+v,false); estimate[source]=0; while(true) &#123; int minn=INF; int min=-1; for(int i=0;i&lt;v;i++) &#123; if(!visited[i]&amp;&amp;estimate[i]&lt;minn) &#123; minn=estimate[i]; min=i; &#125; &#125; if(min==-1) break;//finished visited[min]=true; for(int i=0;i&lt;v;i++) &#123; if(graph[min][i]!=-1) &#123; if(estimate[min]+graph[min][i]&lt;estimate[i]) &#123; estimate[i]=estimate[min]+graph[min][i]; &#125; &#125; &#125; &#125;&#125; 题目要求求出最短路径的数量和其中最大点权的值.当estimate[min]+graph[min][i]&lt;estimate[i]成立时,表示当前到i的路线是新的最短路径预期,所以到i的路线数量和到min的路线数量一致;当estimate[min]+graph[min][i]==estimate[i]时,表明除了经过min到i的路线,还有其他同样短的路径.这时到i的最短路径个数应该加上从起点到min的个数. 接下来考虑点权.当estimate[min]+graph[min][i]&lt;estimate[i]成立时,表示到i的路径是新的最短路径预期,从起点到i的点权和应该为到min的点权和加上i的点权;当当estimate[min]+graph[min][i]==estimate[i]时,点权和应该是i现有点权和,min点权和+i点权两者中较大的那个. 代码如下:12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394#include &lt;iostream&gt;using namespace std;int v;int e;int g[500][500] ;int source;int dest;int v_data[500] ;int dist[500];int path[500];int num[500]; int rescue[500];int dijkstra()&#123; bool visit[500]=&#123;false&#125;; dist[source]=0; num[source]=1; rescue[source]=v_data[source]; while(true) &#123; // find vertex with minimal estimate int min_index=-1; int min=10000000; for(int i=0;i&lt;v;i++) &#123; if(min&gt;dist[i]&amp;&amp;visit[i]==false) &#123; min = dist[i]; min_index = i; &#125; &#125; if(min_index==-1) &#123; break; &#125; visit[min_index]=true; for(int i=0;i&lt;v;i++) &#123; int weight = g[min_index][i]; if(weight!=-1) &#123; if(visit[i]==false) &#123; if(dist[min_index]+weight&lt;dist[i]) &#123; dist[i]=dist[min_index]+weight; path[i]=min_index; num[i]=num[min_index]; rescue[i]=rescue[min_index]+v_data[i]; &#125; else if(dist[min_index]+weight==dist[i]) &#123; num[i]+=num[min_index]; if(rescue[i]&lt;rescue[min_index]+v_data[i]) &#123; rescue[i]=rescue[min_index]+v_data[i]; &#125; &#125; &#125; &#125; &#125; // for each adj vertex, relax &#125;&#125;int main()&#123; cin&gt;&gt;v&gt;&gt;e&gt;&gt;source&gt;&gt;dest; fill(path,path+v,-1); fill(dist,dist+v,1000000); fill(num,num+v,0); fill(rescue,rescue+v,0); for(int i=0;i&lt;v;i++) &#123; for(int j=0;j&lt;v;j++) g[i][j]=-1; &#125; for(int i=0;i&lt;v;i++) &#123; cin&gt;&gt;v_data[i]; &#125; int p1,p2,l; for(int i=0;i&lt;e;i++) &#123; cin&gt;&gt;p1&gt;&gt;p2&gt;&gt;l; g[p1][p2]=g[p2][p1]=l; &#125; dijkstra(); cout&lt;&lt;num[dest]&lt;&lt;\" \"&lt;&lt;rescue[dest]&lt;&lt;endl;&#125;","categories":[],"tags":[{"name":"PAT","slug":"PAT","permalink":"https://verrickt.github.io/tags/PAT/"}]},{"title":"PAT 1004 Counting Leaves","slug":"PAT-1004-Counting-Leaves","date":"2019-08-05T06:56:39.000Z","updated":"2019-08-05T07:34:59.894Z","comments":true,"path":"2019/08/05/PAT-1004-Counting-Leaves/","link":"","permalink":"https://verrickt.github.io/2019/08/05/PAT-1004-Counting-Leaves/","excerpt":"Counting Leaves1004 Counting Leaves A family hierarchy is usually presented by a pedigree tree. Your job is to count those family members who have no child.","text":"Counting Leaves1004 Counting Leaves A family hierarchy is usually presented by a pedigree tree. Your job is to count those family members who have no child.Input Specification: Each input file contains one test case. Each case starts with a line containing 0&lt;N&lt;100, the number of nodes in a tree, and M (&lt;N), the number of non-leaf nodes. Then M lines follow, each in the format: ID K ID[1] ID[2] ... ID[K] where ID is a two-digit number representing a given non-leaf node, K is the number of its children, followed by a sequence of two-digit ID‘s of its children. For the sake of simplicity, let us fix the root ID to be 01. The input ends with N being 0. That case must NOT be processed. Output Specification: For each test case, you are supposed to count those family members who have no child for every seniority level starting from the root. The numbers must be printed in a line, separated by a space, and there must be no extra space at the end of each line. The sample case represents a tree with only 2 nodes, where 01 is the root and 02 is its only child. Hence on the root 01 level, there is 0 leaf node; and on the next level, there is 1 leaf node. Then we should output 0 1 in a line.Sample Input: 122 101 1 02 Sample Output:10 1 题目大意: 给定一颗树,从根节点开始依次输出当前层中叶子节点的个数.树的根节点是01 思路每个成员的孩子个数不确定,不能简单用二叉树实现. 考虑一般树的表示方法 孩子兄弟表示法 双亲表示法 孩子表示法 题目直接给出的是父子关系,孩子兄弟表示法不太合适,排除. 双亲表示法由孩子找双亲比较方便,孩子表示法由双亲找孩子比较方便,所以采用孩子表示法. 采用数组来表示节点的孩子,那么所有的节点可以用一个二维数组表示.因为N&lt;100,所以开个int[101][101]就够用了.我就偷个懒,使用STL的vector 接下来需要找出一层中叶节点的个数.假设已经有了当前层节点的下标,叶子节点的个数可以由遍历很容易的统计.一轮遍历完成后会得到下一层节点的下标.这样就可以一直持续下去了. 12345678910111213141516171819202122vector&lt;int&gt; TriversalTree(vector&lt;vector&lt;int&gt;&gt; tree, vector&lt;int&gt; parents, int&amp; leaf)&#123; vector&lt;int&gt; result; leaf = 0; for (int i = 0; i &lt; parents.size(); i++) &#123; int index = parents[i]; auto node = tree[index]; if (node.size() == 0) &#123; leaf++; &#125; else &#123; for (auto it = node.begin(); it != node.end(); it++) &#123; result.push_back(*it); &#125; &#125; &#125; return result;&#125; 给出第一层的下标即可开始遍历了 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364#include &lt;iostream&gt;#include &lt;string&gt;#include &lt;vector&gt;using namespace std;vector&lt;int&gt; TriversalTree(vector&lt;vector&lt;int&gt;&gt; tree, vector&lt;int&gt; parents, int&amp; leaf)&#123; vector&lt;int&gt; result; leaf = 0; for (int i = 0; i &lt; parents.size(); i++) &#123; int index = parents[i]; auto node = tree[index]; if (node.size() == 0) &#123; leaf++; &#125; else &#123; for (auto it = node.begin(); it != node.end(); it++) &#123; result.push_back(*it); &#125; &#125; &#125; return result;&#125;int main()&#123; int n; cin &gt;&gt; n; if (n == 0) &#123; return 0; &#125; vector&lt;vector&lt;int&gt;&gt; tree = vector&lt;vector&lt;int&gt;&gt;(n + 1); int non_leaf; cin &gt;&gt; non_leaf; for (int i = 0; i &lt; non_leaf; i++) &#123; int root; int num; int child; cin &gt;&gt; root; cin &gt;&gt; num; for (int j = 0; j &lt; num; j++) &#123; cin &gt;&gt; child; tree[root].push_back(child); &#125; &#125; vector&lt;int&gt; layer; layer.push_back(1); int leaf = 0; layer = TriversalTree(tree, layer, leaf); printf(\"%d\", leaf); while (layer.size() &gt; 0) &#123; layer = TriversalTree(tree, layer, leaf); printf(\" %d\", leaf); &#125;; printf(\"\\n\"); return 0;&#125;","categories":[],"tags":[{"name":"PAT","slug":"PAT","permalink":"https://verrickt.github.io/tags/PAT/"}]},{"title":"异常-控制流的突变","slug":"exception-the-abnormal-control-flow","date":"2019-07-08T11:10:02.000Z","updated":"2019-07-08T11:44:47.313Z","comments":true,"path":"2019/07/08/exception-the-abnormal-control-flow/","link":"","permalink":"https://verrickt.github.io/2019/07/08/exception-the-abnormal-control-flow/","excerpt":"什么是异常「现代操作系统」和国内的教材都没有讲清楚「中断、异常」这一组概念，再加上自己懒，一直没弄出个所以然，今天看了CSAPP上的解释，醍醐灌顶。","text":"什么是异常「现代操作系统」和国内的教材都没有讲清楚「中断、异常」这一组概念，再加上自己懒，一直没弄出个所以然，今天看了CSAPP上的解释，醍醐灌顶。对于特定的进程来说，程序计数器按照时间升序所得的序列反映了指令的执行顺序，即控制流。理论上说，程序的控制流在编译期就已经决定。而实际中，经常会出现一些需要暂停执行当前程序，而转去处理更高优先级任务的情况。在这个过程中，程序的控制流就发生了改变。CSAPP中把控制流的非自愿改变定义为异常。强调非自愿，是因为程序自己也可以改变控制流，但那不属于异常。 异常按照类型可以分为四类 中断(Interrupt) 自陷(Trap) 错误(Fault) 中止(Abort) 发生异常时，CPU会根据某种规则来执行对应的处理程序。根据异常类型的不同，处理的结果也不尽相同。 中断中断指，在程序执行过程中，因为外部事件导致当前控制流被改变。因为外部事件的不确定性，中断信号可能会在指令执行的任意阶段发生，因此也称中断具有异步性典型的例子有：DMA传输完成，键盘敲击等。 自陷自陷指，在程序执行过程中，需要请求操作系统提供的服务而发生的自愿性中断。普通程序在用户态运行，当需要进行I/O时，它通过自陷，转入内核态，由操作系统执行对应的I/O指令。 IA32中提供int 80指令触发自陷 错误错误指，在指令执行过程中，出现了某些错误，导致当前指令无法继续执行。但错误可能可以恢复。处理程序处理完成后，可以尝试重现执行出错的指令。如缺页错误(Page fault)。 中止中止是指，执行过程中发生了不可恢复的错误，这时管理权被交给OS，典型情况下，OS会杀死出错的进程，转而调度其他进程执行。例如内存越界(Access violation)","categories":[],"tags":[]},{"title":"FluentTreeView Part.5","slug":"fluent-treeview-part5","date":"2019-04-19T13:18:17.000Z","updated":"2019-04-20T03:22:53.907Z","comments":true,"path":"2019/04/19/fluent-treeview-part5/","link":"","permalink":"https://verrickt.github.io/2019/04/19/fluent-treeview-part5/","excerpt":"递归高亮先来修正选择高亮的问题吧。我更愿意图片中的效果称为递归高亮。递归高亮可以看作是子节点都处于非递归高亮状态。这么想的话实现起来应该非常容易。但我希望FluentTreeView能够更灵活一些。最好通过属性来切换高亮模式。","text":"递归高亮先来修正选择高亮的问题吧。我更愿意图片中的效果称为递归高亮。递归高亮可以看作是子节点都处于非递归高亮状态。这么想的话实现起来应该非常容易。但我希望FluentTreeView能够更灵活一些。最好通过属性来切换高亮模式。 说来就来，要往TreeViewItem上加新的属性，要由TreeViewItem导出子类。要使TreeView的直接ItemContainer使用我们自定义的子类，需要继承TreeView的子类。 12345678910111213141516171819202122class FluentTreeViewItem : TreeViewItem&#123; public bool RecursiveHighlightMode &#123; get &#123; return (bool) GetValue (RecursiveHighlightModeProperty); &#125; set &#123; SetValue (RecursiveHighlightModeProperty, value); &#125; &#125; // Using a DependencyProperty as the backing store for RecursiveHighlightMode. This enables animation, styling, binding, etc... public static readonly DependencyProperty RecursiveHighlightModeProperty = DependencyProperty.Register (\"RecursiveHighlightMode\", typeof (bool), typeof (FluentTreeViewItem), new PropertyMetadata (false)); protected override DependencyObject GetContainerForItemOverride () &#123; return new FluentTreeViewItem (); &#125; protected override bool IsItemItsOwnContainerOverride (object item) &#123; return item is FluentTreeViewItem; &#125;&#125; 123456789101112public class FluentTreeView : TreeView&#123; protected override DependencyObject GetContainerForItemOverride () &#123; return new FluentTreeViewItem (); &#125; protected override bool IsItemItsOwnContainerOverride (object item) &#123; return item is FluentTreeViewItem; &#125;&#125; 重写TreeView的GetContainerForItemOverride是常规操作，大家在自定义ItemContainer的时候肯定都做过。需要注意的是TreeViewItem也是ItemsControl，它的GetContainerForItemOverride也需要被重写。 接下来思考如何实现递归模式和非递归模式的切换。直接让子节点也处于高亮状态不错的办法，但高亮状态依赖的触发器，需要由IsSelected属性触发。为了界面而去修改数据，违背WPF数据驱动界面的原则。那么换一种思路，不再用分治的思路，而是直接在当前节点上全部处理掉。让父节点的高亮区域覆盖所有的子节点即可。宽度已经占满，只剩下高度了。对背景色，可以直接修改最外层面板的背景色。起指示作用的矩形，则可以修改其Grid.RowSpan让它充满整个高度。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758&lt;ControlTemplate TargetType=\"cc:FluentTreeViewItem\"&gt; &lt;Grid x:Name=\"globalHighlight\" Background=\"Transparent\"&gt; &lt;Grid.RowDefinitions&gt; &lt;RowDefinition Height=\"24\"&gt;&lt;/RowDefinition&gt; &lt;RowDefinition Height=\"*\"&gt;&lt;/RowDefinition&gt; &lt;/Grid.RowDefinitions&gt; &lt;Rectangle Panel.ZIndex=\"20\" Grid.RowSpan=\"1\" x:Name=\"selector\" Width=\"2\" Visibility=\"Collapsed\" Fill=\"Red\" HorizontalAlignment=\"Left\"&gt;&lt;/Rectangle&gt; &lt;Border x:Name=\"root\"&gt; &lt;Border.Style&gt; &lt;Style TargetType=\"Border\"&gt; &lt;Style.Triggers&gt; &lt;Trigger Property=\"IsMouseOver\" Value=\"True\"&gt; &lt;Setter Property=\"Background\" Value=\"Red\"&gt;&lt;/Setter&gt; &lt;/Trigger&gt; &lt;/Style.Triggers&gt; &lt;/Style&gt; &lt;/Border.Style&gt; &lt;Grid Background=\"Transparent\" x:Name=\"selectorGrid\"&gt; &lt;Grid Margin=\"&#123;Binding RelativeSource=&#123;RelativeSource AncestorType=TreeViewItem&#125;,Converter=&#123;StaticResource TreeLevelToIndentConverter&#125;&#125;\"&gt; &lt;Grid.ColumnDefinitions&gt; &lt;ColumnDefinition Width=\"24\"&gt;&lt;/ColumnDefinition&gt; &lt;ColumnDefinition&gt;&lt;/ColumnDefinition&gt; &lt;/Grid.ColumnDefinitions&gt; &lt;Expander IsExpanded=\"&#123;Binding RelativeSource=&#123;RelativeSource TemplatedParent&#125;,Path=IsExpanded,Mode=TwoWay&#125;\" x:Name=\"expander\"&gt;&lt;/Expander&gt; &lt;ContentPresenter VerticalAlignment=\"Center\" Grid.Column=\"1\" ContentSource=\"Header\"&gt;&lt;/ContentPresenter&gt; &lt;/Grid&gt; &lt;/Grid&gt; &lt;/Border&gt; &lt;ItemsPresenter Visibility=\"&#123;Binding RelativeSource=&#123;RelativeSource TemplatedParent&#125;,Path=IsExpanded,Converter=&#123;StaticResource BoolToVisibilityConverter&#125;&#125;\" Grid.Row=\"1\"&gt;&lt;/ItemsPresenter&gt; &lt;/Grid&gt; &lt;ControlTemplate.Triggers&gt; &lt;Trigger Property=\"HasItems\" Value=\"False\"&gt; &lt;Setter TargetName=\"expander\" Property=\"Visibility\" Value=\"Collapsed\"&gt;&lt;/Setter&gt; &lt;/Trigger&gt; &lt;Trigger Property=\"IsMouseOver\" Value=\"True\"&gt; &lt;/Trigger&gt; &lt;Trigger Property=\"IsSelected\" Value=\"True\"&gt; &lt;Setter TargetName=\"selector\" Property=\"Visibility\" Value=\"Visible\"&gt;&lt;/Setter&gt; &lt;/Trigger&gt; &lt;Trigger Property=\"RecursiveHighlightMode\" Value=\"True\"&gt; &lt;Setter TargetName=\"selector\" Property=\"Grid.RowSpan\" Value=\"2\"&gt;&lt;/Setter&gt; &lt;/Trigger&gt; &lt;MultiTrigger &gt; &lt;MultiTrigger.Conditions&gt; &lt;Condition Property=\"RecursiveHighlightMode\" Value=\"True\"&gt;&lt;/Condition&gt; &lt;Condition Property=\"IsSelected\" Value=\"True\"&gt;&lt;/Condition&gt; &lt;/MultiTrigger.Conditions&gt; &lt;Setter TargetName=\"globalHighlight\" Property=\"Background\" Value=\"Green\"&gt;&lt;/Setter&gt; &lt;/MultiTrigger&gt; &lt;MultiTrigger &gt; &lt;MultiTrigger.Conditions&gt; &lt;Condition Property=\"RecursiveHighlightMode\" Value=\"False\"&gt;&lt;/Condition&gt; &lt;Condition Property=\"IsSelected\" Value=\"True\"&gt;&lt;/Condition&gt; &lt;/MultiTrigger.Conditions&gt; &lt;Setter TargetName=\"selectorGrid\" Property=\"Background\" Value=\"Green\"&gt;&lt;/Setter&gt; &lt;/MultiTrigger&gt; &lt;/ControlTemplate.Triggers&gt;&lt;/ControlTemplate&gt; 我们根据RecursiveHighlightMode的值在globalHighlight和selectorGrid里选出作为选择高亮的控件，修改selector的Grid.RowSpan。Grid.RowSpan会使布局子系统的Measure失效。在依赖属性的元数据中声明属性变化影响Measure阶段，就实现了动态切换高亮模式。12public static readonly DependencyProperty RecursiveHighlightModeProperty = DependencyProperty.Register (\"RecursiveHighlightMode\", typeof (bool), typeof (FluentTreeViewItem), new FrameworkPropertyMetadata (false, FrameworkPropertyMetadataOptions.AffectsMeasure)); 高亮优先级回顾一下上一篇中模板的可视化树123456Border(x:Name=&quot;root&quot;) Grid(x:Name=&quot;selectorGrid&quot;) Rectangle(x:Name=&quot;selector&quot;) Grid Expander ContentPreserenter 当选中且鼠标悬空时，root和selectorGrid的触发器都生效。因为root是selectorGrid的父级容器，渲染时就被置于靠下的一层，这就导致了选择高亮覆盖鼠标高亮的表象。要改其实很简单，把这他们两个容器的位置互换就行了。123456789101112131415161718192021222324&lt;Grid Background=\"Transparent\" x:Name=\"selectorGrid\"&gt; &lt;Border&gt; &lt;Border.Style&gt; &lt;Style TargetType=\"Border\"&gt; &lt;Style.Triggers&gt; &lt;Trigger Property=\"IsMouseOver\" Value=\"True\"&gt; &lt;Setter Property=\"Background\" Value=\"Red\"&gt;&lt;/Setter&gt; &lt;/Trigger&gt; &lt;/Style.Triggers&gt; &lt;/Style&gt; &lt;/Border.Style&gt; &lt;Grid&gt; &lt;Grid Margin=\"&#123;Binding RelativeSource=&#123;RelativeSource AncestorType=TreeViewItem&#125;,Converter=&#123;StaticResource TreeLevelToIndentConverter&#125;&#125;\"&gt; &lt;Grid.ColumnDefinitions&gt; &lt;ColumnDefinition Width=\"24\"&gt;&lt;/ColumnDefinition&gt; &lt;ColumnDefinition&gt;&lt;/ColumnDefinition&gt; &lt;/Grid.ColumnDefinitions&gt; &lt;Expander IsExpanded=\"&#123;Binding RelativeSource=&#123;RelativeSource TemplatedParent&#125;,Path=IsExpanded,Mode=TwoWay&#125;\" x:Name=\"expander\"&gt;&lt;/Expander&gt; &lt;ContentPresenter VerticalAlignment=\"Center\" Grid.Column=\"1\" ContentSource=\"Header\"&gt;&lt;/ContentPresenter&gt; &lt;/Grid&gt; &lt;/Grid&gt; &lt;/Border&gt;&lt;/Grid&gt; 配色太丑剩下的就是一些小细节问题了。按照Fluent Design的标准，所有的图标都要使用Segoe MDL2 Assets实现，并且要根据Windows当前的主题模式和主题色动态更改颜色。 Fluent.WPF可以取得主题模式和主题色对应的画刷，但在使用它之前需要先在FluentTreeViewItem上新增属性，方便修改。 12345678910111213141516171819public Brush MouseOverBrush&#123; get &#123; return (Brush) GetValue (MouseOverBrushProperty); &#125; set &#123; SetValue (MouseOverBrushProperty, value); &#125;&#125;// Using a DependencyProperty as the backing store for MouseOverBrush. This enables animation, styling, binding, etc...public static readonly DependencyProperty MouseOverBrushProperty = DependencyProperty.Register (\"MouseOverBrush\", typeof (Brush), typeof (FluentTreeViewItem), new PropertyMetadata (new SolidColorBrush (Colors.Red)));public Brush SelectedBrush&#123; get &#123; return (Brush) GetValue (SelectedBrushProperty); &#125; set &#123; SetValue (SelectedBrushProperty, value); &#125;&#125;// Using a DependencyProperty as the backing store for SelectedBrush. This enables animation, styling, binding, etc...public static readonly DependencyProperty SelectedBrushProperty = DependencyProperty.Register (\"SelectedBrush\", typeof (Brush), typeof (FluentTreeViewItem), new PropertyMetadata (new SolidColorBrush (Colors.Green))); 选择高亮使用SystemAltMediumHighColorBrush，鼠标高亮使用SystemBaseMediumLowColorBrush，选择指示用的矩形使用SystemAccentColor。 再把Expander用Segoe MDL2 Assets字体重新实现，我们的FluentTreeView就完成了。 递归高亮 非递归高亮 源代码参见https://github.com/Verrickt/Melchior-Sample/tree/master/FluentTreeView_Part5","categories":[{"name":"FluentTreeView","slug":"FluentTreeView","permalink":"https://verrickt.github.io/categories/FluentTreeView/"}],"tags":[]},{"title":"FluentTreeView Part.4","slug":"fluent-treeview-part4","date":"2019-04-19T09:20:45.000Z","updated":"2019-04-19T13:20:46.514Z","comments":true,"path":"2019/04/19/fluent-treeview-part4/","link":"","permalink":"https://verrickt.github.io/2019/04/19/fluent-treeview-part4/","excerpt":"这篇起我们正式开始实现FluentTreeView。先看看图片 TreeView左侧有一选择高亮，Item有一鼠标高亮。这两个高亮与整个TreeView一样宽。再回想一下我们上一篇用Expander实现的TreeView，Item的缩进是如何实现的？","text":"这篇起我们正式开始实现FluentTreeView。先看看图片 TreeView左侧有一选择高亮，Item有一鼠标高亮。这两个高亮与整个TreeView一样宽。再回想一下我们上一篇用Expander实现的TreeView，Item的缩进是如何实现的？ - Column1 Column2 Row1 Expander ContentPreserenter Row2 / ItemsPreserenter ItemsPreserenter将会被展开为1234ItemsPanel TreeViewItem TreeViewItem TreeViewItem 这就意味着，下一级的TreeViewItem永远处于上一级TreeViewItem的第二列里。将第一列的列宽设置为定值，就实现了各个层级的缩进。 进一步思考，如果我们在这样的结构里去修改TreeViewItem的面板的背景色当作高亮的话，那这个面板自身也是被缩进的。我们试一试，把Style改成这样1234567891011121314151617181920212223242526272829&lt;Style TargetType=&quot;TreeViewItem&quot;&gt; &lt;Setter Property=&quot;Template&quot;&gt; &lt;Setter.Value&gt; &lt;ControlTemplate TargetType=&quot;TreeViewItem&quot;&gt; &lt;Grid x:Name=&quot;root&quot;&gt; &lt;Grid.ColumnDefinitions&gt; &lt;ColumnDefinition Width=&quot;24&quot;&gt;&lt;/ColumnDefinition&gt; &lt;ColumnDefinition&gt;&lt;/ColumnDefinition&gt; &lt;/Grid.ColumnDefinitions&gt; &lt;Grid.RowDefinitions&gt; &lt;RowDefinition Height=&quot;Auto&quot;&gt;&lt;/RowDefinition&gt; &lt;RowDefinition Height=&quot;*&quot;&gt;&lt;/RowDefinition&gt; &lt;/Grid.RowDefinitions&gt; &lt;Expander IsExpanded=&quot;&#123;Binding RelativeSource=&#123;RelativeSource TemplatedParent&#125;,Path=IsExpanded,Mode=TwoWay&#125;&quot; x:Name=&quot;expander&quot;&gt;&lt;/Expander&gt; &lt;ContentPresenter VerticalAlignment=&quot;Center&quot; Grid.Column=&quot;1&quot; ContentSource=&quot;Header&quot;&gt;&lt;/ContentPresenter&gt; &lt;ItemsPresenter Visibility=&quot;&#123;Binding RelativeSource=&#123;RelativeSource TemplatedParent&#125;,Path=IsExpanded,Converter=&#123;StaticResource BoolToVisibilityConverter&#125;&#125;&quot; Grid.Row=&quot;1&quot; Grid.Column=&quot;1&quot;&gt;&lt;/ItemsPresenter&gt; &lt;/Grid&gt; &lt;ControlTemplate.Triggers&gt; &lt;Trigger Property=&quot;HasItems&quot; Value=&quot;False&quot;&gt; &lt;Setter TargetName=&quot;expander&quot; Property=&quot;Visibility&quot; Value=&quot;Collapsed&quot;&gt;&lt;/Setter&gt; &lt;/Trigger&gt; &lt;Trigger Property=&quot;IsSelected&quot; Value=&quot;True&quot;&gt; &lt;Setter TargetName=&quot;root&quot; Property=&quot;Background&quot; Value=&quot;Red&quot;&gt;&lt;/Setter&gt; &lt;/Trigger&gt; &lt;/ControlTemplate.Triggers&gt; &lt;/ControlTemplate&gt; &lt;/Setter.Value&gt; &lt;/Setter&gt;&lt;/Style&gt; 在选中非第一级Item时，高亮色就出现了我们不需要的左边距。这不符合我们的要求，所以不能用递归的方式来实现不同层级的缩进。我们所想要的是，每一层TreeViewItem的宽度都与TreeView本身保持一致，所以TreeViewItem自己必须是打平的。需要特殊处理的是ContentPreserenter。可以根据TreeViewItem的层级计算出所需要的左边距。 再次考虑整个TreeViewItem的布局，鼠标和选择高亮应该撑满可用宽度，倒三角符号(▽)和ContentPreserenter则应该随着层级缩进。有了这些做参照，很容易能写出这样的模板:123456789101112131415161718&lt;Grid &gt; &lt;Grid.RowDefinitions&gt; &lt;RowDefinition Height=\"Auto\"&gt;&lt;/RowDefinition&gt; &lt;RowDefinition Height=\"*\"&gt;&lt;/RowDefinition&gt; &lt;/Grid.RowDefinitions&gt; &lt;Grid x:Name=\"root\"&gt; &lt;Rectangle x:Name=\"selector\" Width=\"2\" Visibility=\"Collapsed\" Fill=\"Green\" HorizontalAlignment=\"Left\"&gt;&lt;/Rectangle&gt; &lt;Grid Margin=\"&#123;Binding RelativeSource=&#123;RelativeSource AncestorType=TreeViewItem&#125;,Converter=&#123;StaticResource TreeLevelToIndentConverter&#125;&#125;\"&gt; &lt;Grid.ColumnDefinitions&gt; &lt;ColumnDefinition Width=\"24\"&gt;&lt;/ColumnDefinition&gt; &lt;ColumnDefinition&gt;&lt;/ColumnDefinition&gt; &lt;/Grid.ColumnDefinitions&gt; &lt;Expander IsExpanded=\"&#123;Binding RelativeSource=&#123;RelativeSource TemplatedParent&#125;,Path=IsExpanded,Mode=TwoWay&#125;\" x:Name=\"expander\"&gt;&lt;/Expander&gt; &lt;ContentPresenter VerticalAlignment=\"Center\" Grid.Column=\"1\" ContentSource=\"Header\"&gt;&lt;/ContentPresenter&gt; &lt;/Grid&gt; &lt;/Grid&gt; &lt;ItemsPresenter Visibility=\"&#123;Binding RelativeSource=&#123;RelativeSource TemplatedParent&#125;,Path=IsExpanded,Converter=&#123;StaticResource BoolToVisibilityConverter&#125;&#125;\" Grid.Row=\"1\"&gt;&lt;/ItemsPresenter&gt;&lt;/Grid&gt; TreeLevelToIndentConverter是根据等级计算缩进的Converter。 12345678910111213141516171819202122232425262728293031323334public class TreeLevelToIndentConverter : IValueConverter&#123; public Thickness Margin &#123; get; set; &#125; public object Convert (object value, Type targetType, object parameter, CultureInfo culture) &#123; if (value is TreeViewItem ti) &#123; int level = 0; FrameworkElement current = ti; do &#123; if (VisualTreeHelper.GetParent (current) is FrameworkElement fe) &#123; if (fe is TreeViewItem) &#123; level++; &#125; if (fe is TreeView) &#123; break; &#125; current = fe; &#125; &#125; while (current != null); return new Thickness (Margin.Left * level, 0, Margin.Right * level, 0); &#125; return value; &#125; public object ConvertBack (object value, Type targetType, object parameter, CultureInfo culture) &#123; throw new NotSupportedException(); &#125;&#125; 选择高亮已经差不多了，但鼠标高亮还有点问题 一番搜索发现，当前鼠标悬空的TreeViewItem以及它所有的祖先的IsMouseOver触发器都会起作用。 当然，你也可以像我一样偷懒：给Border设置MouseOver的触发器。 12345678910111213141516171819202122&lt;Border x:Name=\"root\"&gt; &lt;Border.Style&gt; &lt;Style TargetType=\"Border\"&gt; &lt;Style.Triggers&gt; &lt;Trigger Property=\"IsMouseOver\" Value=\"True\"&gt; &lt;Setter Property=\"Background\" Value=\"Red\"&gt;&lt;/Setter&gt; &lt;/Trigger&gt; &lt;/Style.Triggers&gt; &lt;/Style&gt; &lt;/Border.Style&gt; &lt;Grid&gt; &lt;Rectangle x:Name=\"selector\" Width=\"2\" Visibility=\"Collapsed\" Fill=\"Green\" HorizontalAlignment=\"Left\"&gt;&lt;/Rectangle&gt; &lt;Grid Margin=\"&#123;Binding RelativeSource=&#123;RelativeSource AncestorType=TreeViewItem&#125;,Converter=&#123;StaticResource TreeLevelToIndentConverter&#125;&#125;\"&gt; &lt;Grid.ColumnDefinitions&gt; &lt;ColumnDefinition Width=\"24\"&gt;&lt;/ColumnDefinition&gt; &lt;ColumnDefinition&gt;&lt;/ColumnDefinition&gt; &lt;/Grid.ColumnDefinitions&gt; &lt;Expander IsExpanded=\"&#123;Binding RelativeSource=&#123;RelativeSource TemplatedParent&#125;,Path=IsExpanded,Mode=TwoWay&#125;\" x:Name=\"expander\"&gt;&lt;/Expander&gt; &lt;ContentPresenter VerticalAlignment=\"Center\" Grid.Column=\"1\" ContentSource=\"Header\"&gt;&lt;/ContentPresenter&gt; &lt;/Grid&gt; &lt;/Grid&gt;&lt;/Border&gt; 看起来已经像模像样了，但还有瑕疵： 选择高亮并不对所有的子节点生效。 鼠标高亮没有覆盖选择高亮 配色不对，丑 我们会在下一篇中改正这些问题 源代码参见https://github.com/Verrickt/Melchior-Sample/tree/master/FluentTreeView_Part4","categories":[{"name":"FluentTreeView","slug":"FluentTreeView","permalink":"https://verrickt.github.io/categories/FluentTreeView/"}],"tags":[]},{"title":"FluentTreeView Part.3","slug":"fluent-treeview-part3","date":"2019-04-19T07:22:06.000Z","updated":"2019-04-19T09:23:58.343Z","comments":true,"path":"2019/04/19/fluent-treeview-part3/","link":"","permalink":"https://verrickt.github.io/2019/04/19/fluent-treeview-part3/","excerpt":"上一篇中我们用数据驱动界面的方式使用了TreeView。这一篇我们稍稍改一改TreeView的样式。把TreeView表示节点可以展开的小三角替换成Expander是个不错的开始。既能复习TreeView的可视化树，又避免步子太大扯到蛋。","text":"上一篇中我们用数据驱动界面的方式使用了TreeView。这一篇我们稍稍改一改TreeView的样式。把TreeView表示节点可以展开的小三角替换成Expander是个不错的开始。既能复习TreeView的可视化树，又避免步子太大扯到蛋。 自定义TreeView样式回顾一下，TreeView中我们所看到的所有Item，都是TreeViewItem负责呈现的。因此，我们需要重现实现TreeViewItem的Template。TreeViewItem要负责展示Item的内容，所以Template里应该要有一个ContentPreserenter；TreeViewItem自身也是个ItemsControl，Template里也应有一个ItemsPreserenter。除此之外就是表示Item是否可以展开的小三角，即我们要换成Expander的部分。 结合图片分析一下TreeViewItem的可视化树。首先应有两行，第一行显示当前Item，第二行显示下一级Item；其次应有两列，第一列放置小三角，第二列展示内容。应该是这样的结构：|-|Column1|Column2||———-|———-|———-||Row1|Expander|ContentPreserenter||Row2|/|ItemsPreserenter| 跟其他控件不一样的是，在设计TreeViewItem的模板时，要先不管ItemsPreserenter那部分，就像它已经做好了一样。待到把其余部分的结构都定下来了，ItemsPreserenter部分自然就如愿了。这有实际上就是递归：ItemsPreserenter里有TreeViewItem的模板需要展开，这跟当前所做的问题，即重新实现TreeViewItem的模板，是一样的。我们在ItemsPreserenter的问题已经解决的假设上，解决了TreeViewItem的模板。这像极了汉诺塔。 习惯了这样的递归思考后，把几个显示隐藏的小细节处理好，这就得到了下面的XAML: 1234567891011121314151617181920212223242526&lt;Style TargetType=\"TreeViewItem\"&gt; &lt;Setter Property=\"Template\"&gt; &lt;Setter.Value&gt; &lt;ControlTemplate TargetType=\"TreeViewItem\"&gt; &lt;Grid&gt; &lt;Grid.ColumnDefinitions&gt; &lt;ColumnDefinition Width=\"24\"&gt;&lt;/ColumnDefinition&gt; &lt;ColumnDefinition&gt;&lt;/ColumnDefinition&gt; &lt;/Grid.ColumnDefinitions&gt; &lt;Grid.RowDefinitions&gt; &lt;RowDefinition Height=\"Auto\"&gt;&lt;/RowDefinition&gt; &lt;RowDefinition Height=\"*\"&gt;&lt;/RowDefinition&gt; &lt;/Grid.RowDefinitions&gt; &lt;Expander IsExpanded=\"&#123;Binding RelativeSource=&#123;RelativeSource TemplatedParent&#125;,Path=IsExpanded,Mode=TwoWay&#125;\" x:Name=\"expander\"&gt;&lt;/Expander&gt; &lt;ContentPresenter VerticalAlignment=\"Center\" Grid.Column=\"1\" ContentSource=\"Header\"&gt;&lt;/ContentPresenter&gt; &lt;ItemsPresenter Visibility=\"&#123;Binding RelativeSource=&#123;RelativeSource TemplatedParent&#125;,Path=IsExpanded,Converter=&#123;StaticResource BoolToVisibilityConverter&#125;&#125;\" Grid.Row=\"1\" Grid.Column=\"1\"&gt;&lt;/ItemsPresenter&gt; &lt;/Grid&gt; &lt;ControlTemplate.Triggers&gt; &lt;Trigger Property=\"HasItems\" Value=\"False\"&gt; &lt;Setter TargetName=\"expander\" Property=\"Visibility\" Value=\"Collapsed\"&gt;&lt;/Setter&gt; &lt;/Trigger&gt; &lt;/ControlTemplate.Triggers&gt; &lt;/ControlTemplate&gt; &lt;/Setter.Value&gt; &lt;/Setter&gt;&lt;/Style&gt; 大功告成! 源代码参见https://github.com/Verrickt/Melchior-Sample/tree/master/FluentTreeView_Part3","categories":[{"name":"FluentTreeView","slug":"FluentTreeView","permalink":"https://verrickt.github.io/categories/FluentTreeView/"}],"tags":[]},{"title":"FluentTreeView Part.2","slug":"fluent-treeview-part2","date":"2019-04-19T06:18:40.000Z","updated":"2019-04-19T07:13:08.423Z","comments":true,"path":"2019/04/19/fluent-treeview-part2/","link":"","permalink":"https://verrickt.github.io/2019/04/19/fluent-treeview-part2/","excerpt":"使用TreeView上次说到了TreeView具有递归的结构，而在WPF中是数据驱动界面，这就要求我们的数据源也具有一定的递归结构","text":"使用TreeView上次说到了TreeView具有递归的结构，而在WPF中是数据驱动界面，这就要求我们的数据源也具有一定的递归结构 数据的准备按照数据源是否可以继续展开下去，我们将数据源定义为两个ViewModel:TreeNodeVM,TreeLeafVM，其中TreeNodeVM作为树的中间节点出现，其下可继续出现中间节点和叶子节点，TreeLeafVM作为树的叶子节点出现，不能再出现其他节点。TreeNodeVM中的子节点的个数是不定的，所以应该由集合表示。而TreeNodeVM的子节点可能是TreeNodeVM和TreeLeafVM的任意一种，这就需要他们有一个公共的父类。这给我们如下的数据定义:1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859abstract class TreeVMBase : ViewModelBase&#123; public abstract ObservableCollection&lt;TreeVMBase&gt; Items &#123; get; set; &#125; private string _name; public string Name &#123; get &#123; return _name; &#125; private set &#123; Set (ref _name, value); &#125; &#125; protected static IEnumerable&lt;TreeVMBase&gt; Empty =&gt; Enumerable.Empty&lt;TreeVMBase&gt; (); public TreeVMBase (string name) &#123; Name = name; &#125;&#125;class TreeNodeVM : TreeVMBase&#123; private ObservableCollection&lt;TreeVMBase&gt; _items; public override ObservableCollection&lt;TreeVMBase&gt; Items &#123; get &#123; return _items; &#125; set &#123; Set (ref _items, value); &#125; &#125; public TreeNodeVM (string name, IEnumerable&lt;TreeVMBase&gt; items) : base (name) &#123; _items = new ObservableCollection&lt;TreeVMBase&gt; (items??Empty); &#125;&#125;class TreeLeafVM : TreeVMBase&#123; private static readonly ObservableCollection&lt;TreeVMBase&gt; _items = new ObservableCollection&lt;TreeVMBase&gt; (); public override ObservableCollection&lt;TreeVMBase&gt; Items &#123; get &#123; return _items; &#125; set &#123; &#125; &#125; public TreeLeafVM (string name) : base (name) &#123; &#125;&#125; 我们的MainViewModel需要给TreeView提供数据源。上一节里提到，每一个TreeViewItem都是一棵树的根节点。而我们的TreeView，作为最顶级TreeViewItem的直接父节点，应该是森林。TreeView的ItemsSource的类型应该是IEnumerable&lt;TreeVMBase&gt; 12345678910111213141516171819202122232425262728293031class MainViewModel : ViewModelBase&#123; private ObservableCollection&lt;TreeVMBase&gt; _treeViewBase; public ObservableCollection&lt;TreeVMBase&gt; TreeViewBase &#123; get &#123; return _treeViewBase; &#125; set &#123; Set (ref _treeViewBase, value); &#125; &#125; public MainViewModel () &#123; var leafs = new [] &#123; new TreeLeafVM (\"Leaf1\"), new TreeLeafVM (\"Leaf2\"), new TreeLeafVM (\"Leaf3\") &#125;; var nodes = new [] &#123; new TreeNodeVM (\"Node1\", null), new TreeNodeVM (\"Node2\", leafs), new TreeNodeVM (\"Node3\", leafs.Skip (1).Take(1)) &#125;; TreeViewBase = new ObservableCollection&lt;TreeVMBase&gt; (new [] &#123; new TreeNodeVM (\"Root1\", nodes), new TreeNodeVM (\"Root2\", nodes) &#125;); &#125;&#125; 界面的准备相比有继承关系的数据，界面这边就简单一些了。界面的主要问题是，需要一个办法来告诉TreeViewItem应该如何呈现数据，应该数据上下文的哪一个属性上寻找应该生成孩子的数据源，以及递归地告诉自己的孩子如何呈现数据，如何生成孩子的孩子。 DataTemplate已经能告诉TreeViewItem如何呈现数据，关键在于如何寻找生成孩子的数据源。这个问题的答案就藏在HierarchicalDataTemplate里。Hierarchical有层级的意思，它比DataTemplate多的一个属性叫做ItemsSource。把ItemsSource设置为TreeVMBase的抽象属性Items，TreeViewItem就会自动地在数据上下文上寻找Items属性，按照属性值生成新的TreeViewItem，并且把这个过程递归地进行下去。 123456789&lt;TreeView ItemsSource=\"&#123;Binding TreeViewBase&#125;\"&gt; &lt;TreeView.ItemTemplate&gt; &lt;HierarchicalDataTemplate ItemsSource=\"&#123;Binding Items&#125;\"&gt; &lt;Grid&gt; &lt;TextBlock Text=\"&#123;Binding Name&#125;\"&gt;&lt;/TextBlock&gt; &lt;/Grid&gt; &lt;/HierarchicalDataTemplate&gt; &lt;/TreeView.ItemTemplate&gt;&lt;/TreeView&gt; 大功告成 源代码参见https://github.com/Verrickt/Melchior-Sample/tree/master/FluentTreeView","categories":[{"name":"FluentTreeView","slug":"FluentTreeView","permalink":"https://verrickt.github.io/categories/FluentTreeView/"}],"tags":[]},{"title":"FluentTreeView Part.1","slug":"fluent-treeview-part1","date":"2019-04-19T01:51:10.000Z","updated":"2019-04-19T07:13:10.857Z","comments":true,"path":"2019/04/19/fluent-treeview-part1/","link":"","permalink":"https://verrickt.github.io/2019/04/19/fluent-treeview-part1/","excerpt":"ItemsControl有时需要呈现一组逻辑上平级的控件，他们可以是一个列表，也可以是一个网格；可以横向排列，也可以纵向排列；数量可以固定，也可以按需加载；普通控件的组合无能为力。所以，我们需要新的工具。ItemsControl应运而生。","text":"ItemsControl有时需要呈现一组逻辑上平级的控件，他们可以是一个列表，也可以是一个网格；可以横向排列，也可以纵向排列；数量可以固定，也可以按需加载；普通控件的组合无能为力。所以，我们需要新的工具。ItemsControl应运而生。 ItemsControl自身是个控件，所以它具有继承自控件的布局属性(Margin,Padding,Background等)；WPF作为一个数据驱动界面的框架，它要负责根据数据生成每个Item；多个Item之间需要某种排列逻辑，这也是ItmesControl的任务；在Item可被选择的时候，选中的和未选中的要在视觉上做区分。这种视觉区分最好不需要开发者自己操心。这个光荣的任务又落在了ItemsControl的头上；独立于数据，Item自身需要排序，过滤；逻辑上能够归为一类的Item有时要放在一起，这就又牵扯到分组的了。 ItemsControl职责的列表能列很长，况且这还没说数据虚拟化和界面虚拟化呢。秉持着面向对象设计的原则，ItemsControl这里肯定牵扯到了很多的类。 事实也是如此。为了实现继承自控件的属性，ItemsControl需要把自己放在一个Panel里。为了实现数据驱动界面，ItemsControl借助了ItemContainerGenerator，一个由数据生成UI界面的类；为了解决Item之间的排布方式，引入了ItemsPanel；为了实现不需开发者操心的选中/未选状态的视觉区分，在Item外面套了一层容器(ItemContainer)。这么来看，ItemsControl的可视化树会长成这样就不奇怪了：12345678910ItemsControl Panel ItemsPreserenter ItemsPanel ItemContainer Item(DataTemplate) ItemContainer Item(DataTemplate) ItemContainer Item(DataTemplate) ItemsControl其实并不常用，日常使用接触最多的是它的子类： ListView ListBox ComboBox TreeView 相比聪明的你已经从这个系列的名字里猜出来了，我们要研究的是TreeView TreeView,TreeViewItem与HeaderedItemsControlTreeView是个很特殊的ItemsControl。列表也好，网格也好，其他的ItemsControl所能展示的都是同一等级的Item，而TreeView却可以展示多级。这其中的奥秘就藏在ItemContainer里。 每个ItemControl的子类都有其对应的ItemContainer，ListView对应ListViewItem，ComboBox对应ComboBoxItem，聪明的你，快来想想TreeView对应的是什么😋 现在思考一个问题，把ItemsControl的可视化树中的一个哪一个节点替换掉，会让ItemsControl出现多级的结构？//提示：看本节名字 揭晓答案。一个很自然的想法是，如果TreeView的ItemContainer，即TreeViewItem，自己也是一个ItemsControl的话，那就可以出现两级结构了。进一步想下去，TreeViewItem如果是个ItemsControl的话，它必然要有ItemContainer。如果这个ItemContainer恰好是TreeViewItem呢？要是这样的话，TreeViewItem下面可以再有任意多个TreeViewItem。递归的想下去，这样TreeViewItem就可以有无数级了。 其实，TreeView的实现跟我们猜想是基本一致的。只是我们的猜想中，TreeViewItem里没有位置用来呈现当前的Item，这就引入了另一个类，HeaderedItemsControl。它继承自ItemsControl，添加了Header属性。这个Header属性就可以用来呈现我们的Item。 那么TreeView的可视化树是这样的123456789101112131415161718TreeView Panel ItemsPreserenter ItemsPanel TreeViewItem Header Item(DataTemplate) ItemsPreserenter ItemsPanel TreeViewItem TreeViewItem TreeViewItem Header Item(DataTemplate) ItemsPreserenter ItemsPanel TreeViewItem TreeViewItem 其中每一级的TreeViewItem都可以展开为这样的结构：123456Header Item(DataTemplate)ItemsPreserenter ItemsPanel TreeViewItem TreeViewItem 在WPF这个数据驱动界面的UI框架里，想要呈现递归的UI界面，最自然的方法就是把数据也变得递归化。即，数据也要变成树状结构。(严格来说应该是数据结构中森林的概念。TreeView是森林，TreeViewItem则是一棵树。对应地，数据也要变成森林的结构。即，树状的集合) 既然UI结构已经递归化，原来指明单层Items如何呈现的ItemTemplate已经不够用了。HierarchicalItemTemplate闪亮登场。最重要的属性，ItemsSource是干什么的，我们卖个关子。 FluentTreeView终于要讲讲什么是FluentTreeView了。作为一个十多岁的UI框架，WPF已经进入了维护期。新出生的UWP功能尚且不全，导致很多应用只得使用WPF。相比UWP，WPF的逊色的地方就在于外观。每次功能更新，UWP默认的外观就向Windows的未来的样子，Fluent design靠近一步。为了让WPF也能变得更现代一些，本系列旨在将WPF控件Fluent design化。 空口无凭，来看看我们的目标 左侧有当前主题色(Accent color)的选中高亮指示 鼠标高亮撑满整个控件宽度 在实现FluentTreeView的时候，经常ItemsControl和TreeView的相关知识。所以看不懂后面的时候，尽管回来翻看第一篇。那么我们下篇见。","categories":[{"name":"FluentTreeView","slug":"FluentTreeView","permalink":"https://verrickt.github.io/categories/FluentTreeView/"}],"tags":[]},{"title":"获取程序的编译时间","slug":"get-link-time-from-csharp","date":"2018-10-22T15:16:44.000Z","updated":"2018-10-22T16:23:49.590Z","comments":true,"path":"2018/10/22/get-link-time-from-csharp/","link":"","permalink":"https://verrickt.github.io/2018/10/22/get-link-time-from-csharp/","excerpt":"日常新需求新的需求又来了。这次是程序在编译后6个月拒绝启动。BETA性质的软件都有类似的需求。但大部分软件要么是启动时检查更新，要么是联网判断是否过期。对于我们现在做的这个小工具太小题大做了。根据编译时间判断过期的需求看似奇葩，也是有点道理的。","text":"日常新需求新的需求又来了。这次是程序在编译后6个月拒绝启动。BETA性质的软件都有类似的需求。但大部分软件要么是启动时检查更新，要么是联网判断是否过期。对于我们现在做的这个小工具太小题大做了。根据编译时间判断过期的需求看似奇葩，也是有点道理的。 实现遇事不决问SO，我们输入”C# get compile time”找到的第一个问题就是了。 方法有很多，大致分为这几类。 读取PE头部时间戳 添加Build Task将编译时间以资源嵌入程序集 读取文件创建时间 严格意义上说，文件创建时间与文件系统相关，依赖程序外部，碰到不保留创建时间的操作就只能干瞪眼了，所以排除掉。 而添加Build Task又需要程序读取资源，反序列化云云，比较麻烦。因此主要关注1中的方法。 .NET的程序集都包含PE头部。先来一张图感受一下。 我们需要的字段在COFFHeader的TimeDateStamp处。 需要注意的一点是，图中的偏移量是相对PE头的，而在PE头部之前还有DOS头部。 再来看看StackOverflow上的答案，是不是比较直观呢？ 12345678910111213141516171819202122232425262728293031323334353637383940// see https://stackoverflow.com/a/1601079struct _IMAGE_FILE_HEADER&#123; public ushort Machine; public ushort NumberOfSections; public uint TimeDateStamp; public uint PointerToSymbolTable; public uint NumberOfSymbols; public ushort SizeOfOptionalHeader; public ushort Characteristics;&#125;;static DateTime GetBuildDateTime(Assembly assembly)&#123; var path = assembly.GetName().CodeBase; if (File.Exists(path)) &#123; var buffer = new byte[Math.Max(Marshal.SizeOf(typeof(_IMAGE_FILE_HEADER)), 4)]; using (var fileStream = new FileStream(path, FileMode.Open,FileAccess.Read)) &#123; fileStream.Position = 0x3C; fileStream.Read(buffer, 0, 4); fileStream.Position = BitConverter.ToUInt32(buffer, 0); // COFF header offset fileStream.Read(buffer, 0, 4); // \"PE\\0\\0\" fileStream.Read(buffer, 0, buffer.Length); &#125; var pinnedBuffer = GCHandle.Alloc(buffer, GCHandleType.Pinned); try &#123; var coffHeader = (_IMAGE_FILE_HEADER)Marshal.PtrToStructure(pinnedBuffer.AddrOfPinnedObject(),typeof(_IMAGE_FILE_HEADER)); return TimeZone.CurrentTimeZone.ToLocalTime(new DateTime(1970, 1, 1) + new TimeSpan(coffHeader.TimeDateStamp * TimeSpan.TicksPerSecond)); &#125; finally &#123; pinnedBuffer.Free(); &#125; &#125; return new DateTime();&#125; 从Ox3C位置读到PE头部的位置后Seek到该位置，读取内容后将其转换为自定义的_IMAGE_FILE_HEADER结构，读取TimeDateStamp即可。 直接运行的结果是1900/1/1 12:00:00，不对。原因是AssemblyName.CodeBase属性返回的并不是程序集所在路径，而是File scheme的URI file:\\\\\\c:\\MyDirectory\\MyAssemlby.exe。使用AssemblyName后问题解决。从Ox3C位置读到PE头部的位置后Seek到该位置，读取内容后将其转换为自定义的_IMAGE_FILE_HEADER.TimeDateStamp，读取TimeDateStamp即可。PE头部的TimeDateStamp字段是从unix epoll算起的，8102年的.NET中也有专门处理这里情况的DateTimeOffSet，而异常情况我们完全可以返回null。修改后代码如下 123456789101112131415161718192021222324252627282930static DateTimeOffset? GetBuildDateTime(this Assembly assembly)&#123; var path = assembly.Location; if (File.Exists(path)) &#123; var buffer = new byte[Math.Max(Marshal.SizeOf(typeof(_IMAGE_FILE_HEADER)), 4)]; using (var fileStream = new FileStream(path, FileMode.Open, FileAccess.Read)) &#123; fileStream.Position = 0x3C; fileStream.Read(buffer, 0, 4); fileStream.Position = BitConverter.ToUInt32(buffer, 0); // COFF header offset fileStream.Read(buffer, 0, 4); // \"PE\\0\\0\" fileStream.Read(buffer, 0, buffer.Length); &#125; var pinnedBuffer = GCHandle.Alloc(buffer, GCHandleType.Pinned); try &#123; var coffHeader = (_IMAGE_FILE_HEADER)Marshal.PtrToStructure(pinnedBuffer.AddrOfPinnedObject(), typeof(_IMAGE_FILE_HEADER)); return DateTimeOffset.FromUnixTimeSeconds(coffHeader.TimeDateStamp); &#125; finally &#123; pinnedBuffer.Free(); &#125; &#125; else &#123; return null; &#125;&#125; 修改后发现返回的是一个在2090年之后的日期，还是不对。 Deterministic Build 来背锅这次的原因是MS在某个Roslyn版本中默认开启了Deterministic Build ….. The /deterministic flag causes the compiler to emit the exact same EXE / DLL, byte for byte, when given the same inputs. 既然输入相同输出必定相同，那可能会变的部分就只能固定下来了。例如时间戳。 … the MVID, PDB ID and Timestamp are the core issues to solve for deterministic builds. MS选了一个比较折中的方案——由文件内容计算 Why not just use all 0s for the timestamp? This is actually how the original implementation of determinism functioned in the compiler. Unfortunately it turned out there were a lot of tools we used in our internal process that validated the timestamp. They got a bit cranky when the discovered binaries claiming to be written in 1970, over 25 years before .NET was even invented. The practice of validating the time stamp is questionable but given tools were doing it there was a significant back compat risk. Hence we moved to the current computed value and haven’t seen any issues since then. 所以我们得到一个奇怪的值也是Work as expected了🙃 解决办法非常简单，在.csproj中将1&lt;deterministic&gt;true&lt;/deterministic&gt; 改为1&lt;deterministic&gt;false&lt;/deterministic&gt; 即可。需要注意的是，Deterministic Build在.NET Core上默认开启，要使PE头部的TimeDateStamp有意义需要将其关闭。而在.NET Framework上则是只有在VS2017的某个特定版本后新建的工程才会开启。 题外话，这个默认开启的Deterministic Build还搞出了其他幺蛾子，例如这里","categories":[],"tags":[{"name":"C#","slug":"C","permalink":"https://verrickt.github.io/tags/C/"}]},{"title":"使用MSBuild编写构建脚本","slug":"build-script-with-msbuild","date":"2018-08-30T15:05:54.000Z","updated":"2018-09-01T08:17:17.105Z","comments":true,"path":"2018/08/30/build-script-with-msbuild/","link":"","permalink":"https://verrickt.github.io/2018/08/30/build-script-with-msbuild/","excerpt":"背景项目需求变更，需要从一份代码里编译出好几个不同的版本。编译和部署的复杂度都成指数增加，简单的Release构建搞不定了，写构建脚本迫在眉睫。","text":"背景项目需求变更，需要从一份代码里编译出好几个不同的版本。编译和部署的复杂度都成指数增加，简单的Release构建搞不定了，写构建脚本迫在眉睫。 大致介绍下项目的组成吧。整个项目由三部分组成： Installer.vcxproj Main.csproj Updater.csproj Main是项目本体。Installer负责项目的安装和卸载,Updater负责项目的更新。解决方案里除Installer使为C++外，其余均为C#。 解决方案的结构如下：1234MyProject.sln- Installer.vcxproj - Main.csproj- Updater.csproj 现在需要从MyProject.sln条件编译出多个版本，要求 对于每个版本，Installer和Main须重新编译 所有版本共享同一份Updater的二进制。 Roslyn与MSBuild构建的过程自然离不开编译。编译器虽软能将代码转化为二进制，但这个转换的单位是文件:12gcc --helpUsage: gcc [options] file... 作为.NET的编译器，Roslyn也不例外:123456789101112131415161718192021csc /?......... - INPUT FILES - /recurse:&lt;wildcard&gt; Include all files in the current directory and subdirectories according to the wildcard specifications /reference:&lt;alias&gt;=&lt;file&gt; Reference metadata from the specified assembly file using the given alias (Short form: /r) /reference:&lt;file list&gt; Reference metadata from the specified assembly files (Short form: /r) /addmodule:&lt;file list&gt; Link the specified modules into this assembly /link:&lt;file list&gt; Embed metadata from the specified interop assembly files (Short form: /l) /analyzer:&lt;file list&gt; Run the analyzers from this assembly (Short form: /a) /additionalfile:&lt;file list&gt; Additional files that don&apos;t directly affect code generation but may be used by analyzers for producing errors or warnings. /embed Embed all source files in the PDB. /embed:&lt;file list&gt; Embed specific files in the PDB 我们不会把所有源文件的路径、所有引用的程序集的都告诉编译器，这样既低效又不利于维护。最好有个能有个程序，能把所有编译的参数都记下来。这个过程最好能自动化：在文件夹里添加一个源文件、新增一个引用项，它都自动的更新编译参数。存在这样的工具吗？当然了。在Visual Studio里添加一个文件是不需要动编译选项的。有心的人会发现，Visual Studio在.csproj记录了新增的文件。.csproj是项目文件。一个叫MSBuild的工具可以解析.csproj，并生成对应的编译选项调用编译器。这样的工具称为构建自动化工具。用微软自己的话说，是构建引擎。大型软件的开发离不开构建引擎。与MSBuild同类的工具还有有很多常见的有make,maven等。 相比于Roslyn，MSBuild的使用就简单多了，只需要指定项目文件就万事大吉了。下面是在一个新建WPF项目下调用MSbuild的输出。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101C:\\Users\\Von\\source\\repos\\WpfApp1&gt;msbuild WpfApp1.slnMicrosoft (R) Build Engine version 15.7.180.61344 for .NET FrameworkCopyright (C) Microsoft Corporation. All rights reserved.Building the projects in this solution one at a time. To enable parallel build, please add the \"/m\" switch.Build started 9/1/2018 2:08:01 PM.Project \"C:\\Users\\Von\\source\\repos\\WpfApp1\\WpfApp1.sln\" on node 1 (default targets).ValidateSolutionConfiguration: Building solution configuration \"Debug|Any CPU\".Project \"C:\\Users\\Von\\source\\repos\\WpfApp1\\WpfApp1.sln\" (1) is building \"C:\\Users\\Von\\source\\repos\\WpfApp1\\WpfApp1\\WpfApp1.csproj\" (2) on node 1 (default targets).GenerateBindingRedirects: No suggested binding redirects from ResolveAssemblyReferences.Project \"C:\\Users\\Von\\source\\repos\\WpfApp1\\WpfApp1\\WpfApp1.csproj\" (2) is building \"C:\\Users\\Von\\source\\repos\\WpfApp1\\WpfApp1\\WpfApp1_r1lsxnqk_wpftmp.csproj\" (3) on node 1 (_CompileTemporaryAssembly target(s)).CoreCompile: C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\MSBuild\\15.0\\bin\\Roslyn\\csc.exe /noconfig /nowarn:1701, 1702 /nostdlib+ /platform:anycpu32bitpreferred /errorreport:prompt /warn:4 /define:DEBUG;TRACE /highentropyva+ /refer ence:\"C:\\Program Files (x86)\\Reference Assemblies\\Microsoft\\Framework\\.NETFramework\\v4.7.1\\Microsoft.CSharp.dll\" /ref erence:\"C:\\Program Files (x86)\\Reference Assemblies\\Microsoft\\Framework\\.NETFramework\\v4.7.1\\mscorlib.dll\" /reference :\"C:\\Program Files (x86)\\Reference Assemblies\\Microsoft\\Framework\\.NETFramework\\v4.7.1\\PresentationCore.dll\" /referen ce:\"C:\\Program Files (x86)\\Reference Assemblies\\Microsoft\\Framework\\.NETFramework\\v4.7.1\\PresentationFramework.dll\" / reference:\"C:\\Program Files (x86)\\Reference Assemblies\\Microsoft\\Framework\\.NETFramework\\v4.7.1\\System.Core.dll\" /ref erence:\"C:\\Program Files (x86)\\Reference Assemblies\\Microsoft\\Framework\\.NETFramework\\v4.7.1\\System.Data.DataSetExten sions.dll\" /reference:\"C:\\Program Files (x86)\\Reference Assemblies\\Microsoft\\Framework\\.NETFramework\\v4.7.1\\System.Da ta.dll\" /reference:\"C:\\Program Files (x86)\\Reference Assemblies\\Microsoft\\Framework\\.NETFramework\\v4.7.1\\System.dll\" /reference:\"C:\\Program Files (x86)\\Reference Assemblies\\Microsoft\\Framework\\.NETFramework\\v4.7.1\\System.Net.Http.dll\" /reference:\"C:\\Program Files (x86)\\Reference Assemblies\\Microsoft\\Framework\\.NETFramework\\v4.7.1\\System.Xaml.dll\" /r eference:\"C:\\Program Files (x86)\\Reference Assemblies\\Microsoft\\Framework\\.NETFramework\\v4.7.1\\System.Xml.dll\" /refer ence:\"C:\\Program Files (x86)\\Reference Assemblies\\Microsoft\\Framework\\.NETFramework\\v4.7.1\\System.Xml.Linq.dll\" /refe rence:\"C:\\Program Files (x86)\\Reference Assemblies\\Microsoft\\Framework\\.NETFramework\\v4.7.1\\WindowsBase.dll\" /debug+ /debug:full /filealign:512 /optimize- /out:obj\\Debug\\WpfApp1.exe /ruleset:\"C:\\Program Files (x86)\\Microsoft Visual St udio\\2017\\Community\\Team Tools\\Static Analysis Tools\\\\Rule Sets\\MinimumRecommendedRules.ruleset\" /subsystemversion:6. 00 /target:winexe /utf8output App.xaml.cs MainWindow.xaml.cs Properties\\AssemblyInfo.cs Properties\\Resources.Designer .cs Properties\\Settings.Designer.cs C:\\Users\\Von\\source\\repos\\WpfApp1\\WpfApp1\\obj\\Debug\\MainWindow.g.cs C:\\Users\\Von\\ source\\repos\\WpfApp1\\WpfApp1\\obj\\Debug\\App.g.cs Using shared compilation with compiler from directory: C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\ MSBuild\\15.0\\bin\\RoslynDone Building Project \"C:\\Users\\Von\\source\\repos\\WpfApp1\\WpfApp1\\WpfApp1_r1lsxnqk_wpftmp.csproj\" (_CompileTemporaryAssembly target(s)).MarkupCompilePass2: MarkupCompilePass2 successfully generated BAML or source code files.CleanupTemporaryTargetAssembly: Deleting file \"obj\\Debug\\WpfApp1.exe\".CoreResGen: \"C:\\Program Files (x86)\\Microsoft SDKs\\Windows\\v10.0A\\bin\\NETFX 4.6.1 Tools\\resgen.exe\" /useSourcePath /r:\"C:\\Program Files (x86)\\Reference Assemblies\\Microsoft\\Framework\\.NETFramework\\v4.7.1\\Microsoft.CSharp.dll\" /r:\"C:\\Program Files (x86)\\Reference Assemblies\\Microsoft\\Framework\\.NETFramework\\v4.7.1\\mscorlib.dll\" /r:\"C:\\Program Files (x86)\\Referen ce Assemblies\\Microsoft\\Framework\\.NETFramework\\v4.7.1\\PresentationCore.dll\" /r:\"C:\\Program Files (x86)\\Reference Ass emblies\\Microsoft\\Framework\\.NETFramework\\v4.7.1\\PresentationFramework.dll\" /r:\"C:\\Program Files (x86)\\Reference Asse mblies\\Microsoft\\Framework\\.NETFramework\\v4.7.1\\System.Core.dll\" /r:\"C:\\Program Files (x86)\\Reference Assemblies\\Micr osoft\\Framework\\.NETFramework\\v4.7.1\\System.Data.DataSetExtensions.dll\" /r:\"C:\\Program Files (x86)\\Reference Assembli es\\Microsoft\\Framework\\.NETFramework\\v4.7.1\\System.Data.dll\" /r:\"C:\\Program Files (x86)\\Reference Assemblies\\Microsof t\\Framework\\.NETFramework\\v4.7.1\\System.dll\" /r:\"C:\\Program Files (x86)\\Reference Assemblies\\Microsoft\\Framework\\.NET Framework\\v4.7.1\\System.Net.Http.dll\" /r:\"C:\\Program Files (x86)\\Reference Assemblies\\Microsoft\\Framework\\.NETFramewo rk\\v4.7.1\\System.Xaml.dll\" /r:\"C:\\Program Files (x86)\\Reference Assemblies\\Microsoft\\Framework\\.NETFramework\\v4.7.1\\S ystem.Xml.dll\" /r:\"C:\\Program Files (x86)\\Reference Assemblies\\Microsoft\\Framework\\.NETFramework\\v4.7.1\\System.Xml.Li nq.dll\" /r:\"C:\\Program Files (x86)\\Reference Assemblies\\Microsoft\\Framework\\.NETFramework\\v4.7.1\\WindowsBase.dll\" /co mpile Properties\\Resources.resx,obj\\Debug\\WpfApp1.Properties.Resources.resources Processing resource file \"Properties\\Resources.resx\" into \"obj\\Debug\\WpfApp1.Properties.Resources.resources\".CoreCompile: C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\MSBuild\\15.0\\bin\\Roslyn\\csc.exe /noconfig /nowarn:1701, 1702 /nostdlib+ /platform:anycpu32bitpreferred /errorreport:prompt /warn:4 /define:DEBUG;TRACE /highentropyva+ /refer ence:\"C:\\Program Files (x86)\\Reference Assemblies\\Microsoft\\Framework\\.NETFramework\\v4.7.1\\Microsoft.CSharp.dll\" /ref erence:\"C:\\Program Files (x86)\\Reference Assemblies\\Microsoft\\Framework\\.NETFramework\\v4.7.1\\mscorlib.dll\" /reference :\"C:\\Program Files (x86)\\Reference Assemblies\\Microsoft\\Framework\\.NETFramework\\v4.7.1\\PresentationCore.dll\" /referen ce:\"C:\\Program Files (x86)\\Reference Assemblies\\Microsoft\\Framework\\.NETFramework\\v4.7.1\\PresentationFramework.dll\" / reference:\"C:\\Program Files (x86)\\Reference Assemblies\\Microsoft\\Framework\\.NETFramework\\v4.7.1\\System.Core.dll\" /ref erence:\"C:\\Program Files (x86)\\Reference Assemblies\\Microsoft\\Framework\\.NETFramework\\v4.7.1\\System.Data.DataSetExten sions.dll\" /reference:\"C:\\Program Files (x86)\\Reference Assemblies\\Microsoft\\Framework\\.NETFramework\\v4.7.1\\System.Da ta.dll\" /reference:\"C:\\Program Files (x86)\\Reference Assemblies\\Microsoft\\Framework\\.NETFramework\\v4.7.1\\System.dll\" /reference:\"C:\\Program Files (x86)\\Reference Assemblies\\Microsoft\\Framework\\.NETFramework\\v4.7.1\\System.Net.Http.dll\" /reference:\"C:\\Program Files (x86)\\Reference Assemblies\\Microsoft\\Framework\\.NETFramework\\v4.7.1\\System.Xaml.dll\" /r eference:\"C:\\Program Files (x86)\\Reference Assemblies\\Microsoft\\Framework\\.NETFramework\\v4.7.1\\System.Xml.dll\" /refer ence:\"C:\\Program Files (x86)\\Reference Assemblies\\Microsoft\\Framework\\.NETFramework\\v4.7.1\\System.Xml.Linq.dll\" /refe rence:\"C:\\Program Files (x86)\\Reference Assemblies\\Microsoft\\Framework\\.NETFramework\\v4.7.1\\WindowsBase.dll\" /debug+ /debug:full /filealign:512 /optimize- /out:obj\\Debug\\WpfApp1.exe /ruleset:\"C:\\Program Files (x86)\\Microsoft Visual St udio\\2017\\Community\\Team Tools\\Static Analysis Tools\\\\Rule Sets\\MinimumRecommendedRules.ruleset\" /subsystemversion:6. 00 /resource:obj\\Debug\\WpfApp1.g.resources /resource:obj\\Debug\\WpfApp1.Properties.Resources.resources /target:winexe /utf8output App.xaml.cs MainWindow.xaml.cs Properties\\AssemblyInfo.cs Properties\\Resources.Designer.cs Properties\\Set tings.Designer.cs C:\\Users\\Von\\source\\repos\\WpfApp1\\WpfApp1\\obj\\Debug\\MainWindow.g.cs C:\\Users\\Von\\source\\repos\\WpfAp p1\\WpfApp1\\obj\\Debug\\App.g.cs \"C:\\Users\\Von\\AppData\\Local\\Temp\\.NETFramework,Version=v4.7.1.AssemblyAttributes.cs\" Using shared compilation with compiler from directory: C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\ MSBuild\\15.0\\bin\\Roslyn_CopyAppConfigFile: Copying file from \"App.config\" to \"bin\\Debug\\WpfApp1.exe.config\".CopyFilesToOutputDirectory: Copying file from \"obj\\Debug\\WpfApp1.exe\" to \"bin\\Debug\\WpfApp1.exe\". WpfApp1 -&gt; C:\\Users\\Von\\source\\repos\\WpfApp1\\WpfApp1\\bin\\Debug\\WpfApp1.exe Copying file from \"obj\\Debug\\WpfApp1.pdb\" to \"bin\\Debug\\WpfApp1.pdb\".Done Building Project \"C:\\Users\\Von\\source\\repos\\WpfApp1\\WpfApp1\\WpfApp1.csproj\" (default targets).Done Building Project \"C:\\Users\\Von\\source\\repos\\WpfApp1\\WpfApp1.sln\" (default targets).Build succeeded. 0 Warning(s) 0 Error(s)Time Elapsed 00:00:02.60 看看CoreCompile里传给Roslyn的参数。自己调用Roslyn的话光给出正确的参数就让人头痛了。所以还是直接调用MSBuild好了。单单调用一次MSBuild无法完成我们的需求，因此构建脚本是少不了的了。 找到MSBuild首先在`Developer command prompt for VS 2017找到MSbuiid的完全路径:123$ where msbuild&gt; C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\MSBuild\\15.0\\Bin\\MSBuild.exe&gt; C:\\Windows\\Microsoft.NET\\Framework\\v4.0.30319\\MSBuild.exe 这里需要的是Visual Studio目录下的那一个。 MSBuild的命令行选项 /t:Rebuild强制重新编译。 /p:Configuration=RELEASE以Release配置编译。会开启优化。 /p:DefineConstants=&quot;Value1,Value2,...&quot;定义条件编译常量。 /p:Platform= 指定目标架构。x86,x64和AnyCpu三选一。 /p:OutputPath输出目录。可接受绝对和相对路径。 各个参数中间用空格分开。 一个例子1msbuild WpfApp1.csproj /t:Rebuild /p:Configuration=RELEASE /p:DefineConstants=&quot;TRACE,DEMO&quot; /p:Platform=&quot;x86&quot; /p:OutputPath=&quot;F:\\Release&quot; /p:TargetFrameworkVersion=4.7.1 /tv:15.0 更多参数请参阅这里 能配好参数调用MSBuild是构建脚本中最重要的一步。至于需求里的其他要求没什么难度，无非用合适的控制流去调用MSBuild。这里就略过了。 踩坑部分参数无效如果你发现你设置的部分参数无效，那可能是被因为项目文件的参数覆盖掉了。为了少掉坑，请将项目文件的路径放作为给MSBuild的第一个参数。 C++工程指定输出目录无效配好/p:Platform后MSbuild会提示你用/p:OutputPath指定输出路径。如果你用了OutputPath，那么恭喜你掉进了微软挖的暗坑：OutputPath对C++项目无效。解决方法：请使用使用/p:OutputDir指定输出目录原因： To expand on what @AndyGerlicher said, we can’t do what you’re asking because we too have lost the reasoning behind this decision. The current team thinks it looks pretty broken. From comments in the targets OutDir and OutputPath are distinguished for legacy reasons, and OutDir should be used if at all possible. It seems like we got stuck in the middle of a transition. However, as you’ve discovered, there’s a ton of MSBuild code out there that exploits the differences between the two variables. That keeps us from completing the transition (or for that matter backing it out), because it would cause a lot of churn in customer projects. 更多详情请参阅这里 C++工程DefineConstants无效喜闻乐见的legacy reasons 比较简单的一个workaround 在.vcxproj的Label为Global的PropertyGroup中加上&lt;DefineConstants&gt;&lt;/DefineConstants&gt; 在所有的PreprocessorDefinitions内容的开头添加$(DefineConstants); 最终结果如下 12345678910111213141516&lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;&lt;Project DefaultTargets=\"Build\" ToolsVersion=\"14.0\" xmlns=\"http://schemas.microsoft.com/developer/msbuild/2003\"&gt; ..... &lt;PropertyGroup Label=\"Global\"&gt; &lt;DefineConstants&gt;&lt;/DefineConstants&gt; &lt;/PropertyGroup&gt; ...... &lt;ItemDefinitionGroup&gt; &lt;ClCompile&gt; .... &lt;PreprocessorDefinitions&gt;$(DefineConstants);NDEBUG;WIN32;_WINDOWS;NO_EXP10;_CRT_SECURE_NO_WARNINGS;%(PreprocessorDefinitions);&lt;/PreprocessorDefinitions&gt; .... &lt;/ClCompile&gt; &lt;/ItemDefinitionGroup&gt;&lt;/Project&gt; 更多详情请参阅这里 对PowerShell的吐槽本以为基于.NET对象的PowerShell挺好上手的。没想到由JSON反序列化出来的对象是PSObject类型，原来类里的方法都用不了了，想把PSObject转换回去也十分麻烦。可能Shell天生跟OOP合不来吧。最后用C#写了构建脚本。C#，脚本，是不是哪里不太对啊🤔","categories":[],"tags":[{"name":"MSBuild","slug":"MSBuild","permalink":"https://verrickt.github.io/tags/MSBuild/"}]},{"title":"Build a (partial) self-contained WPF application","slug":"self-contained-wpf-application","date":"2018-07-31T14:43:47.000Z","updated":"2018-08-03T03:14:31.174Z","comments":true,"path":"2018/07/31/self-contained-wpf-application/","link":"","permalink":"https://verrickt.github.io/2018/07/31/self-contained-wpf-application/","excerpt":"巨人的肩膀新事物的产生总是与老事物有千丝万缕的联系。或是从中得到启发，或是对其全面改良。新事物的源头通常可以追溯到很久远的一些概念上。因此有了「站在巨人的肩膀上」 这样的说法。在程序设计里面，「巨人们的肩膀」 就是我们的应用程序使用的库了。踩在这些「巨人」们的肩膀上我们的程序才得以重见天日；为了实现一个库，有时候会使用到其他的库。我们所依赖的「巨人」又踩在了其他「巨人」的肩膀上，把依赖关系变成了树状结构，我们的程序处在根节点。 扯远了:)","text":"巨人的肩膀新事物的产生总是与老事物有千丝万缕的联系。或是从中得到启发，或是对其全面改良。新事物的源头通常可以追溯到很久远的一些概念上。因此有了「站在巨人的肩膀上」 这样的说法。在程序设计里面，「巨人们的肩膀」 就是我们的应用程序使用的库了。踩在这些「巨人」们的肩膀上我们的程序才得以重见天日；为了实现一个库，有时候会使用到其他的库。我们所依赖的「巨人」又踩在了其他「巨人」的肩膀上，把依赖关系变成了树状结构，我们的程序处在根节点。 扯远了:) 开发的时候有包管理工具帮我们管理依赖，而到了分发的时候，需要一个容器把我们的程序和它的依赖项一起分发。这就需要用到安装程序。最终用户只要拿到安装程序，运行安装，由安装程序去操心依赖项到底应该放在哪。 人是懒惰的动物，从用户发现软件到真正用上软件之间，每多一个步骤都会让损失一批潜在用户。而现在随着手机的流行，用户已经不想安装了。他们只想下载「软件」，双击就能直接运行。至于什么安装路径，UAC权限之类的用户才不想操心呢。 这就要求我们的程序要将安装这个过程隐藏起来。在用户看不到的情况下部署自己的依赖项。这样的程序在英文里被称为 self-contained self-containedadjective (of a thing) complete, or having all that is needed, in itself. (of a person) quiet and independent; not depending on or influenced by others. 这几天我们也有了将最终程序self-contain化的需求。终于可以合理的抛弃MFC写的Installer了😁 Self-contained in .NET.NET在设计之初只是想提高Windows程序员的开发效率，顺便解决一下DLL Hell。至于应用分发根本就不再日程上。就算真的考虑过，也一定会采用动态链接的方式。因为当时的硬盘还是很贵滴。 总之，.NET就这样决定采用动态链接了。.NET里的几个基本概念也都与动态链接脱不开关系： 程序集)是.NET世界里最常见的分发单元。程序集有独立的版本号。在引用其他程序集的时候，需要显式指明对应的版本。这样，相同名字不同版本的程序集就可以被区别开，以此为基础就解决Dll hell的问题。CLR在运行的时候以一套复杂的规则试图加载程序的依赖项。 看起来似乎.NET与静态链接无缘了。不过也有好消息，在Build 2018开发者大会上，微软宣布现有的桌面程序可以在明年推出的.Net core 3上选择与运行时静态链接在一起，将整个程序变为单一的可执行文件。 Side-by-side and App-local Deployment For cases where the maximum isolation is required, you can deploy .NET Core with your application. We’re working on new build tools that will bundle your app and .NET Core together as in a single executable, as a new option. We’ve had requests for deployment options like this for many years, but were never able to deliver those with the .NET Framework. The much more modular architecture used by .NET Core makes these flexible deployment options possible. .Net Core 3.0的推出时间是明年，我们显然不能等到那个时候。为了实现需求，先向搜索引擎求助吧。 静态链接静态链接将依赖项打包进我们的程序，生成单一的二进制文件。这是个很直观的切入点。以C# static link为关键词，发现有几个同类型的工具。例如ILMerge。在.NET的世界中，源代码经过编译后产生的程序集里存储的并不是机器代码而是一种叫MSIL的中间代码。程序集由CLR加载后被JIT即时编译为机器码。 ILMerge之类的工具工作在IL层面。它们将不同程序集的IL代码粘合起来，生成单一的程序集。 听起来是不是很美好？如果你写个小demo的话会发现确实很好用。但是ILMerge无法对WPF中的XAML资源进行改写。程序挂在运行时。 内嵌资源.NET中的程序集有资源的概念。任何文件都能以资源的形式嵌入进程序集。另一个思路是是把依赖项当作资源嵌进我们的主程序。只要能在运行时把它们暴露给CLR，就能实现self-contain。先来看看API： Assembly.GetManifestResourceStream(string name): 12//Loads the specified manifest resource from this assembly.public virtual System.IO.Stream GetManifestResourceStream (Type type, string name) AppDomain.Load(Byte[]): 12//Loads the Assembly with a common object file format (COFF) based image containing an emitted Assembly.public System.Reflection.Assembly Load (byte[] rawAssembly); AppDomain.AssemblyResolve 12//Occurs when the resolution of an assembly fails.public event ResolveEventHandler AssemblyResolve; 我们把依赖项嵌入到程序内部后，CLR找不到引用的程序集的时候就会触发AppDomain.AssemblyResolve事件。我们可以在这里拿到依赖的AssemblyName,Assembly.GetManifestResourceStream(string name)得到包含依赖项的的stream，把stream的内容读出来放到byte[]里，调用AppDomain.Load(Byte[])将程序集加载。 大致过程走得通，不过有点麻烦。hardcode依赖的名字后，以后添加新依赖项还要更改hardcode的名字。毕竟程序员也是人，也想偷懒。这些活最好自动化。要是能和VS里的编译结合起来就更好了🤤 你别说，还真有这样的工具。Fody可以对.NET程序集做多种操作。Costura是Fody的一个扩展，专门用来将依赖嵌入成资源。Fody与VS的完美集成，编译完成后自动开始操作。Nuget安装完Fody和Costura后，如果解决方案根路径下没有FodyWeavers.xml则新建它。把内容替换为 1234&lt;?xml version=\"1.0\" encoding=\"utf-8\" ?&gt;&lt;Weavers&gt; &lt;Costura/&gt;&lt;/Weavers&gt;` 就完事了。太易用了。我做了测试，WPF和控制台应用都可以。看文档，它还支持.NET Core。 Dependencies of my dependencies, is not my dependenciesILMerge也好，Fody也好，它们解决的都是应用程序所依赖库的问题。而我们的程序之所以叫.NET程序，是因为它依赖.NET Framework。如果要做到fully-self-contained的话，我们还需要把.NET Framework也塞进去。这就意味这我们需要一个不依赖.NET Framework的程序来释放.NET Framework和我们的应用程序。这其实就是重新发明安装程序了。退一步说，就算我们真的把.NET Framework打包进去了，因为.NET Framework的安装过程比较耗时，用户在点了按钮后几秒钟如果UI还没出来可能会认为我们的程序有问题。因此，把程序和.NET Framework一起分发的事情就交给愿意折腾的同学们了。我还是期待下.NET Core 3.0吧。 总结如果你的程序是WPF程序，请使用Fody.Costura； 如果你的程序不是WPF程序，那么可以任选ILMerge和Fody.Costura的一个。 如果你的程序需要与.NET Framework一起分发，请在处理主程序后使用支持静默安装的Installer将主程序与.NET运行时一起打包。 或者等.Net Core 3.0发布 Reference .Net Core 3.0 - blogs.msdn.microsoft.com How the Runtime Locates Assemblies - docs.microsoft.com Fody - github.com Costura - github.com .NET Framework deployment guide for developers - docs.microsoft.com","categories":[],"tags":[{"name":"WPF","slug":"WPF","permalink":"https://verrickt.github.io/tags/WPF/"},{"name":".Net Framework","slug":"Net-Framework","permalink":"https://verrickt.github.io/tags/Net-Framework/"}]},{"title":"引用类型的开销","slug":"overhead-of-reference-types","date":"2018-07-19T12:26:05.000Z","updated":"2018-08-03T03:14:39.131Z","comments":true,"path":"2018/07/19/overhead-of-reference-types/","link":"","permalink":"https://verrickt.github.io/2018/07/19/overhead-of-reference-types/","excerpt":"值类型与引用类型C#中的类型分为引用类型和值类型。使用struct或enum关键字修饰的类型定义是值类型，使用class或delegate关键字修饰的类型是引用类型。引用类型和值类型各有限制，分别适用于不同的场景。不同于C++，C#中的值类型只能分配在栈上*1，引用类型只能分配在GC堆上。C#中的GC是精确式GC，这就对GC堆上的指针有了一些要求。这是引用类型有开销的原因之一。 Sync block index 与 Type object pointer读过C# vir CLR的同学会知道，引用类型的开销是Sync blockindex和Type object pointer。他们的长度是都是一个字长。即在32位CLR上是4字节，在64位CLR上是8字节。Sync block index在CLR中是用于实现lock,Monitor等线程同步原语，Type object pointer是指向当前对象运行时类型信息的一个指针。Sync block index与Type object pointer只在CLR层面存在，对C#程序来说是透明的。但是CLR将它们暴封装后露C#，例如线程同步原语和反射API。","text":"值类型与引用类型C#中的类型分为引用类型和值类型。使用struct或enum关键字修饰的类型定义是值类型，使用class或delegate关键字修饰的类型是引用类型。引用类型和值类型各有限制，分别适用于不同的场景。不同于C++，C#中的值类型只能分配在栈上*1，引用类型只能分配在GC堆上。C#中的GC是精确式GC，这就对GC堆上的指针有了一些要求。这是引用类型有开销的原因之一。 Sync block index 与 Type object pointer读过C# vir CLR的同学会知道，引用类型的开销是Sync blockindex和Type object pointer。他们的长度是都是一个字长。即在32位CLR上是4字节，在64位CLR上是8字节。Sync block index在CLR中是用于实现lock,Monitor等线程同步原语，Type object pointer是指向当前对象运行时类型信息的一个指针。Sync block index与Type object pointer只在CLR层面存在，对C#程序来说是透明的。但是CLR将它们暴封装后露C#，例如线程同步原语和反射API。 特殊的类型-数组C#中绝大部分类型的大小在编译期就可以确定，只要把对应的成员的大小相加即可1SizeOf(T) = T.GetFields.Select(f=&gt;f.IsClass?WordSize:Sizeof(f)).Sum(); 数组是个很特殊的存在，特殊在它的大小是与元素的类型和数量有关。数组的内存布局包含了所有的元素。(另一个这样的类型是string)。byte[4]和byte[3]的类型都是byte[]，然而它们的大小却不一样。我们这次就好仔细观察下数组 使用WinDBGWinDBG是Windows下常用的Debugger。虽然是以调试非托管代码设计，但是加上相关的插件以后也可以用来调试托管代码。SOS.dll是一个提供托管代码调试支持的的插件。它同时支持CLR和CoreCLR。 安装好WinDBG后，就可以开始调试了。简单起见，这里使用如下代码。1234567891011121314151617namespace SOSFromEE&#123; class Program &#123; const int Length = 10; static void Main(string[] args) &#123; var ints = Enumerable.Range(1, Length).ToArray(); var strs = ints.Select(i =&gt; i.ToString()).ToArray(); Console.ReadLine(); GC.KeepAlive(ints); GC.KeepAlive(strs); Console.Read(); &#125; &#125;&#125; 在WinDBG中选择Launch对应的可执行程序即可。在第一个断点时CLR还没有加载，我们继续让程序运行，等到不再出现ModLoad相关的提示时就可以让程序暂停了。我们在这时加载SOS扩展。 1.loadby sos CLRNameHere sos后跟的是CLR的名称 CLR2.0(.Net framework 3.5及以前)是mscorwks CLR4.0(.Net framework 4.0及以后)是clr .net core是 coreclr 加载模块的过程中需要从微软的服务器上下载相关的pdb文件。由于你懂的原因需要很长时间 CLR2.0 下的数组我们首先在32位的CLR2.0下观察。 首先使用.loadby sos mscorwks加载SOS扩展模块。 !dumpheap -type TypeNameHere命令可以查看当前托管堆上类型名为TypeNameHere的对象。 System.String[]我们先看看String[]都有哪些: 123456789100:006&gt; !dumpheap -type System.String[] Address MT Size03591390 78ae46e4 80 ..........................total 17 objectsStatistics: MT Count TotalSize Class Name78ae46e4 17 676 System.Object[]Total 17 objects Address显示的是对象在托管堆中的地址，Method Table就是上文中说的Type object pointer了。 我们查看下位于地址03591390的String[]:1234567891011121314151617181920212223240:006&gt; !dumparray 03591390Name: System.String[]MethodTable: 78ae46e4EEClass: 788cda74Size: 80(0x50) bytesArray: Rank 1, Number of elements 16, Type CLASSElement Methodtable: 78b10f14[0] 035911c8[1] 03591250[2] null[3] null[4] null[5] null[6] null[7] null[8] null[9] null[10] null[11] null[12] null[13] null[14] null[15] null 可以看到这个数组一共有16个元素，元素的类型是string。 我们再来看看03591390位置的内存布局 12345670:006&gt; dd 03591390 -0x40359138c 00000000 78ae46e4 00000010 78b10f140359139c 035911c8 03591250 00000000 00000000035913ac 00000000 00000000 00000000 00000000035913bc 00000000 00000000 00000000 00000000035913cc 00000000 00000000 00000000 00000000 dd 03591390 -0x4告诉WinDBG从03591390向前偏移4个字节的位置开始展示内存。之所以向前偏移4个字节是为了展示引用类型的开销。0359138c位置的值00000000是Sync block index，位于03591390的78ae46e4看起来有点摸不着头脑，在WinDBG的输出中查找后发现是System.String[]的MethodTable。12Name: System.String[]MethodTable: 78ae46e4 MethodTable是CLR级别的概念，对应到这里就是Type Object Pointer。GC堆上的对象往前偏４字节就能得到Sync block index，接下来是对象的Type object pointer。为了叙述方便，下文统称为对象头:) 由此推测，对象头长度是两个字长。起始位置为当前指针的位置往前偏移一个字长。有兴趣的同学可以在64位CLR上自行验证。 数组的长度是16(0x10)，地址03591395的值就是它。 地址03591388的78b10f14不知道是什么。没关系，CTRF+F查找后发现它在上文出现过。 1Element Methodtable: 78b10f14 应该是String的Method table。 dump一下System.String:12340:006&gt; !dumpheap -type System.String MT Count TotalSize Class Name78b10f14 154 6224 System.String 看来猜得没错。总结一下，引用类型的数组有4字长的开销，分别是2字长的对象头，1字长的长度，1字长的元素类型指针 接下来看看值类型数组的布局 System.Int32[]老样子，首先先找到堆上的Int32[]. 1234567890:006&gt; !dumpheap -type System.Int32[] Address MT Size03591e50 78b130b0 296 ................................total 18 objectsStatistics: MT Count TotalSize Class Name78b130b0 18 1440 System.Int32[]Total 18 objects dump一下相关属性 1234567!dumparray 03591e50 Name: System.Int32[]MethodTable: 78b130b0EEClass: 788ce6a8Size: 296(0x128) bytesArray: Rank 1, Number of elements 71, Type Int32Element Methodtable: 78b13160 查看内存布局 1234567890:006&gt; dd 03591e50 -0x403591e4c 00000000 78b130b0 00000047 0000000003591e5c 00000000 00000000 00000000 0000000003591e6c 00000000 00000000 00000000 0000000003591e7c 00000000 00000000 00000000 0000000003591e8c 00000000 00000000 00000000 0000000003591e9c 00000000 00000000 00000000 0000000003591eac 00000000 00000000 00000000 0000000003591ebc 00000000 00000000 00000000 00000000 首先Int32[]也有对象头和长度的开销，但是却没有元素类型指针的开销。 有心的同学可能已经发现了，Int32[]明确指出了元素的类型，而String[]却没有。 123456!dumparray 03591390Name: System.String[]MethodTable: 78ae46e4EEClass: 788cda74Size: 80(0x50) bytesArray: Rank 1, Number of elements 16, Type *CLASS* 123456!dumparray 03591e50 Name: System.Int32[]MethodTable: 78b130b0EEClass: 788ce6a8Size: 296(0x128) bytesArray: Rank 1, Number of elements 71, Type *Int32* String[]的Type是个CLASS而不是String，难道说System.String[]是个’假的’的字符串数组？ …. …. …. 恭喜你猜对了。使用!objsize命令发现String[]是个带了层皮的Object[]1234!objsize 03591e50 sizeof(03591e50) = 296 ( 0x128) bytes (System.Int32[])0:006&gt; !objsize 03591390 sizeof(03591390) = 392 ( 0x188) bytes (System.Object[]) C# in depth中对泛型有这样的描述 对于一个泛型类MyGeneric&lt;T&gt;，对于T的是引用类型的情况，JIT只会为其生成一份代码;对于T是值类型的情况，则为每一个不同的T生成各自的代码。其中的原因是，在JIT运行时，指针的长度总是固定的，因而可以共用一套代码相同的代码。而值类型的长度是不确定的，因此需要为每个值类型单独生成代码。 这里可能也是相同的原因吧。指针的长度相同，因而才需要储存元素的类型指针，实现类型检查。而值类型的代码不共用，所以不需要储存元素的类型指针。 本来到已经可以结束了，可是我在32位的CLR4.0观察到的结果却不太一样。 CLR 4.0下的数组与CLR 2.0不同，CLR 4.0下加载SOS的名字是clr .loadby sos clr System.String[]首先dumpheap： 123456780:006&gt; !dumpheap -type System.String[] Address MT Size02531590 6979dfe0 84 ...........................Statistics: MT Count TotalSize Class Name6979dfe0 24 912 System.String[]Total 24 objects 然后是dumparray:1234567Name: System.String[]MethodTable: 6979dfe0EEClass: 69374b80Size: 84(0x54) bytesArray: Rank 1, Number of elements 18, Type CLASSElement Methodtable: 6979d488[0] 02531254 最后dd:12345670:006&gt; dd 02531590 -0x40253158c 00000000 6979dfe0 00000012 025312540253159c 025312d8 00000000 00000000 02531568025315ac 00000000 00000000 00000000 00000000025315bc 00000000 00000000 00000000 00000000025315cc 00000000 00000000 00000000 00000000025315dc 00000000 00000000 位于02531598的值02531254是第一个元素的值而不是String的Method Table!12345!dumpheap -type StringStatistics: MT Count TotalSize Class Name6979d488 193 5932 System.StringTotal 224 objects CLR 4.0把Method Table去掉了？ 经过简单的算术，确实是这样的18个元素，占据空间18*4=72。 对象头和数组大小占据2*4+4=12 84=72+12，跟dumparray出来的值一样。(有兴趣的同学根据上文中CLR2.0的数据计算下) !objsize也确认了我们的猜测120:006&gt; !objsize 02531590sizeof(02531590) = 428 (0x1ac) bytes (System.String[]) System.Int32[]!dumpheap:123456780:006&gt; !dumpheap -type System.Int32[] Address MT Size02531f1c 6979f2a0 300 .............................. Statistics: MT Count TotalSize Class Name6979f2a0 20 844 System.Int32[]Total 20 objects !dumparray:1234567890:006&gt; !dumparray 02531f1c Name: System.Int32[]MethodTable: 6979f2a0EEClass: 693752d8Size: 300(0x12c) bytesArray: Rank 1, Number of elements 72, Type Int32Element Methodtable: 6979f2dc[0] 02531f24[1] 02531f28 dd:1234567890:006&gt; dd 02531f1c -0x402531f18 00000000 6979f2a0 00000048 0000000302531f28 00000007 0000000b 00000011 0000001702531f38 0000001d 00000025 0000002f 0000003b02531f48 00000047 00000059 0000006b 0000008302531f58 000000a3 000000c5 000000ef 0000012502531f68 00000161 000001af 00000209 0000027702531f78 000002f9 00000397 0000044f 0000052f02531f88 0000063d 0000078b 0000091d 00000af1 值类型数组倒是没有什么变化。 总结C#中引用类型有2个字长的对象头开销。分别是Sync block index和Type Object Pointer。数组是特殊的类型，它的大小与包含的元素相关，因此具有额外的开销。 在CLR 2.0下 引用类型的数组包含额外的2字长的开销。分别是长度和元素的类型指针 值类型的数组包含长度的额外开销开销。大小是一个字长。 在CLR 4.0和CoreCLR中 引用类型和值类型的数组包含长度的额外开销开销。大小是一个字长。 限于篇幅，CoreCLR的情况不再赘述，还请读者自行验证。 /*其实本来看到Stackoverflow的回答只是想自己验证下的，但是自己动手的结果和答案里提到的不太一样，查了原因发现答案里用的是CLR2.0，我自己用的是CLR4.0。这就挖出来了CLR实现的更改。CoreCLR里使用的是CLR4.0里的规则。目前还不清楚MS为何要改实现*/ 参考 Overhead of a .NET array? SOS.dll (SOS Debugging Extension)","categories":[],"tags":[{"name":"C#","slug":"C","permalink":"https://verrickt.github.io/tags/C/"},{"name":"reference type","slug":"reference-type","permalink":"https://verrickt.github.io/tags/reference-type/"},{"name":"overhead","slug":"overhead","permalink":"https://verrickt.github.io/tags/overhead/"}]},{"title":"面向抽象编程","slug":"program-upon-abstractions","date":"2018-07-12T13:56:26.000Z","updated":"2018-08-04T08:30:08.370Z","comments":true,"path":"2018/07/12/program-upon-abstractions/","link":"","permalink":"https://verrickt.github.io/2018/07/12/program-upon-abstractions/","excerpt":"Preface说来惭愧，直到近几天才明白了一点面向对象设计的。给我带来启发的是SOLID中的D，它代表Dependency Inversion（依赖反转）。尽管写/背定义很无聊，但我还是想写一下依赖反转的核心 上层模块不应该依赖下层模块，他们都应该依赖抽象 Talk is cheap, show me the code最近在写一个音乐电台应用，采用服务端、客户端的方式实现。在服务端，用户可以指定一个路径，程序根据这个路径生成播放列表。 需求服务器是一个一旦开起来就不会轻易关闭的程序，我希望播放列表能够自动刷新。这样当用户添加或删除了某首音乐后不用重启服务器就可以反映变化。考虑到易用性，应该支持由路径直接生成播放列表。歌曲是有封面等其他信息的，要满足这些信息的可定制化，程序也支持由配置文件指定的播放列表。","text":"Preface说来惭愧，直到近几天才明白了一点面向对象设计的。给我带来启发的是SOLID中的D，它代表Dependency Inversion（依赖反转）。尽管写/背定义很无聊，但我还是想写一下依赖反转的核心 上层模块不应该依赖下层模块，他们都应该依赖抽象 Talk is cheap, show me the code最近在写一个音乐电台应用，采用服务端、客户端的方式实现。在服务端，用户可以指定一个路径，程序根据这个路径生成播放列表。 需求服务器是一个一旦开起来就不会轻易关闭的程序，我希望播放列表能够自动刷新。这样当用户添加或删除了某首音乐后不用重启服务器就可以反映变化。考虑到易用性，应该支持由路径直接生成播放列表。歌曲是有封面等其他信息的，要满足这些信息的可定制化，程序也支持由配置文件指定的播放列表。 现有代码123456789internal class PlaylistManager&#123; public IReadonlyList&lt;Song&gt; AllSong =&gt; _backLogs.AsReadOnly(); private readonly List&lt;Song&gt; _backLogs; public PlaylistManager(IEnumerable&lt;Song&gt; backlog) &#123; _backLogs = new List&lt;Song&gt;(backlog); &#125;&#125; 现有代码中构造函数的IEnumerable\\参数指定了播放列表，可是刷新却需要外部的支持：基于路径的刷新和基于配置文件的刷新是完全不一样的。我完全可以把这两个刷新都放到PlaylistManager里，根据生成播放列表的类型决定调用那个版本。这样就把PlaylistManager完全和列表生成的逻辑绑死在一起了，如果以后要在加一个新的生成方式就还要修改PlaylistManager的代码，尽管它跟PlaylistManager并无关系 修改思路如果我们将生成播放列表这一行为抽象为接口IPlaylistProvider的话，PlaylistManager就可以完全跟这部分逻辑分开了 123456789101112131415161718192021222324public interface IPlaylistProvider&#123; IReadonlyList&lt;Song&gt; AllSongs &#123; get; &#125; void Refresh();&#125;internal class PlaylistManager&#123; public IReadonlyList&lt;Song&gt; AllSong =&gt; _backLogs.AsReadOnly(); private readonly List&lt;Song&gt; _backLogs; private readonly IPlaylistProvider _provider; public PlaylistManager(IPlaylistProvider provider) &#123; _provider = provider; _backLogs = new List&lt;Song&gt;(_provider.AllSongs); &#125; internal void Refresh() &#123; Refresh(); _backLogs.Clear(); _backLogs.AddRange(_provider.AllSongs); &#125;&#125; 我们可以分别实现基于路径和配置文件的IPlaylistProvider 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768public class FileSystemProvider : IPlaylistProvider&#123; public FileSystemProvider(string path,bool recursive) &#123; if (!Directory.Exists(path)) &#123; throw new ArgumentException($\"&#123;path&#125; is not a directory\"); &#125; Path = path; Recursive = recursive; _songs = ReadSongs(); &#125; static readonly string[] MusicExtension = new[] &#123;\".mp3\",\".wma\" &#125;; private List&lt;Song&gt; ReadSongs() &#123; var song = new List&lt;Song&gt;(); var dir = new DirectoryInfo(Path); var files = dir.GetFiles().Where(i =&gt; MusicExtension.Contains(i.Extension)); song.AddRange(files.Select(f =&gt; new Song() &#123; FilePath = f.FullName, Title = f.Name, &#125;)); return song; &#125; public IReadOnlyList&lt;Song&gt; AllSongs =&gt; _songs.AsReadOnly(); public string Path &#123; get; &#125; public bool Recursive &#123; get; &#125; private List&lt;Song&gt; _songs; public void Refresh() &#123; _songs = ReadSongs(); &#125;&#125;public class JsonPlaylistProvider : IPlaylistProvider&#123; public JsonPlaylistProvider(string configPath) &#123; _songs = ReadSongs(); ConfigPath = configPath; if (!File.Exists(ConfigPath)) &#123; throw new ArgumentException($\"Can't find config file at &#123;ConfigPath&#125;\"); &#125; ReadSongs(); &#125; private List&lt;Song&gt; ReadSongs() &#123; var json = File.ReadAllText(ConfigPath); return JsonConvert.DeserializeObject&lt;List&lt;Song&gt;&gt;(json)??new List&lt;Song&gt;(); &#125; private List&lt;Song&gt; _songs; public string ConfigPath &#123; get; &#125; public IReadOnlyList&lt;Song&gt; AllSongs =&gt; _songs.AsReadOnly(); public void Refresh() &#123; ReadSongs(); &#125;&#125; 这个例子中，PlaylistManager不依赖FileSystemProvider和JsonPlaylistProvider，它们三者都依赖于IPlaylistProvider这一接口。代码的可读性和可维护性相比于把刷新的逻辑放在PlaylistManager里高了好多 这里体现了依赖反转的原则： 上层模块不应该依赖下层模块，它们都应该依赖与抽象 我在用SOLID重构手头的项目，改的差不多的时候打算写写开发笔记。咕咕咕","categories":[],"tags":[{"name":"OOP","slug":"OOP","permalink":"https://verrickt.github.io/tags/OOP/"},{"name":"SOLID","slug":"SOLID","permalink":"https://verrickt.github.io/tags/SOLID/"}]},{"title":"记一次愉(dan)快(teng)的捉虫","slug":"hell-mode-debugging","date":"2018-05-21T13:03:59.000Z","updated":"2018-08-04T10:24:55.494Z","comments":true,"path":"2018/05/21/hell-mode-debugging/","link":"","permalink":"https://verrickt.github.io/2018/05/21/hell-mode-debugging/","excerpt":"废话BUG是任何软件都会遇到的问题。它通常是开发人员考虑问题不全面而埋下的隐形炸弹，在条件合适的时候就会爆炸；开发自己埋BUG往往比较容易除错，因为代码都是自己写的，定位问题后git blame一下就可以甩锅了；而程序依赖项里的BUG则排查起来则让人一筹莫展，尤其是在没有足够多信息的情况。 面对来自客户和QA的压力，绞尽脑汁仍无法定位问题开发们只能找借口了🤣 It’s a feature,not a bug. That’s the design decision, so not a bug 我以前也遇到框架出问题的情况，不过都是自己改代码改出来的，倒也不是很难排查。但这次挖出来的BUG就完全不一样了","text":"废话BUG是任何软件都会遇到的问题。它通常是开发人员考虑问题不全面而埋下的隐形炸弹，在条件合适的时候就会爆炸；开发自己埋BUG往往比较容易除错，因为代码都是自己写的，定位问题后git blame一下就可以甩锅了；而程序依赖项里的BUG则排查起来则让人一筹莫展，尤其是在没有足够多信息的情况。 面对来自客户和QA的压力，绞尽脑汁仍无法定位问题开发们只能找借口了🤣 It’s a feature,not a bug. That’s the design decision, so not a bug 我以前也遇到框架出问题的情况，不过都是自己改代码改出来的，倒也不是很难排查。但这次挖出来的BUG就完全不一样了 背景一个WPF的项目，勉强在deadline之前赶完了功能。出了新版本后接到QA报告，程序在Windows 8.1上死于OOM: System.OutOfMemoryException: Insufficient memory to continue the execution of the program. at System.Windows.Media.MediaContext.NotifyPartitionIsZombie(Int32 failureCode) at System.Windows.Media.MediaContext.NotifyChannelMessage() at System.Windows.Interop.HwndTarget.HandleMessage(Int32 msg, IntPtr wparam, IntPtr lparam) at System.Windows.Interop.HwndSource.HwndTargetFilterMessage(IntPtr hwnd, Int32 msg, IntPtr wParam, IntPtr lParam, Boolean&amp; handled) at MS.Win32.HwndWrapper.WndProc(IntPtr hwnd, Int32 msg, IntPtr wParam, IntPtr lParam, Boolean&amp; handled) at MS.Win32.HwndSubclass.DispatcherCallbackOperation(Object o) at System.Windows.Threading.ExceptionWrapper.InternalRealCall(Delegate callback, Object args, Boolean isSingleParameter) at System.Windows.Threading.ExceptionWrapper.TryCatchWhen(Object source, Delegate callback, Object args, Boolean isSingleParameter, Delegate catchHandler) at System.Windows.Threading.Dispatcher.WrappedInvoke(Delegate callback, Object args, Boolean isSingleParameter, Delegate catchHandler) at System.Windows.Threading.Dispatcher.InvokeImpl(DispatcherPriority priority, TimeSpan timeout, Delegate method, Object args, Boolean isSingleParameter) at System.Windows.Threading.Dispatcher.Invoke(DispatcherPriority priority, Delegate method, Object arg) at MS.Win32.HwndSubclass.SubclassWndProc(IntPtr hwnd, Int32 msg, IntPtr wParam, IntPtr lParam) at MS.Win32.UnsafeNativeMethods.DispatchMessage(MSG&amp; msg) at System.Windows.Threading.Dispatcher.PushFrameImpl(DispatcherFrame frame) at System.Windows.Threading.Dispatcher.PushFrame(DispatcherFrame frame) at System.Windows.Threading.Dispatcher.Run() 捉虫出师不利程序里用自定义的Window Style替代了WPF自带的。但是自定义的Window Style在最大化的时候会覆盖任务栏。为了解决这个问题，我们使用了回答里自定义的消息循环。看到调用栈里的Hwnd，初步怀疑是自定义的消息循环的问题。把消息循环相关的代码拿掉，问题依旧。看来不是消息循环的问题。不过为什么我在开发的时候没有遇到问题呢？ 寻找原因把出问题版本直接放在我的开发机上跑发现并没有问题。放在另一个同事的机子上也没问题。这时候意识到问题有点蹊跷。一般来说这种不能稳定重现的问题都或多或少会和多线程和死锁有关。然而上个版本到这个版本并没有写这些方面的代码。这时候开始怀疑是环境的问题了。找来各个版本的Windows，依次尝试后发现只在Windows 8.1上有问题。难道是OS的问题？同事提醒了一句，说以前的版本是可以在Windows 8.1上正常跑的。用二分法，找到了最晚的能用的版本B和最早的不能用的版本A。 又入困境按理说，找到版本后diff下改动的代码就能比较快的定位问题了。不过现在有了稳定浮现的版本B，我决定从异常本身入手。一番搜索后找到了微软的一篇博客，其中提到这个错误通常是渲染子系统出问题的症状： … WPF render thread encountered some fatal error … Most of the time, a failure occurs when calling into DirectX/D3D … When a failure is detected, The render thread will attempt to map the failure it receives to an appropriate managed exception. The render thread only synchronizes with the UI thread in a few locations …The most common locations when they synchronize are … or as a result of the UI thread handling a “channel” message from DirectX. If a render thread failure manifests as a System.OutOfMemoryException, then the likelihood is that the render thread was a victim of the process exhausting some resource .The exhausted resource is most often available/contiguous virtual address space … It might be a situation where sometimes the WPF render thread is the victim of the resource exhaustion … WPF里的渲染是由非托管的DirectX做的。为了从程序员的角度隐藏掉渲染这个过程，WPF将自身拆分为不同的组件。PreserentationFramework.dll和PreserentationFrameworkCore.dll是程序员直接能接触到的。这里实现的是组合子系统。渲染子系统则在非托管的milcore.dll里实现。System.Windows.Media.Visual是WPF组合子系统的入口点，它通过私有的通信协议与milcore里的渲染子系统通信，从而将组合子系统和渲染子系统连接起来。 看起来我们的代码耗尽了DirectX里边的某种资源，从而让渲染失败了。C#做为一个托管语言，居然能捅这么大的篓子，完全不科学。这个BUG越来越有意思了🤔 既然异常上看不出来什么头绪那就要从代码入手了。找diff的时候发现B版本没有打tag。 没打tag不算大问题，多花了些时间成功找到了那个commit，但是心情有点不爽。 diff的代码不多。排除掉无关的文件就只剩下一些布局文件比较可疑。改流程，依次跳过这些页面，发现一跳到登录页面就挂了。登录页面的diff有点多，二分删除很快就定位到了问题的根源:ImageButton。 ImageButton是个在普通、鼠标悬停、被按下三种不同状态时显示不同的图片的按钮。 ImageButton.cs: 123456789101112131415161718192021222324252627282930313233343536using System.Windows;using System.Windows.Controls;using System.Windows.Media;namespace Debugging&#123; public class ImageButton : Button &#123; public ImageSource Source &#123; get &#123; return (ImageSource)GetValue(SourceProperty); &#125; set &#123; SetValue(SourceProperty, value); &#125; &#125; public static readonly DependencyProperty SourceProperty = DependencyProperty.Register(\"Source\", typeof(ImageSource), typeof(ImageButton), new PropertyMetadata(null)); public ImageSource MouseOverImage &#123; get &#123; return (ImageSource)GetValue(MouseOverImageProperty); &#125; set &#123; SetValue(MouseOverImageProperty, value); &#125; &#125; public static readonly DependencyProperty MouseOverImageProperty = DependencyProperty.Register(\"MouseOverImage\", typeof(ImageSource), typeof(ImageButton), new PropertyMetadata(null)); public ImageSource PressedImage &#123; get &#123; return (ImageSource)GetValue(PressedImageProperty); &#125; set &#123; SetValue(PressedImageProperty, value); &#125; &#125; public static readonly DependencyProperty PressedImageProperty = DependencyProperty.Register(\"PressedImage\", typeof(ImageSource), typeof(ImageButton), new PropertyMetadata(null)); &#125;&#125; ImageButton.xaml: 1234567891011121314151617181920212223&lt;ResourceDictionary xmlns=\"http://schemas.microsoft.com/winfx/2006/xaml/presentation\" xmlns:x=\"http://schemas.microsoft.com/winfx/2006/xaml\" xmlns:local=\"clr-namespace:Debugging\"&gt; &lt;Style TargetType=\"local:ImageButton\"&gt; &lt;Setter Property=\"Template\"&gt; &lt;Setter.Value&gt; &lt;ControlTemplate TargetType=\"local:ImageButton\"&gt; &lt;Grid&gt; &lt;Image x:Name=\"image\" Source=\"&#123;TemplateBinding Source&#125;\" /&gt; &lt;/Grid&gt; &lt;ControlTemplate.Triggers&gt; &lt;Trigger Property=\"IsMouseOver\" Value=\"True\"&gt; &lt;Setter TargetName=\"image\" Property=\"Source\" Value=\"&#123;Binding RelativeSource=&#123;RelativeSource TemplatedParent&#125;, Path=MouseOverImage&#125;\" /&gt; &lt;/Trigger&gt; &lt;Trigger Property=\"IsPressed\" Value=\"True\"&gt; &lt;Setter TargetName=\"image\" Property=\"Source\" Value=\"&#123;Binding RelativeSource=&#123;RelativeSource TemplatedParent&#125;, Path=PressedImage&#125;\" /&gt; &lt;/Trigger&gt; &lt;/ControlTemplate.Triggers&gt; &lt;/ControlTemplate&gt; &lt;/Setter.Value&gt; &lt;/Setter&gt; &lt;/Style&gt;&lt;/ResourceDictionary&gt; 登陆页面中是这样使用ImageButton的:123....&lt;ImageButton Source=\"&#123;StaticResource SomeImage&#125;\" MouseOverImage=\"&#123;StaticResouce SomeOtherImage&#125;\" PressedImage=\"&#123;StaticResources AnotherImage&#125;\" /&gt;.... 把Source=&quot;{StaticResource SomeImage}&quot;这一句去掉就正常了。似乎它就是罪魁祸首。但是，把这部分代码和图片拿出来放到另一个程序里却又一切正常。 转机接下来又尝试了 把ImageButton替换为Image 把Template.Trigger去掉 把SomeImage指向其他图片 但是还是找不到root cause。一模一样的代码不能在其他程序复现，那就说明BUG是我们程序自己造成的。但是这个BUG只在Windows 8.1下出现，说明跟环境也有可能有关系。距离发现BUG已经过去一天半了，放弃的话实在是不甘心，接下来只能猜了。 堆栈里显示程序挂在WPF的程序集里，基于这个现象决定retarget到.NET Framework的最新版本4.7.2。编译打包部署一套流程走完，在运行的时候发现Windows 8.1没有安装.NET Framework 4.7.2。好吧，那就装。可是安装包告诉我不满足条件： 需要安装KB2919355？ 微软来背锅在微软的支持网站上详细列出了KB2919355的change log。以image为关键字CTRL+F发现这样的描述： Article number Article Title 2929755 Out of memory when you load some image resources in a Windows application 我去，这就是我们遇到的问题。 对应的页面给出的描述： Symptoms When you load some image resources in an application such as Microsoft Visual Studio on a computer that has Windows 8.1, Windows Server 2012 R2, Windows 8, Windows Server 2012, Windows 7, or Windows Server 2008 R2 installed, the application stops responding and memory leaks in Windows. This issue occurs after you install the following update:2670838 Platform update for Windows 7 SP1 and Windows Server 2008 R2 SP1 File Information For all supported x86-based versions of Windows 8: File name File version File size Date Time Platform Windowscodecs.dll 6.2.9200.16809 1,339,392 31-Jan-2014 00:48 x86 Windowscodecs.dll 6.2.9200.20930 1,319,936 31-Jan-2014 06:04 x86 这个BUG是KB2670838引入的。在2670838的更新中,微软更新了现有的Windows Imaging Component (WIC) This update improves the range and performance of the following graphics and imaging components: Windows Imaging Component (WIC) 我们在diff里重点关注了Image相更改，发现改了这么一行： 123456&lt;Application.Resources&gt; &lt;Style TargetType=\"Image\"&gt; &lt;Setter Property=\"RenderOptions.BitmapScalingMode\" Value=\"Fant\" /&gt; &lt;/Style&gt;&lt;/Application.Resources&gt; RenderOptons.BitmapScalingMode指明在需要缩放时使用的算法。Fant是质量最高的一种之一。尝试Fant改为Linear后程序正常运行。继续尝试其他的值，发现除了Linear和Unspecified都会崩溃。 原因分析在KB2670838里的WIC在特定情况会OOM爆掉。WPF里的BitmapScalingMode.Fant依赖WIC实现，在装了KB2670838的环境里跟就会跟WIC一起OOM爆掉；微软在后来发行的KB2919355中修复了这个BUG。 我们的测试机的Windows系统由RTM的镜像安装并且关闭了自动更新。在Windows 8.1之前的RTM镜像没有自带KBB2670838，在Windows 8.1之后RTM镜像自带了KB2919355。偏偏这个Windows 8.1的RTM镜像内置了KB2670838而没有内置KB2919355，程序就这样死于OOM。 验证Windowscodecs.dll的版本也支持这个结论。 总结 定位环境问题的时候要控制变量 二分法能快速定位出错的位置 打好Tag很重要 一直以来依赖的底层也会有BUG。大胆质疑，小心求证。 该甩锅的时候要果断的甩给微软 ReferenceWPF Architecture - docs.microsoft.com WPF ToolTip Rendering Issue : Application Hang - social.msdn.microsoft.com Windows RT 8.1, Windows 8.1, and Windows Server 2012 R2 update: April 2014 - support.microsoft.com Platform update for Windows 7 SP1 and Windows Server 2008 R2 SP1 - support.microsoft.com Windows Imaging Component - msdn.microsoft.com","categories":[],"tags":[{"name":"C#","slug":"C","permalink":"https://verrickt.github.io/tags/C/"},{"name":"Debugging","slug":"Debugging","permalink":"https://verrickt.github.io/tags/Debugging/"},{"name":"WPF","slug":"WPF","permalink":"https://verrickt.github.io/tags/WPF/"}]},{"title":"闭包，变量捕获与重入问题","slug":"closure-and-re-entrance","date":"2018-03-08T11:59:20.000Z","updated":"2018-08-04T08:26:21.220Z","comments":true,"path":"2018/03/08/closure-and-re-entrance/","link":"","permalink":"https://verrickt.github.io/2018/03/08/closure-and-re-entrance/","excerpt":"问题描述公司的APP，在断网后进行一个10秒的倒计时操作，每秒钟都会尝试重新联网。当秒数到0时又重新开始计时，倒计时在用户退出程序或者连上网络结束。 按理说是个很简单的Case。QA却报过来个BUG，说APP状态由 联网-&gt;断网-&gt;联网-&gt;断网 变化后，倒计时的秒数变为 9 3 8 2 7 1 变得不连续了。","text":"问题描述公司的APP，在断网后进行一个10秒的倒计时操作，每秒钟都会尝试重新联网。当秒数到0时又重新开始计时，倒计时在用户退出程序或者连上网络结束。 按理说是个很简单的Case。QA却报过来个BUG，说APP状态由 联网-&gt;断网-&gt;联网-&gt;断网 变化后，倒计时的秒数变为 9 3 8 2 7 1 变得不连续了。 思考了一下，觉得应该是第一次倒计时的没有退出，第二次倒计时开始后两者都开始更新UI。 看了代码，果然是这样的 123456789101112131415161718192021222324void OnIPChanged()&#123; ShowOfflineUI();&#125;void ShowOfflineUI()&#123; Task.Factory.Start(delegate &#123; int i = 10; while(true) &#123; i = (i-1)%10; Invoke(UpdateCountDown(i))//update UI on UI thread try &#123; Thread.Sleep(TimeSpan.FromSeconds(1)) if(Connect()) &#123; break; &#125; &#125; &#125; &#125;)&#125; 在第二次触发倒计时的时候原来的倒计时还在继续，即，两个倒计时同时执行是不安全的。这是典型的重入问题) 说实话我不是很喜欢直接用Thread.Sleep来做操作，这样占用了一个线程忙等，浪费了资源。C#中一般采用基于Task的异步编程来做，这样不会浪费资源。Task提供的CancellationToken能够比较容易的实现取消。放到这个Case里，只要开始倒计时的时候取消上一次的倒计时就好了。 说干就干,我最喜欢用Task重写基于Thread的并发/异步了 12345678910111213141516171819202122232425262728TaskCancellationToken _cts;void OnIPChanged()&#123; ShowOfflineUI(); ShowOfflineUI();&#125;async Task ShowOfflineUI()&#123; _cts?.Cancel(); _cts = new CancellationToken(); await Task.Run( async ()=&gt; &#123; int i = 0; while(true) &#123; i = (i-1)%10; Invoke(UpdateCountDown(i)); await Task.Delay(TimeSpan.FromSeconds(1),_cts.Token); if(Connect()) &#123; break; &#125; &#125; &#125; );&#125; 为了验证方法的正确性，我特地在OnIPChanged()调用了两次ShowOfflineUI()。但是第一个Task并没有被取消。 分析问题写出了跟自己预期不一样的代码怎么办？当然是上Debugger了。在Debugger的火眼金睛下，我很快注意到了最明显的现象：在第二次调用ShowOfflineUI的时候，_cts.Cancel()并没有把第一个Task中的CancellationToken的IsCancellationRequest变为True。计算机科学里有句老话，那就是永远不要怀疑近30年内编译器的正确性。经过数十分钟的排(谷)查(歌)定位了问题的原因:两个Task引用的_cts是同一个。 原因剖析如果一个匿名函数引用了不属于他自己的局部变量，那么这个现象就称为闭包。因为这实在是太自然了所以我才没往这上面想。在给Task.Run中，我传入了一个Action类型的函数，它捕获了外部变量_cts。在函数中每次用到_cts的时候，被捕获的变量的值被重新计算，结果作为实际的值。而我的本意是让两个Task拥有不同的_cts，这样后边的Task就可以取消前边的Task了。明白了这些后就很好改了，在ShowOfflineUI里放一个局部变量，存储_cts的值，让匿名函数捕获这个局部变量就好了1234567891011121314151617181920212223async Task ShowOfflineUI()&#123; var local = _cts; local?.Cancel(); _cts = new CancellationToken(); await Task.Run( async ()=&gt; &#123; int i = 0; while(true) &#123; i = (i-1)%10; Invoke(UpdateCountDown(i)); await Task.Delay(TimeSpan.FromSeconds(1),local?.Token??CancellationToken.None); if(Connect()) &#123; break; &#125; &#125; &#125; );&#125; 有兴趣的朋友可以猜一猜这两段代码的结果，再运行验证一下 123456789101112131415161718public void NonLocal()&#123; List&lt;Action&gt; actions = new List&lt;Action&gt;(); for(int i=0 ;i&lt;10;i++) actions.Add(()=&gt;Console.WriteLine(i)); actions.ForEach(a=&gt;a());&#125;public void Local()&#123; List&lt;Action&gt; actions = new List&lt;Action&gt;(); for(int i=0 ;i&lt;10;i++) &#123; int j = i; actions.Add(()=&gt;Console.WriteLine (j)); &#125; actions.ForEach(a=&gt;a());&#125; 相信名字就给大家足够多的提示了 最后祝你身体健康，再见","categories":[],"tags":[{"name":"Reentrance","slug":"Reentrance","permalink":"https://verrickt.github.io/tags/Reentrance/"},{"name":"async/await","slug":"async-await","permalink":"https://verrickt.github.io/tags/async-await/"},{"name":"lambda","slug":"lambda","permalink":"https://verrickt.github.io/tags/lambda/"}]},{"title":"HTTPClient 踩坑记","slug":"httpclient-good-part-and-tipfalls","date":"2017-10-24T11:43:28.000Z","updated":"2018-08-02T15:04:19.455Z","comments":true,"path":"2017/10/24/httpclient-good-part-and-tipfalls/","link":"","permalink":"https://verrickt.github.io/2017/10/24/httpclient-good-part-and-tipfalls/","excerpt":"HttpClient是随着.Net framework 4.5一起发布的现代Http库。比起WebClient，HttpClient最大的优点就是加入了C#5中的async/await异步方法的支持。async/await的坑暂且不表，今天就来说一说这个HttpClient HttpClient的坑HttpClient实现了IDisposable接口，很多小伙伴一看到IDisposeable接口就纷纷把HttpClient套在了using里边 12345//bad httpclient usageusing(var client = new HttpClient())&#123; //do stuffs&#125; 这种用法是错误的.HttpClient在设计之初被设计为一个可重用的对象，它的生命周期应该与应用程序相一致.上述错误的用法每发起一个请求就会创建一个新的HttpClient，并且在收到回复之后立即把HttpClient dispose掉。众所周知TCP连接在真正断开之前会有几分钟处于CLOSE_WAIT状态。这个状态下TCP链接并没有真正断开。短时间内大量发出Http请求会使系统可用的端口急剧消耗。","text":"HttpClient是随着.Net framework 4.5一起发布的现代Http库。比起WebClient，HttpClient最大的优点就是加入了C#5中的async/await异步方法的支持。async/await的坑暂且不表，今天就来说一说这个HttpClient HttpClient的坑HttpClient实现了IDisposable接口，很多小伙伴一看到IDisposeable接口就纷纷把HttpClient套在了using里边 12345//bad httpclient usageusing(var client = new HttpClient())&#123; //do stuffs&#125; 这种用法是错误的.HttpClient在设计之初被设计为一个可重用的对象，它的生命周期应该与应用程序相一致.上述错误的用法每发起一个请求就会创建一个新的HttpClient，并且在收到回复之后立即把HttpClient dispose掉。众所周知TCP连接在真正断开之前会有几分钟处于CLOSE_WAIT状态。这个状态下TCP链接并没有真正断开。短时间内大量发出Http请求会使系统可用的端口急剧消耗。 MS的人推荐重用HttpClient以使其生命周期与应用相同 1234567891011//good httpclient usageclass GoodHttpClientSample&#123; private static readonly HttpClient client = new HttpClient(); public Task&lt;string&gt; GetStringAsync(string url) &#123; var resposne = await client.GetAsync(url).ConfigureAwait(false); return response.Content.ReadAsStringAsync(); &#125; &#125; HttpClient的优点踩过了坑我们再来说说他的好处。去掉async/await支持这个最大的有点，HttpClient的一个构造函数的重载接受一个HttpMessageHandler。这个重载很有意思。HttpMessageHandler可以在发出Http请求和接受Http回复时做出一些回应。.net framework里有一个类叫做DelegatingHandler，它继承了HttpMessageHandler。叫做DelegatingHandler是因为它有个类型为HttpMessageHandler的InnerHandler属性，因而可以把请求delegate给InnerHandler。通过这个DelegatingHandler我们可以请以实现像Java web里的filter chain一样的逻辑。 今天重构了公司的代码。公司现有的HttpManager提供了GET和POST两种Http动词的异步方法。在这些方法中还进行了日志记录和失败重试。日志记录和失败重试相关的代码非常重复，但是又无法写成一个函数。因此我把这部分逻辑抽出来做成了两个DelegatingHandler 12345678910111213141516171819202122232425class LogHandler:DelegatingHandler&#123; private readonly ILog _log; public LogHandler(ILog log,HttpMessageHandler handler):base(handler) &#123; _log = log; &#125; protected async override Task&lt;HttpResponseMessage&gt; SendAsync(HttpRequestMessage request, CancellationToken cancellationToken) &#123; try &#123; var begin = DateTime.Now; _log.I($\"&#123;request.Method&#125; -&gt;&#123;request.RequestUri&#125;\"); var response = await base.SendAsync(request,cancellationToken); var end = DateTime.Now; var diff = (end-begin).TotalMillseconds; _log.I($\"&#123;request.Method&#125; &lt;- &#123;response.Content.ReadAsStringAsync()&#125; cost&#123;diff&#125;ms\"); &#125; catch(Exception e) &#123; _log.E($\"&#123;request.Method&#125; -&gt;&#123;request.RequestUri&#125;,&#123;e&#125;\"); throw; &#125; &#125; 1234567891011121314151617181920212223242526272829303132333435class RetryHandler:DelegatingHandler&#123; protected async override Task&lt;HttpResponseMessage&gt; SendAsync(HttpRequestMessage request,CancellationToken cancellationToken) &#123; HttpResponseMessage response = null; for(int i=0;i&lt;=_retryTimes,i++) &#123; try &#123; response = await base.SendAsync(request,cancellationToken); return response; &#125; catch(Exception) &#123; if(i==_retryTimes) &#123; throw; &#125; &#125; //make compiler happy. return response; &#125; &#125; private readonly int _retryTimes = 0; public RetryHandler(int retryTimes,HttpMessageHandler handler):base(handler) &#123; if(retryTimes&lt;0) &#123; throw new ArgumentOutOfRangeException(nameof(retryTimes)); &#125; _retryTimes = retryTimes; &#125;&#125; HttpClientHandler是个真正实现HttpClient逻辑的HttpMessageHandler。因此，我们只要保证最内部的Handler时HttpClientHandler就OK了。 1234private static readonly ILog _log;private static readonly HttpClient _client = new HttpClient(new RetryHandler(3,new LogHandler(_log,new HttpClientHandler()))); 需要注意的是，如果HttpHandler没有返回HttpResponseMessage,对应的异步方法会在运行时抛出InvalidOperationException 重构之后整个Http封装类的代码行数从400行减少到了120行左右，可读性和可维护性提升显著。 Further readingFUN WITH THE HTTPCLIENT PIPELINE YOU’RE USING HTTPCLIENT WRONG AND IT IS DESTABILIZING YOUR SOFTWARE Do HttpClient and HttpClientHandler have to be disposed?","categories":[],"tags":[{"name":"C#","slug":"C","permalink":"https://verrickt.github.io/tags/C/"},{"name":"HttpClient","slug":"HttpClient","permalink":"https://verrickt.github.io/tags/HttpClient/"}]}]}