<!DOCTYPE html>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  
  <title>机器学习基础——概率论与信息论 | Melchior on CLR</title>
  <meta name="description" content="A programmer focus mainly on Microsoft Technologies" />
  <meta name="keywords" content="Windows desktop,C#,XAML,UWP,WPF,OpenSource,GNU/Linux,CLI" />
  <meta name="HandheldFriendly" content="True" />
  <meta name="apple-mobile-web-app-capable" content="yes">
  <link rel="shortcut icon" href="/">
  <link rel="alternate" href="/atom.xml" title="Melchior on CLR">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="前言本文的主体是机器学习中所用到的概率论知识，因此奉行Lazy evaluation，对这些知识更深层次的探究只在绝对必要时完成。本文假设你学过《概率论与数理统计》，仅指出花书的现有定义的不同，同时补充花书中特有的知识">
<meta name="keywords" content="Machine learning,The flower book,Probability,Information theory">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习基础——概率论与信息论">
<meta property="og:url" content="https://verrickt.github.io/2020/06/26/machine-learning-basics-probability-and-information-theory/index.html">
<meta property="og:site_name" content="Melchior on CLR">
<meta property="og:description" content="前言本文的主体是机器学习中所用到的概率论知识，因此奉行Lazy evaluation，对这些知识更深层次的探究只在绝对必要时完成。本文假设你学过《概率论与数理统计》，仅指出花书的现有定义的不同，同时补充花书中特有的知识">
<meta property="og:locale" content="zh-CN,en-US">
<meta property="og:updated_time" content="2020-06-27T12:01:07.788Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="机器学习基础——概率论与信息论">
<meta name="twitter:description" content="前言本文的主体是机器学习中所用到的概率论知识，因此奉行Lazy evaluation，对这些知识更深层次的探究只在绝对必要时完成。本文假设你学过《概率论与数理统计》，仅指出花书的现有定义的不同，同时补充花书中特有的知识">
    
  <link href="https://fonts.googleapis.com/css?family=Inconsolata|Titillium+Web" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css?family=Roboto+Mono" rel="stylesheet">
  <link href='//cdn.bootcss.com/node-waves/0.7.5/waves.min.css' rel='stylesheet'>
  <link rel="stylesheet" href="/style.css">
  <script>
    function setLoadingBarProgress(num) {
      document.getElementById('loading-bar').style.width=num+"%";
    }
  </script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="loading-bar-wrapper">
  <div id="loading-bar"></div>
</div>


  <script>setLoadingBarProgress(20)</script> 
  <header class="l_header">
	<div class='wrapper'>
		<div class="nav-main container container--flex">
			<a class="logo flat-box" href='/' >
				Melchior on CLR
			</a>
			<div class='menu'>
				<ul class='h-list'>
					
						<li>
							<a class='flat-box nav-home' href='/'>
								Home
							</a>
						</li>
					
						<li>
							<a class='flat-box nav-archives' href='/archives'>
								Archives
							</a>
						</li>
					
						<li>
							<a class='flat-box nav-about' href='/about'>
								About
							</a>
						</li>
					
				</ul>
				<div class='underline'></div>
			</div>
			
				<div class="m_search">
					<form name="searchform" class="form u-search-form">
						<input type="text" class="input u-search-input" placeholder="Search" />
						<span class="icon icon-search"></span>
					</form>
				</div>
			
			<ul class='switcher h-list'>
				
					<li class='s-search'><a href='javascript:void(0)'><span class="icon icon-search flat-box"></span></a></li>
				
				<li class='s-menu'><a href='javascript:void(0)'><span class="icon icon-menu flat-box"></span></a></li>
			</ul>
		</div>
		
		<div class='nav-sub container container--flex'>
			<a class="logo" class="flat-box" href='javascript:void(0)'>
				Word of Forks
			</a>

			<ul class='switcher h-list'>
				<li class='s-comment'><a href='javascript:void(0)'><span class="icon icon-chat_bubble_outline flat-box"></span></a></li>
				<li class='s-top'><a href='javascript:void(0)'><span class="icon icon-arrow_upward flat-box"></span></a></li>
				<li class='s-toc'><a href='javascript:void(0)'><span class="icon icon-format_list_numbered flat-box"></span></a></li>
			</ul>
		</div>
	</div>
</header>
<aside class="menu-phone">
	<nav>
		
			<a href="/" class="nav-home nav">
				Home
			</a>
		
			<a href="/archives" class="nav-archives nav">
				Archives
			</a>
		
			<a href="/about" class="nav-about nav">
				About
			</a>
		
	</nav>
</aside>

    <script>setLoadingBarProgress(40);</script>
  <div class="l_body">
    <div class='container clearfix'>
      <div class='l_main'>
        <article id="post-machine-learning-basics-probability-and-information-theory"
  class="post white-box article-type-post"
  itemscope itemprop="blogPost">
	<section class='meta'>
	<h2 class="title">
  	<a href="/2020/06/26/machine-learning-basics-probability-and-information-theory/">
    	机器学习基础——概率论与信息论
    </a>
  </h2>
	<time>
	  Jun 26, 2020
	</time>
	
    
    <div class='cats'>
        <a href="/categories/Machine-learning/">Machine learning</a>
    </div>

	</section>
	
		<section class="toc-wrapper"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#前言"><span class="toc-number">1.</span> <span class="toc-text">前言</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Probability"><span class="toc-number">2.</span> <span class="toc-text">Probability</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Conditional-probability"><span class="toc-number">2.1.</span> <span class="toc-text">Conditional probability</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Independence"><span class="toc-number">2.2.</span> <span class="toc-text">Independence</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Covariance"><span class="toc-number">2.3.</span> <span class="toc-text">Covariance</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Gaussian-Distribution"><span class="toc-number">2.4.</span> <span class="toc-text">Gaussian Distribution</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Dirac-delta-distribution-amp-empirical-distribution"><span class="toc-number">2.5.</span> <span class="toc-text">Dirac delta distribution & empirical distribution</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Mixture-distribution"><span class="toc-number">2.6.</span> <span class="toc-text">Mixture distribution</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Latent-variable"><span class="toc-number">2.7.</span> <span class="toc-text">Latent variable</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Useful-properties-of-Common-Functions"><span class="toc-number">2.8.</span> <span class="toc-text">Useful properties of Common Functions</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Bayes’-Rule"><span class="toc-number">2.9.</span> <span class="toc-text">Bayes’ Rule</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#prior-probability-and-posterior-probability"><span class="toc-number">2.10.</span> <span class="toc-text">prior-probability and posterior-probability</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#PDF-of-y-where-y-g-x"><span class="toc-number">2.11.</span> <span class="toc-text">PDF of y where y=g(x)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Information-theory"><span class="toc-number">3.</span> <span class="toc-text">Information theory</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Self-information-amp-Shanon-entropy"><span class="toc-number">3.1.</span> <span class="toc-text">Self-information & Shanon entropy</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Structed-Probabilistic-Models"><span class="toc-number">3.2.</span> <span class="toc-text">Structed Probabilistic Models</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#DAG"><span class="toc-number">3.2.1.</span> <span class="toc-text">DAG</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#UAG"><span class="toc-number">3.2.2.</span> <span class="toc-text">UAG</span></a></li></ol></li></ol></li></ol></section>
	
	<section class="article typo">
  	<div class="article-entry" itemprop="articleBody">
    	<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML" async></script>

<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>本文的主体是机器学习中所用到的概率论知识，因此奉行<a href="https://en.wikipedia.org/wiki/Lazy_evaluation" target="_blank" rel="external">Lazy evaluation</a>，对这些知识更深层次的探究只在<strong>绝对必要</strong>时完成。本文假设你学过《概率论与数理统计》，仅指出花书的现有定义的不同，同时补充花书中特有的知识<br><a id="more"></a></p>
<h2 id="Probability"><a href="#Probability" class="headerlink" title="Probability"></a>Probability</h2><h3 id="Conditional-probability"><a href="#Conditional-probability" class="headerlink" title="Conditional probability"></a>Conditional probability</h3><script type="math/tex; mode=display">P(A|B)=\frac{P(AB)}{P(B)}</script><p>Chain rule/product rule of conditiona probability</p>
<script type="math/tex; mode=display">P(x^{(1)},...,x^{(n)})=P(x^{(1)})\prod_{i=2}^{n}P(x^{1}|x^{(1)},...,x^{(n)})</script><p>eg</p>
<script type="math/tex; mode=display">P(a,b,c)=P(a|b,c)P(b,c)</script><script type="math/tex; mode=display">P(b,c)=P(b|c)P(c)</script><script type="math/tex; mode=display">P(a,b,c)=P(a|b,c)P(b|c)P(c)</script><h3 id="Independence"><a href="#Independence" class="headerlink" title="Independence"></a>Independence</h3><p>Indenpendence:</p>
<script type="math/tex; mode=display">\forall x \in x,y \in y,p(x=x,y=y)=p(x=x)p(y=y)</script><p>则x,y互相独立。记作</p>
<script type="math/tex; mode=display">x \perp y</script><p>Conditional independent:</p>
<script type="math/tex; mode=display">\forall x \in x,y \in y ,z\in z,p(x=x,y=y|z=z)=p(x=x|z=z)p(y=y|z=z)</script><p>称x,y在z下相互独立，记作</p>
<script type="math/tex; mode=display">x \perp y \mid z</script><h3 id="Covariance"><a href="#Covariance" class="headerlink" title="Covariance"></a>Covariance</h3><script type="math/tex; mode=display">Cov(f(x),g(y))=\mathbb{E}[(f(x))-\mathbb{E}[f(x)])(g(x))-\mathbb{E}[g(x)]]</script><p>协方差值越高，意味着f和g的变化非常大，并且同时距离各自的均值很远。如果协方差是正值，那么f和g倾向于同时相对大的值。如果是负值，则一个取高值的同时另一个取低值。</p>
<p>向量</p>
<script type="math/tex; mode=display">x \in \mathbb{R}^n</script><p>的covariance matrix(协方差矩阵)是一个<code>nxn</code>的方阵，其中</p>
<script type="math/tex; mode=display">Cov(x)_{i,j}=Cov(x_i,x_j)</script><p>对于对角线的元素，</p>
<script type="math/tex; mode=display">Cov(x_i,x_i)=Var(x_i)</script><h3 id="Gaussian-Distribution"><a href="#Gaussian-Distribution" class="headerlink" title="Gaussian Distribution"></a>Gaussian Distribution</h3><p>高斯分布，又称正态分布。<br>概率密度函数(PDF):</p>
<script type="math/tex; mode=display">\mathcal{N}(x;\mu,\sigma)=\sqrt{\frac{1}{2 \pi \sigma^2}}exp(-\frac{1}{2\sigma^2}(x-\mu)^2)</script><p>计算概率密度时，经常要取σ的平方倒数，工程中常使用另一个参数β∈(0,∞)表示高斯分布的<strong>精准度</strong></p>
<script type="math/tex; mode=display">\mathcal{N}(x;\mu,\beta^{-1})=\sqrt{\frac{\beta}{2 \pi \sigma^2}}exp(-\frac{1}{2}\beta(x-\mu)^2)</script><p>高斯分布的特点</p>
<ul>
<li>现实中很多复杂的系统可以由高斯分布建模（<strong>中心极限定理</strong>）</li>
<li>在方差相同的所有分布中，高斯分布在实数范围上的“不确定度”最高。换句话说，高斯分布是所有分布中对样本做出最少先验假设的</li>
</ul>
<p>N维高斯分布：</p>
<script type="math/tex; mode=display">\mathcal{N}(x;\mu,\Sigma)=\sqrt{\frac{1}{(2\pi)^n det(\Sigma)}}exp(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu))</script><p>其中Σ是正定对称矩阵。μ是矢量形式的分布均值，Σ给出分布的协方差矩阵。为了便于计算，对于N维高斯分布，常用<strong>准确度矩阵β</strong>作为参数：</p>
<script type="math/tex; mode=display">\mathcal{N}(x;\mu,\beta^{-1})=\sqrt{\frac{det(\beta)}{(2\pi)^n }}exp(-\frac{1}{2}(x-\mu)^T\beta(x-\mu))</script><p>实践上通常将协方差矩阵固定为对角阵。更简单的方式是将<strong>isotropic matrix</strong>作为协方差矩阵，其中<strong>isotropic matrix</strong>指标量数乘单位矩阵的结果。</p>
<h3 id="Dirac-delta-distribution-amp-empirical-distribution"><a href="#Dirac-delta-distribution-amp-empirical-distribution" class="headerlink" title="Dirac delta distribution &amp; empirical distribution"></a>Dirac delta distribution &amp; empirical distribution</h3><p>有时我们希望所有的概率密度都聚集在一个点附近。这可以通过Dirac delta函数</p>
<script type="math/tex; mode=display">$\delta(x)$$$实现：
$$p(x)=\delta(x-\mu)</script><p>Dirac delta分布常常被用作empirical distribution（经验分布）的一个组件：</p>
<script type="math/tex; mode=display">\hat{p}(x)=\frac{1}{m}\sum_{i=1}^m\delta(x-x^{(i)})</script><p>empirical distribution在全部m个点</p>
<script type="math/tex; mode=display">x^{(1)},...x^{(m)}</script><p>上放置</p>
<script type="math/tex; mode=display">\frac{1}{m}</script><p>概率密度</p>
<h3 id="Mixture-distribution"><a href="#Mixture-distribution" class="headerlink" title="Mixture distribution"></a>Mixture distribution</h3><p>用其他简单的概率分布来定义概率分布是十分普遍的，混合分布（mixture distribution)就是这样一种方式。混合分布由好几个组件(component)组成。每次采样时，由一个多重分布的结果选择组件标识(component identity)，由此最终结果是由哪一个分布给出的。</p>
<script type="math/tex; mode=display">P(x)=\sum_iP(c=i)P(x\mid c=i)</script><p>其中P(c)是所有组件上的多重分布。</p>
<p>一种常见且强大的混合模型是高斯混合模型。高斯混合模型所有组件都是高斯分布，他们具有不同的参数<code>μ</code>和<code>Σ</code>。有些分布可以增加限制，如所有组件共用协方差矩阵等。</p>
<p>高斯混合分布是概率密度的通用近似方式。具有足够分量的高斯混合模型可以用任何特定的非零误差量来近似任何平滑密度。</p>
<p>PS，如果用高斯分布来做分类问题，让两个分布共用协方差矩阵Σ通常效果会比使用各自的协方差矩阵效果好。</p>
<h3 id="Latent-variable"><a href="#Latent-variable" class="headerlink" title="Latent variable"></a>Latent variable</h3><p>隐含变量（Latent variable)指无法直接观测的随机变量。混合分布中的组件标识变量c就是隐含变量。</p>
<script type="math/tex; mode=display">P(x,c)=P(x\mid c)P(c)</script><p>隐含变量的分布<code>P(c)</code>和条件分布<code>P(x|c)</code>共同决定了<code>P(x)</code>的分布。尽管<code>P(x)</code>可以在没有隐含变量的条件下被计算出来。</p>
<h3 id="Useful-properties-of-Common-Functions"><a href="#Useful-properties-of-Common-Functions" class="headerlink" title="Useful properties of Common Functions"></a>Useful properties of Common Functions</h3><p>logistic sigmoid:</p>
<script type="math/tex; mode=display">\sigma(x)=\frac{1}{1+exp(-x)}</script><p>softplus:</p>
<script type="math/tex; mode=display">\zeta(x)=log(1+exp(x))</script><blockquote>
<p>why the name softplus?<br>It’s a “softeded” version of x^=max(0,x)</p>
</blockquote>
<h3 id="Bayes’-Rule"><a href="#Bayes’-Rule" class="headerlink" title="Bayes’ Rule"></a>Bayes’ Rule</h3><p>已知<code>P(y|x)</code>和<code>P(x)</code>求<code>P(x|y)</code>时可以使用贝叶斯公式：</p>
<script type="math/tex; mode=display">P(x|y)=\frac{P(x)P(y \mid x)}{P(y)}</script><p>其中</p>
<script type="math/tex; mode=display">P(y)=\sum_xP(y \mid x)P(x)</script><h3 id="prior-probability-and-posterior-probability"><a href="#prior-probability-and-posterior-probability" class="headerlink" title="prior-probability and posterior-probability"></a>prior-probability and posterior-probability</h3><ul>
<li>prior-probability<br>即先验概率。指根据以往经验和分析得到的概率</li>
<li>posterior-probability<br>后验概率是在考虑和给出相关证据或数据后所得到的条件概率</li>
</ul>
<p>考虑bayes’ Rule</p>
<script type="math/tex; mode=display">P(\theta \mid x)=\frac{P(x\mid\theta)P(\theta)}{P(x)}</script><ul>
<li>θ：parameter</li>
<li>x：observed value</li>
<li>P(x)：evidence</li>
<li>P(θ)：prior</li>
<li>P(x|θ)：likelihood</li>
<li>P(θ|x)：posterior</li>
</ul>
<h3 id="PDF-of-y-where-y-g-x"><a href="#PDF-of-y-where-y-g-x" class="headerlink" title="PDF of y where y=g(x)"></a>PDF of y where y=g(x)</h3><p>假设有现随机变量x，y，其中y=g(x)，求y的PDF</p>
<p>PDF根据pdf的定义,</p>
<script type="math/tex; mode=display">P(x)_{x \in \delta}=\int_{x}p(x)dx</script><p>p(x)dx为x落在某一邻域δ内的概率。现保留该属性，则有</p>
<script type="math/tex; mode=display">|p_y(g(x))dy|=|p_x(x)dx|</script><script type="math/tex; mode=display">p_y(y)=p_x(g^{-1}(y))\mid \frac{\partial x}{\partial y} \mid</script><script type="math/tex; mode=display">p_x(x)=p_y(g(x))\mid \frac{\partial g(x)}{\partial y} \mid</script><p>考虑高维情况，x与y为向量，定义雅可比矩阵J，其中</p>
<script type="math/tex; mode=display">J_{i,j}=\frac{\partial x_i}{\partial y_j}</script><p>则</p>
<script type="math/tex; mode=display">p_x(x)=p_y(g(x))\mid \det \left\{ \frac{\partial g(x)}{\partial x}\right\} \mid</script><h2 id="Information-theory"><a href="#Information-theory" class="headerlink" title="Information theory"></a>Information theory</h2><p>basic assemption:</p>
<ul>
<li>Likely events should have low information content, and in the extreme case, events that are guaranteed to happen should have no information content whatsoever.<ul>
<li>Less likely events should have higher information content.</li>
<li>Independent events should have additive information. For example, finding out that a tossed coin has come up as heads twice should convey twice as much information as finding out that a tossed coin has come up as heads once.</li>
</ul>
</li>
</ul>
<h3 id="Self-information-amp-Shanon-entropy"><a href="#Self-information-amp-Shanon-entropy" class="headerlink" title="Self-information &amp; Shanon entropy"></a>Self-information &amp; Shanon entropy</h3><p>单一事件所含的信息，单位为nat</p>
<script type="math/tex; mode=display">I(x)=-\log P(x)</script><p>整个概率分布上的不确定性，即香农熵</p>
<script type="math/tex; mode=display">H(x)=E_{x\sim P}[I(x)]=-\mathbb{E}_{x\sim P}[\log P(x)]</script><p>也记作H(P).</p>
<p>当P(x)和Q(x)为相同随机变量x的分部时，两种分布间的“距离”用<br>Kullback-Leibler (KL) divergence定义：</p>
<script type="math/tex; mode=display">D_{KL}(P\|Q)=\mathbb{E}_{x\sim P}\left[\log\frac{P(x)}{Q(x)}\right]=\mathbb{E}_{x\sim p}[\log P(x) - \log Q(x)]</script><p>the value means the extra amount of information<br>needed to send a message containing symbols drawn from probability distribution P, when we use a code that was designed to minimize the length of messages drawn from probability distribution .</p>
<p>KL divergence</p>
<ul>
<li>non-negarive</li>
<li>not symmetric<script type="math/tex; mode=display">D_{KL}(P\|Q)\not ={D_{KL}(Q\|P)}</script></li>
</ul>
<p>与KL divergence密切相关的一种度量是cross-entropy<br>定义为</p>
<script type="math/tex; mode=display">
H(P,Q) = H(P)+D_{KL}(P\|Q)=-\mathbb{E}_{x\sim P}\log Q(x)</script><p>Minimizing the cross-entropy with respect to Q is equivalent to minimizing the KL divergence, because Q does not participate in the omitted term.</p>
<h3 id="Structed-Probabilistic-Models"><a href="#Structed-Probabilistic-Models" class="headerlink" title="Structed Probabilistic Models"></a>Structed Probabilistic Models</h3><p>机器学习中的参数成千上万，使用含有这么多参数的分布不切实际。根据条件概率的乘法公式，可以把大分布拆成小分布的乘积（这一过程叫factorization)。当使用CS中的图来表示这种<code>factorization</code>时，就把模型称为structured probabilistic model或graphical model。structed probabilistic model分为两类，分别使用DAG和UAG。</p>
<h4 id="DAG"><a href="#DAG" class="headerlink" title="DAG"></a>DAG</h4><p>Directed models use graphs with directed edges, and they represent factorizations into conditional probability distributions,  Specifically, a directed model contains one factor for every random variable xi in the distribution</p>
<script type="math/tex; mode=display">p(\mathbf{x})=\prod_ip(x_i|Pa\mathcal{G}(x_i))</script><p>where</p>
<script type="math/tex; mode=display">Pa\mathcal{G}(x_i)</script><p>is the parents of xi.</p>
<h4 id="UAG"><a href="#UAG" class="headerlink" title="UAG"></a>UAG</h4><p>Undirected models use graphs with undirected edges, and they represent<br>factorizations into a set of functions; unlike in the directed case, these functions are usually not probability distributions of any kind. <strong>Any</strong> set of nodes that are all <strong>connected to each other</strong> in G is called a <strong><a href="https://en.wikipedia.org/wiki/Clique_(graph_theory)" target="_blank" rel="external">clique</a></strong>. Each clique Ci in an undirected model is associated with a factor φi . These factors are just functions, not probability distributions. The output of each factor must be non-negative, but there is no constraint that the factor must sum or integrate to 1 like a probability distribution.</p>
<p>The probability of a configuration of random variables is proportional to the<br>product of all of these factors—assignments that result in larger factor values are more likely. Of course, there is no guarantee that this product will sum to 1. We therefore divide by a normalizing constant Z, defined to be the sum or integral over all states of the product of the φ functions, in order to obtain a normalized probability distribution:</p>
<script type="math/tex; mode=display">p(x)=\frac{1}{Z}\prod_i\phi^{(i)}(\mathcal{C}^{(i)})</script><p>DAG和UAG都是描述概率分布的方法，他们并不是互斥的概率分布。使用DAG还是UAG并不是概率分布的属性，而是某种特定<strong>描述方式</strong>的属性</p>

  	</div>
	  
	  <div class="article-tags tags">
      
        <a href="/tags/Machine-learning/">Machine learning</a>
      
        <a href="/tags/The-flower-book/">The flower book</a>
      
        <a href="/tags/Probability/">Probability</a>
      
        <a href="/tags/Information-theory/">Information theory</a>
      
	  </div>
    
		
	
		<div class="art-item-footer">
				
					<span class="art-item-left"><i class="icon icon-chevron-thin-left"></i>prev：<a href="/2020/06/28/machine-learning-basics-numerical-computation/" rel="prev"  title="机器学习基础——数值计算">
						机器学习基础——数值计算 
					</a></span>
				
				
					<span class="art-item-right">next：<a href="/2020/06/24/machine-learning-basics-linear-algebra/" rel="next"  title="机器学习基础——线性代数">
						机器学习基础——线性代数
					</a><i class="icon icon-chevron-thin-right"></i></span>
				
		</div>
	
	</section>
</article>
<script>
	window.subData = {
		title: '机器学习基础——概率论与信息论',
		tools: true
	}
</script>

      </div>
      <aside class='l_side'>
        
  <section class='m_widget about'>

<img class='avatar waves-image' src='https://avatars0.githubusercontent.com/u/11483783' />

<div class='header'>Verrickt</div>
<div class='content'>
<div class='desc'>Von as a programmer</div>
</div>
</section>

  <section class='m_widget links'>
<div class='header'>Links</div>
<div class='content'>
    <ul class="entry">
    
        <li><a class="flat-box" target="_blank" href="https://sjx1995.github.io">
            <div class='name'>Sunly</div>
        </a></li>
    
        <li><a class="flat-box" target="_blank" href="https://liun1an.github.io">
            <div class='name'>LiuN1an</div>
        </a></li>
    
    </ul>
</div>
</section>

  <section class='m_widget categories'>
<div class='header'>Categories</div>
<div class='content'>
    
    <ul class="entry">
    
        <li><a class="flat-box" href="/categories/FluentTreeView/"><div class='name'>FluentTreeView</div><div class='badget'>5</div></a></li>
    
        <li><a class="flat-box" href="/categories/Machine-learning/"><div class='name'>Machine learning</div><div class='badget'>7</div></a></li>
    
    </ul>
    
</div>
</section>

  
<div class="m_widget tagcloud">
    <div class="header">Tags</div>
    <div class='content'>
        <a href="/tags/Net-Framework/" style="font-size: 14px; color: #808080">.Net Framework</a> <a href="/tags/Backpropagation/" style="font-size: 14px; color: #808080">Backpropagation</a> <a href="/tags/C/" style="font-size: 17.6px; color: #333">C#</a> <a href="/tags/CPP/" style="font-size: 14px; color: #808080">CPP</a> <a href="/tags/Debugging/" style="font-size: 14px; color: #808080">Debugging</a> <a href="/tags/HTML/" style="font-size: 14px; color: #808080">HTML</a> <a href="/tags/HttpClient/" style="font-size: 14px; color: #808080">HttpClient</a> <a href="/tags/Information-theory/" style="font-size: 14px; color: #808080">Information theory</a> <a href="/tags/Linear-algebra/" style="font-size: 14px; color: #808080">Linear algebra</a> <a href="/tags/MSBuild/" style="font-size: 14px; color: #808080">MSBuild</a> <a href="/tags/Machine-learning/" style="font-size: 18.8px; color: #1a1a1a">Machine learning</a> <a href="/tags/Numberical-computation/" style="font-size: 14px; color: #808080">Numberical computation</a> <a href="/tags/OOP/" style="font-size: 14px; color: #808080">OOP</a> <a href="/tags/PAT/" style="font-size: 20px; color: #000">PAT</a> <a href="/tags/Parser/" style="font-size: 14px; color: #808080">Parser</a> <a href="/tags/Probability/" style="font-size: 14px; color: #808080">Probability</a> <a href="/tags/Reentrance/" style="font-size: 14px; color: #808080">Reentrance</a> <a href="/tags/Regularization/" style="font-size: 14px; color: #808080">Regularization</a> <a href="/tags/SOLID/" style="font-size: 14px; color: #808080">SOLID</a> <a href="/tags/The-flower-book/" style="font-size: 16.4px; color: #4d4d4d">The flower book</a> <a href="/tags/UWP/" style="font-size: 14px; color: #808080">UWP</a> <a href="/tags/WPF/" style="font-size: 15.2px; color: #666">WPF</a> <a href="/tags/async-await/" style="font-size: 14px; color: #808080">async/await</a> <a href="/tags/classification/" style="font-size: 14px; color: #808080">classification</a> <a href="/tags/lambda/" style="font-size: 14px; color: #808080">lambda</a> <a href="/tags/logistic-regression/" style="font-size: 14px; color: #808080">logistic regression</a> <a href="/tags/neural-network/" style="font-size: 14px; color: #808080">neural network</a> <a href="/tags/overhead/" style="font-size: 14px; color: #808080">overhead</a> <a href="/tags/reference-type/" style="font-size: 14px; color: #808080">reference type</a>
    </div>
</div>



      </aside>
      <script>setLoadingBarProgress(60);</script>
    </div>
  </div>
  <footer id="footer" class="clearfix">

  <div class="social-wrapper">
    
      
        <a href="https://github.com/verrickt" class="social github" target="_blank" rel="external">
          <span class="icon icon-github"></span>
        </a>
        
        <a href="https://twitter.com/verrickt" class="social twitter" target="_blank" rel="external">
          <span class="icon icon-twitter"></span>
        </a>
        
        <a href="/atom.xml" class="social rss" target="_blank" rel="external">
          <span class="icon icon-rss"></span>
        </a>
        
          
  </div>

  <div> 2017-2020 by verrickt. Powered by
    <a href="https://hexo.io/" class="codename">Hexo</a>. Theme by
    <a href='https://github.com/stkevintan/hexo-theme-material-flow' class="codename">MaterialFlow</a>
    <div>
      <span>
        Unless explicitly stated, contents of this blog are licensed under
        <a href="https://creativecommons.org/licenses/by-sa/4.0/" class="codename" target="_blank">CC BY-SA</a>
        <a href="https://creativecommons.org/licenses/by-sa/4.0/" target="_blank">
          <img style="vertical-align: middle" alt="graphical represerentaion of CC BY-SA license" src="https://licensebuttons.net/l/by-sa/3.0/88x31.png"
            height="24.75" width="66">
        </a>
      </span>
    </div>
</footer>
  <script>setLoadingBarProgress(80);</script>
  

<script src="//apps.bdimg.com/libs/jquery/2.1.4/jquery.min.js"></script>
<script src='//cdn.bootcss.com/node-waves/0.7.5/waves.min.js'></script>
<script src="//cdn.bootcss.com/scrollReveal.js/3.3.2/scrollreveal.min.js"></script>
<script src="/js/jquery.fitvids.js"></script>
<script>
	var GOOGLE_CUSTOM_SEARCH_API_KEY = "";
	var GOOGLE_CUSTOM_SEARCH_ENGINE_ID = "";
	var ALGOLIA_API_KEY = "";
	var ALGOLIA_APP_ID = "";
	var ALGOLIA_INDEX_NAME = "";
  var AZURE_SERVICE_NAME = "";
  var AZURE_INDEX_NAME = "";
  var AZURE_QUERY_KEY = "";
  var BAIDU_API_ID = "";
  var SEARCH_SERVICE = "hexo";
  var ROOT = "/"||"/";
  if(!ROOT.endsWith('/'))ROOT += '/';
</script>
<script src="/js/search.js"></script>
<script src="/js/app.js"></script>


  <script>setLoadingBarProgress(100);</script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
</body>
</html>
