<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<!DOCTYPE html>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  
  <title>神经网络中的Regularization | Melchior on CLR</title>
  <meta name="description" content="A programmer focus mainly on Microsoft Technologies" />
  <meta name="keywords" content="Windows desktop,C#,XAML,UWP,WPF,OpenSource,GNU/Linux,CLI" />
  <meta name="HandheldFriendly" content="True" />
  <meta name="apple-mobile-web-app-capable" content="yes">
  <link rel="shortcut icon" href="/">
  <link rel="alternate" href="/atom.xml" title="Melchior on CLR">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Deep learning algorithms are typically applied to extremely complicated domains where the true generation process essentially involves simulating the entire universe… Controlling the complexity of t">
<meta name="keywords" content="Machine learning,Regularization">
<meta property="og:type" content="article">
<meta property="og:title" content="神经网络中的Regularization">
<meta property="og:url" content="https://verrickt.github.io/2020/07/15/regularization-of-neural-network/index.html">
<meta property="og:site_name" content="Melchior on CLR">
<meta property="og:description" content="Deep learning algorithms are typically applied to extremely complicated domains where the true generation process essentially involves simulating the entire universe… Controlling the complexity of t">
<meta property="og:locale" content="zh-CN,en-US">
<meta property="og:image" content="https://verrickt.github.io/2020/07/15/regularization-of-neural-network/early-stopping.png">
<meta property="og:image" content="https://verrickt.github.io/2020/07/15/regularization-of-neural-network/dropout.png">
<meta property="og:image" content="https://verrickt.github.io/2020/07/15/regularization-of-neural-network/linearity.png">
<meta property="og:updated_time" content="2021-01-11T12:07:36.819Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="神经网络中的Regularization">
<meta name="twitter:description" content="Deep learning algorithms are typically applied to extremely complicated domains where the true generation process essentially involves simulating the entire universe… Controlling the complexity of t">
<meta name="twitter:image" content="https://verrickt.github.io/2020/07/15/regularization-of-neural-network/early-stopping.png">
    
  <link href="https://fonts.googleapis.com/css?family=Inconsolata|Titillium+Web" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css?family=Roboto+Mono" rel="stylesheet">
  <link href='//cdn.bootcss.com/node-waves/0.7.5/waves.min.css' rel='stylesheet'>
  <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
  <link rel="manifest" href="/site.webmanifest">
  <link rel="stylesheet" href="/style.css">
  <script>
    function setLoadingBarProgress(num) {
      document.getElementById('loading-bar').style.width=num+"%";
    }
  </script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="loading-bar-wrapper">
  <div id="loading-bar"></div>
</div>


  <script>setLoadingBarProgress(20)</script> 
  <header class="l_header">
	<div class='wrapper'>
		<div class="nav-main container container--flex">
			<a class="logo flat-box" href='/' >
				Melchior on CLR
			</a>
			<div class='menu'>
				<ul class='h-list'>
					
						<li>
							<a class='flat-box nav-home' href='/'>
								Home
							</a>
						</li>
					
						<li>
							<a class='flat-box nav-archives' href='/archives'>
								Archives
							</a>
						</li>
					
						<li>
							<a class='flat-box nav-about' href='/about'>
								About
							</a>
						</li>
					
				</ul>
				<div class='underline'></div>
			</div>
			
				<div class="m_search">
					<form name="searchform" class="form u-search-form">
						<input type="text" class="input u-search-input" placeholder="Search" />
						<span class="icon icon-search"></span>
					</form>
				</div>
			
			<ul class='switcher h-list'>
				
					<li class='s-search'><a href='javascript:void(0)'><span class="icon icon-search flat-box"></span></a></li>
				
				<li class='s-menu'><a href='javascript:void(0)'><span class="icon icon-menu flat-box"></span></a></li>
			</ul>
		</div>
		
		<div class='nav-sub container container--flex'>
			<a class="logo" class="flat-box" href='javascript:void(0)'>
				Word of Forks
			</a>

			<ul class='switcher h-list'>
				<li class='s-comment'><a href='javascript:void(0)'><span class="icon icon-chat_bubble_outline flat-box"></span></a></li>
				<li class='s-top'><a href='javascript:void(0)'><span class="icon icon-arrow_upward flat-box"></span></a></li>
				<li class='s-toc'><a href='javascript:void(0)'><span class="icon icon-format_list_numbered flat-box"></span></a></li>
			</ul>
		</div>
	</div>
</header>
<aside class="menu-phone">
	<nav>
		
			<a href="/" class="nav-home nav">
				Home
			</a>
		
			<a href="/archives" class="nav-archives nav">
				Archives
			</a>
		
			<a href="/about" class="nav-about nav">
				About
			</a>
		
	</nav>
</aside>

    <script>setLoadingBarProgress(40);</script>
  
  <div class="l_body">
    <div class='container clearfix'>
      <div class='l_main'>
        <article id="post-regularization-of-neural-network"
  class="post white-box article-type-post"
  itemscope itemprop="blogPost">
	<section class='meta'>
	<h2 class="title">
  	<a href="/2020/07/15/regularization-of-neural-network/">
    	神经网络中的Regularization
    </a>
  </h2>
  <div class="status" >
	  <div class='item-container'>
		<time>
			Published at
			Jul 15, 2020  
		  </time>
	  </div>
	<div class='item-container'>
		<time>
			Last updated
			Jan 11, 2021
		  </time>
	</div>
	<div class='item-container hide-on-mobile'>
		<time>
			1.4k
			words
		  </time>
	</div>
	<div class='item-container hide-on-mobile'>
		<time>
			<time  id='busuanzi_value_page_pv'></time>
view			
		  </time>
	</div>
	
    
    <div class='cats'>
        <a href="/categories/Machine-learning/">Machine learning</a>
    </div>

</div>
	</section>
	
		<section class="toc-wrapper"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Parameter-Penalities"><span class="toc-number">1.</span> <span class="toc-text">Parameter Penalities</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#L2-norm"><span class="toc-number">1.1.</span> <span class="toc-text">L2 norm</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#L1-norm"><span class="toc-number">1.2.</span> <span class="toc-text">L1 norm</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Early-stopping"><span class="toc-number">2.</span> <span class="toc-text">Early stopping</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Ensemble"><span class="toc-number">3.</span> <span class="toc-text">Ensemble</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#dropout"><span class="toc-number">3.1.</span> <span class="toc-text">dropout</span></a></li></ol></li></ol></section>
	
	<section class="article typo">
  	<div class="article-entry" itemprop="articleBody">
    	<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML" async></script>

<p>Deep learning algorithms are typically applied to extremely complicated domains where the true generation process essentially involves simulating the entire universe… Controlling the complexity of the model is not a simple matter of finding the model of the right size, with the right number of parameters. Instead, we might find—and indeed in practical deep learning scenarios, we almost always do find—that the best fitting model (in the sense of minimizing generalization error) is a large model that has been regularized appropriately</p>
<a id="more"></a>
<p>regularization是我们应对overfitting的常用手段。在比较简单的模型，例如linear-regresseion中，人对的capacity的估计通常比较准确，一般不需要做regularization。而在神经网络动辄成千上万个节点，数以百万的参数让估测capacity变得极为困难。正如花书所说，泛化性能最好的往往是复杂的模型加上合理的regularization而得到的</p>
<h2 id="Parameter-Penalities"><a href="#Parameter-Penalities" class="headerlink" title="Parameter Penalities"></a>Parameter Penalities</h2><p>正如三次式最高次的系数为零时退化为二次式，有越多的参数接近0，模型的capacity就越低，即「绑住手脚」。一般来说regularization不会考虑<code>bias</code>，因为<code>bias</code>对capacity的影响并不如<code>weight</code>那么大，但有需要仍可以给<code>bias</code>加上惩罚。</p>
<p>参数惩罚的思路是，给<code>Loss</code>加上所有参数<code>θ</code>的函数<code>Ω(θ)</code>这一项，让模型更偏好于θ接近0的归纳假设。</p>
<script type="math/tex; mode=display">
\hat{L}(\theta,\alpha)=L(\theta)+\alpha \Omega(\theta)</script><h3 id="L2-norm"><a href="#L2-norm" class="headerlink" title="L2 norm"></a>L2 norm</h3><p>L2 norm中所选的函数Ω是</p>
<script type="math/tex; mode=display">\Omega(\theta)=\frac{1}{2}\theta^{\top}\theta</script><p><code>Loss</code>的梯度变为</p>
<script type="math/tex; mode=display">\nabla_{\theta} \hat{L}=\nabla_{\theta}L+\alpha\theta</script><p>带入梯度下降公式</p>
<script type="math/tex; mode=display">
\theta^{N+1} \leftarrow\theta^N-\epsilon\nabla_\theta \hat{L}
=\theta^N-\epsilon\nabla_\theta L - \alpha\epsilon \theta^N = (1-\alpha \epsilon)\theta ^N-\epsilon\nabla_\theta L</script><p>每次梯度下降更新参数时会把原来的参数<strong>乘</strong>上一个固定的值<code>1-αε</code>，使得θ接近于0</p>
<h3 id="L1-norm"><a href="#L1-norm" class="headerlink" title="L1 norm"></a>L1 norm</h3><p>L1 norm中的函数Ω是</p>
<script type="math/tex; mode=display">
\Omega(\theta)=\sum_{i=1}^N|\theta_i|</script><p><code>Loss</code>的梯度变为</p>
<script type="math/tex; mode=display">\nabla_{\theta} \hat{L}=\nabla_{\theta}L+\alpha \mathrm{sgn}(\theta)</script><p>带入梯度下降公式</p>
<script type="math/tex; mode=display">
\theta^{N+1} \leftarrow\theta^N-\epsilon\nabla_\theta \hat{L}
=\theta^N-\epsilon\nabla_{\theta}L-\alpha\epsilon \mathrm{sgn}(\theta^N) = \theta ^N-\epsilon\nabla_\theta L -\alpha\epsilon \mathrm{sgn}(\theta^N)</script><p>发现参数更新时会在每一个分量<code>i</code>中<strong>减</strong>掉一个固定值<code>αε*sgn(i)</code></p>
<p>L2 norm中，每次都乘上一个小于1的数，当参数大于1时，减小的速度就非常快；当参数小于1时，减小的速度就比较慢。因此L2中最终各个参数比较接近，但不会出现很多0。</p>
<p>而L1 norm中，每次减小一个固定值。对很大的参数，减法的步长很小，减小的效果就不是很明显。而参数很小时，减法的步长相对很大，就很容易出现0。因此L1中很容易出现很大的参数和较多的0。</p>
<p>含较多0的矩阵叫做稀疏矩阵(Sparse matrix)，因此用L1能够得到比较稀疏的参数。</p>
<h2 id="Early-stopping"><a href="#Early-stopping" class="headerlink" title="Early stopping"></a>Early stopping</h2><p><img src="/2020/07/15/regularization-of-neural-network/early-stopping.png" alt="early stopping"></p>
<p>early stopping的思路就更简单了。既然复杂的模型训练时会关注一些无关的特征，那干脆不要跑那么多次训练就好了。</p>
<p>early stopping的形式化描述：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line">Let n be the number of steps between evaluations.</div><div class="line">Let p be the “patience,” the number of times to observe worsening validation set error before giving up. </div><div class="line">Let θ^o be the initial parameters. </div><div class="line">θ ← θ^o </div><div class="line">i ← 0 </div><div class="line">j ← 0 </div><div class="line">v ←∞ </div><div class="line">θ∗ ← θ </div><div class="line">i∗ ← i </div><div class="line">while j &lt; p do θ</div><div class="line">    Update by running the training algorithm for n steps. </div><div class="line">    i ← i + n</div><div class="line">    v&apos; ← ValidationSetError(θ) </div><div class="line">    if v&apos; &lt; v then </div><div class="line">        j  ← 0</div><div class="line">        θ∗ ← θ </div><div class="line">        i∗ ← i</div><div class="line">        v  ← v&apos;</div><div class="line">    else </div><div class="line">        j ← j + 1 </div><div class="line">    end if</div><div class="line">end while</div></pre></td></tr></table></figure></p>
<hr>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">nodes:</div><div class="line">Best parameters are θ∗ , best number of training steps is i∗</div></pre></td></tr></table></figure>
<h2 id="Ensemble"><a href="#Ensemble" class="headerlink" title="Ensemble"></a>Ensemble</h2><p>Ensemble认为，模型中出现的错误是随机的，那么使用<strong>同样</strong>的数据训练出k个不同的模型，将它们的结果取平均就可以得到比较好的结果。</p>
<p>复杂模型error主要来自variance。从直觉上说，对多个模型取平均就能很好的抵消一部分variance。<br>ensemble的坏处在于，它需要训练k个不同的模型，导致计算量大增。有时这样的计算量是无法承受的，因此希望有一个代价不那么大的ensemble方法，这就是dropout</p>
<h3 id="dropout"><a href="#dropout" class="headerlink" title="dropout"></a>dropout</h3><p>dropout和以往的方法不同，它把当前训练的模型当作是很多模型的叠加：每次训练时，每个单元都有p的概率从网络中被移除，每次只针对这些被留下来的单元做参数的更新。有n个单元时，因为每一个单元都可以被留下/丢掉，这就可能产生2^N个不同的网络结构。dropout就认为当前的模型是这2^N个网络的ensemble。</p>
<p>下面是一个2层的神经网络。它有2个隐藏单元。dropout可以产生的网络由16个。使用dropout的网络经过一次训练后相当于训练了2^N个子网络，计算量的问题就这样解决了。<br><img src="/2020/07/15/regularization-of-neural-network/dropout.png" alt="droupout"></p>
<p>使用dropout的网络训练完成后，在测试时不再移除神经元，而是给网络整体的参数乘上1-p。<br>一个直觉的解释：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">设w是dropout得到的参数，w&apos;是训练时的参数。</div><div class="line">在训练时神经元有p的概率被丢弃，所以网络中实际神经元个数的期望是n*(1-p)。</div><div class="line">现在要让网络的输出尽可能的一致，则</div><div class="line">n*w*(1-p) = n*w&apos;</div><div class="line">即w&apos;=w*(1-p)</div></pre></td></tr></table></figure></p>
<p><img src="/2020/07/15/regularization-of-neural-network/linearity.png" alt="linearity"></p>
<p>实际的分析发现，在整个网络是线性时，上面的论述是精确成立的，而加上激活函数的网络往往不是线性的。但实际使用上dropout的性能也确实比较接近ensemble的结果。为什么会这样我们不清楚，总之拿来主义了。世界各地有很多科学家也在探究背后的奥秘，希望有一天能找出一个精确的解释吧。</p>

  	</div>
	  
	  <div class="article-tags tags">
      
        <a href="/tags/Machine-learning/">Machine learning</a>
      
        <a href="/tags/Regularization/">Regularization</a>
      
	  </div>
    
		
	
		<div class="art-item-footer">
				
					<span class="art-item-left"><i class="icon icon-chevron-thin-left"></i>prev：<a href="/2020/07/18/convolutional-neural-network/" rel="prev"  title="卷积神经网络">
						卷积神经网络 
					</a></span>
				
				
					<span class="art-item-right">next：<a href="/2020/07/11/back-propagation-gradient-descent-improved/" rel="next"  title="后向传播-进阶版的梯度下降算法">
						后向传播-进阶版的梯度下降算法
					</a><i class="icon icon-chevron-thin-right"></i></span>
				
		</div>
	
	</section>
	
</article>
<script>
	window.subData = {
		title: '神经网络中的Regularization',
		tools: true
	}
</script>

      </div>
      <aside class='l_side'>
        
  <section class='m_widget about'>

<img class='avatar waves-image' src='/rei.png' />

<div class='header'>Verrickt</div>
<div class='content'>
<div class='desc'>What one programmer can do in one month, two programmers can do in two months</div>
</div>
</section>

  <section class='m_widget links'>
<div class='header'>Links</div>
<div class='content'>
    <ul class="entry">
    
        <li><a class="flat-box" target="_blank" href="https://sjx1995.github.io">
            <div class='name'>Sunly</div>
        </a></li>
    
        <li><a class="flat-box" target="_blank" href="https://liun1an.github.io">
            <div class='name'>LiuN1an</div>
        </a></li>
    
    </ul>
</div>
</section>

  <section class='m_widget categories'>
<div class='header'>Categories</div>
<div class='content'>
    
    <ul class="entry">
    
        <li><a class="flat-box" href="/categories/FluentTreeView/"><div class='name'>FluentTreeView</div><div class='badget'>5</div></a></li>
    
        <li><a class="flat-box" href="/categories/Machine-learning/"><div class='name'>Machine learning</div><div class='badget'>12</div></a></li>
    
        <li><a class="flat-box" href="/categories/Other/"><div class='name'>Other</div><div class='badget'>2</div></a></li>
    
    </ul>
    
</div>
</section>

  
<div class="m_widget tagcloud">
    <div class="header">Tags</div>
    <div class='content'>
        <a href="/tags/Net-Framework/" style="font-size: 14px; color: #808080">.Net Framework</a> <a href="/tags/Anaconda/" style="font-size: 15.2px; color: #666">Anaconda</a> <a href="/tags/Backpropagation/" style="font-size: 14px; color: #808080">Backpropagation</a> <a href="/tags/C/" style="font-size: 17.6px; color: #333">C#</a> <a href="/tags/CNN/" style="font-size: 14px; color: #808080">CNN</a> <a href="/tags/CPP/" style="font-size: 14px; color: #808080">CPP</a> <a href="/tags/CUDA/" style="font-size: 14px; color: #808080">CUDA</a> <a href="/tags/Debugging/" style="font-size: 14px; color: #808080">Debugging</a> <a href="/tags/HTML/" style="font-size: 14px; color: #808080">HTML</a> <a href="/tags/HttpClient/" style="font-size: 14px; color: #808080">HttpClient</a> <a href="/tags/Information-theory/" style="font-size: 14px; color: #808080">Information theory</a> <a href="/tags/Jupyter/" style="font-size: 14px; color: #808080">Jupyter</a> <a href="/tags/Linear-algebra/" style="font-size: 14px; color: #808080">Linear algebra</a> <a href="/tags/MSBuild/" style="font-size: 14px; color: #808080">MSBuild</a> <a href="/tags/Machine-learning/" style="font-size: 20px; color: #000">Machine learning</a> <a href="/tags/Numberical-computation/" style="font-size: 14px; color: #808080">Numberical computation</a> <a href="/tags/OOP/" style="font-size: 14px; color: #808080">OOP</a> <a href="/tags/PAT/" style="font-size: 18.8px; color: #1a1a1a">PAT</a> <a href="/tags/PCA/" style="font-size: 14px; color: #808080">PCA</a> <a href="/tags/Parser/" style="font-size: 14px; color: #808080">Parser</a> <a href="/tags/Probability/" style="font-size: 14px; color: #808080">Probability</a> <a href="/tags/RNN/" style="font-size: 14px; color: #808080">RNN</a> <a href="/tags/Reentrance/" style="font-size: 14px; color: #808080">Reentrance</a> <a href="/tags/Regularization/" style="font-size: 14px; color: #808080">Regularization</a> <a href="/tags/SOLID/" style="font-size: 14px; color: #808080">SOLID</a> <a href="/tags/SVM/" style="font-size: 14px; color: #808080">SVM</a> <a href="/tags/Semi-supervised-learning/" style="font-size: 14px; color: #808080">Semi-supervised learning</a> <a href="/tags/Tensorflow/" style="font-size: 14px; color: #808080">Tensorflow</a> <a href="/tags/The-flower-book/" style="font-size: 16.4px; color: #4d4d4d">The flower book</a> <a href="/tags/UWP/" style="font-size: 14px; color: #808080">UWP</a> <a href="/tags/VSCode/" style="font-size: 14px; color: #808080">VSCode</a> <a href="/tags/WPF/" style="font-size: 15.2px; color: #666">WPF</a> <a href="/tags/async-await/" style="font-size: 14px; color: #808080">async/await</a> <a href="/tags/classification/" style="font-size: 14px; color: #808080">classification</a> <a href="/tags/lambda/" style="font-size: 14px; color: #808080">lambda</a> <a href="/tags/logistic-regression/" style="font-size: 14px; color: #808080">logistic regression</a> <a href="/tags/neural-network/" style="font-size: 14px; color: #808080">neural network</a> <a href="/tags/overhead/" style="font-size: 14px; color: #808080">overhead</a> <a href="/tags/reference-type/" style="font-size: 14px; color: #808080">reference type</a>
    </div>
</div>



  <section class='m_widget categories'>

    <div class='header'>Statistics</div>
    <div class='content'>
        <ul class="entry">
            <li>
                <a class='flat-box'>
                    <div class='name'>Page Viewer</div>
                    <div class='badget' id='busuanzi_value_site_pv'></div>
                </a>
            </li>
            <li>
                <a class='flat-box'>
                    <div class='name'>Unique Viewer</div>
                    <div class='badget' id='busuanzi_value_site_uv'></div>
                </a>
            </li>
            <li>
                <a class='flat-box'>
                    <div class='name'>Up time</div>
                    <div class='badget'>
                        
                            
                                1375
                                    Days
                    </div>
                </a>
            </li>
            <li>
                <a class='flat-box'>
                    <div class='name'>Last Updated</div>
                    <div class='badget'>
                            Jul 31, 2021
                                    
                    </div>
                </a>
            </li>
            <li>
                <a class='flat-box'>
                    <div class='name'>Blogs</div>
                    <div class='badget'>
                        44
                    </div>
                </a>
            </li>
        </ul>
    </div>

      </aside>
      <script>setLoadingBarProgress(60);</script>
    </div>
  </div>
  <footer id="footer" class="clearfix">

	<div class="social-wrapper">
  	
      
        <a href="https://github.com/verrickt" class="social github"
          target="_blank" rel="external">
          <span class="icon icon-github"></span>
        </a>
      
        <a href="https://twitter.com/verrickt" class="social twitter"
          target="_blank" rel="external">
          <span class="icon icon-twitter"></span>
        </a>
      
        <a href="/atom.xml" class="social rss"
          target="_blank" rel="external">
          <span class="icon icon-rss"></span>
        </a>
      
    
  </div>
  <div class="footer_content">
    Melchior on CLR
    ©
    2017
      -
    2021,
    brought to life with ❤️ By
    <a  href='/' >
      Verrickt
    </a>

  </div>
  <div class="footer_content">
    Powered by <a href='https://hexo.io/' target="blank_">Hexo</a> |
    Theme based on <a target="blank_" href='https://github.com/stkevintan/hexo-theme-material-flow' class="">MaterialFlow</a></div>
  
</footer>


  <script>setLoadingBarProgress(80);</script>
  

<script src="//apps.bdimg.com/libs/jquery/2.1.4/jquery.min.js"></script>
<script src='//cdn.bootcss.com/node-waves/0.7.5/waves.min.js'></script>
<script src="//cdn.bootcss.com/scrollReveal.js/3.3.2/scrollreveal.min.js"></script>
<script src="/js/jquery.fitvids.js"></script>
<script>
	var GOOGLE_CUSTOM_SEARCH_API_KEY = "";
	var GOOGLE_CUSTOM_SEARCH_ENGINE_ID = "";
	var ALGOLIA_API_KEY = "";
	var ALGOLIA_APP_ID = "";
	var ALGOLIA_INDEX_NAME = "";
  var AZURE_SERVICE_NAME = "";
  var AZURE_INDEX_NAME = "";
  var AZURE_QUERY_KEY = "";
  var BAIDU_API_ID = "";
  var SEARCH_SERVICE = "hexo";
  var ROOT = "/"||"/";
  if(!ROOT.endsWith('/'))ROOT += '/';
</script>
<script src="/js/search.js"></script>
<script src="/js/app.js"></script>
<script src="/js/canvas.js"></script>
<script src="/js/typed.js"></script>

  <script>setLoadingBarProgress(100);</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>
</html>
